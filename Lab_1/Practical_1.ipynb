{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e692347b",
   "metadata": {},
   "source": [
    "**This is the notebook for the practical part of lab 1.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f6faf",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95fe469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import gzip\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import copy\n",
    "\n",
    "#import data_loading_code as dlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0c91b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "#print(torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad8c3f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9742f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pandas(data): #, columns\n",
    "    # df_ = pd.DataFrame(columns=columns)\n",
    "    data['Sentence'] = data['Sentence'].str.lower()\n",
    "    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
    "    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
    "    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]','')                                                       # remove special characters\n",
    "    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
    "    # for index, row in data.iterrows():\n",
    "    #     word_tokens = word_tokenize(row['Sentence'])\n",
    "    #     filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
    "    #     df_.loc[len(df_)] = {\n",
    "    #         \"index\": row['index'],\n",
    "    #         \"Class\": row['Class'],\n",
    "    #         \"Sentence\": \" \".join(filtered_sent)\n",
    "    #     }\n",
    "    #return df_\n",
    "    return data\n",
    "\n",
    "\n",
    "def split_data(data):\n",
    "    # split the data into training, validation, and test splits\n",
    "    # first get 50% for training\n",
    "    X_train, X_rest, y_train, y_rest = train_test_split(\n",
    "        data['Sentence'],\n",
    "        data['Class'],\n",
    "        test_size=0.5,\n",
    "        random_state=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # split rest to get 50/20/30 split\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_rest,\n",
    "        y_rest,\n",
    "        test_size=0.6,  # 60% of 50% = 30% test\n",
    "        random_state=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e0857",
   "metadata": {},
   "source": [
    "small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f661c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and pre-process data \n",
    "data = pd.read_csv(\"amazon_cells_labelled.txt\", delimiter='\\t', header=None)\n",
    "\n",
    "data.columns = ['Sentence', 'Class']\n",
    "data['index'] = data.index # add new column index\n",
    "data = preprocess_pandas(data) # pre-process\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d17adc",
   "metadata": {},
   "source": [
    "large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "480bcdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and pre-process data\n",
    "data_large = pd.read_csv(\"amazon_cells_labelled_LARGE_25K.txt\", delimiter='\\t', header=None)\n",
    "\n",
    "data_large.columns = ['Sentence', 'Class']\n",
    "data_large['index'] = data_large.index\n",
    "data_large = preprocess_pandas(data_large)\n",
    "\n",
    "# Split data\n",
    "X_train_l, X_val_l, X_test_l, y_train_l, y_val_l, y_test_l = split_data(data_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d35a2",
   "metadata": {},
   "source": [
    "Loading amazon review dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfac1b3",
   "metadata": {},
   "source": [
    "Download the amazon reviews dataset 'Electronics_5.json.gz' on https://nijianmo.github.io/amazon/index.html#subsets. It is 1.2GB big."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86a706b",
   "metadata": {},
   "source": [
    "Use following cell once to load, preprocess, and safe the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18aefc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'Electronics_5.json.gz'\n",
    "output_file = 'electronics_reviews_preprocessed.parquet'\n",
    "\n",
    "data_elec = []\n",
    "with gzip.open(input_file, 'rt', encoding='utf-8') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        try:\n",
    "            review = json.loads(line)\n",
    "            overall = review.get('overall')\n",
    "            text = review.get('reviewText')\n",
    "            if overall is not None and text:  # ensure both exist\n",
    "                data_elec.append({\n",
    "                    'index': idx,\n",
    "                    'Class': overall,\n",
    "                    'Sentence': text.strip()\n",
    "                })\n",
    "        except json.JSONDecodeError:\n",
    "            continue  # skip malformed lines\n",
    "\n",
    "# Convert to DataFrame\n",
    "data_elec = pd.DataFrame(data_elec)\n",
    "\n",
    "# Preprocess data\n",
    "data_elec = preprocess_pandas(data_elec)\n",
    "\n",
    "# Converte DataFrame to PyArrow Table\n",
    "table = pa.Table.from_pandas(data_elec)\n",
    "\n",
    "# Save data in Parquet file to quickly access it again\n",
    "pq.write_table(table, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c53a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed dataset\n",
    "table = pq.read_table('electronics_reviews_preprocessed.parquet')\n",
    "data_elec = table.to_pandas()\n",
    "\n",
    "# Split data\n",
    "X_train_elec, X_val_elec, X_test_elec, y_train_elec, y_val_elec, y_test_elec = split_data(data_elec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d89ee6",
   "metadata": {},
   "source": [
    "## Vectorise using tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40499850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(X_train, X_val, X_test, y_train, y_val, y_test, tfidf_vectorizer, batch_size=32, handle_class_imbalance=False):\n",
    "    \"\"\"\n",
    "    Convert text data to TF-IDF vectors, then to PyTorch tensors, and return DataLoaders for train, validation, and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: List or array of training sentences.\n",
    "    - X_val: List or array of validation sentences.\n",
    "    - X_test: List or array of test sentences.\n",
    "    - y_train: List or array of training labels.\n",
    "    - y_val: List or array of validation labels.\n",
    "    - y_test: List or array of test labels.\n",
    "    - tfidf_vectorizer: A fitted TfidfVectorizer.\n",
    "    - batch_size: Batch size for DataLoader (default is 32).\n",
    "    \n",
    "    Returns:\n",
    "    - train_loader: DataLoader for the training set.\n",
    "    - val_loader: DataLoader for the validation set.\n",
    "    - test_loader: DataLoader for the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    # if handle_class_imbalance:\n",
    "        # classes_len = []\n",
    "        # for i in range(handle_class_imbalance):\n",
    "        #     classes_len.append(len(list(filter(lambda x: x == i+1, y_train))))\n",
    "        # total = sum(classes_len)\n",
    "        # part = list(map(lambda val: val/total, classes_len))\n",
    "        # print(classes_len)\n",
    "        # print(total)\n",
    "        # print(part)\n",
    "\n",
    "    \n",
    "    # Transform texts to sparse matrix and then to dense matrix for Pytorch\n",
    "    X_train_vec = tfidf_vectorizer.fit_transform(X_train).todense()\n",
    "    X_val_vec = tfidf_vectorizer.transform(X_val).todense()\n",
    "    X_test_vec = tfidf_vectorizer.transform(X_test).todense()\n",
    "\n",
    "    # NOTE: This isn't fully done yet\n",
    "    # if handle_class_imbalance:\n",
    "        # pass\n",
    "\n",
    "    # Convert to tensors\n",
    "    train_x_tensor = torch.from_numpy(np.array(X_train_vec)).type(torch.FloatTensor).to(device)\n",
    "    train_y_tensor = torch.from_numpy(np.array(y_train)).type(torch.FloatTensor).to(device)\n",
    "\n",
    "    val_x_tensor = torch.from_numpy(np.array(X_val_vec)).type(torch.FloatTensor).to(device)\n",
    "    val_y_tensor = torch.from_numpy(np.array(y_val)).type(torch.FloatTensor).to(device)\n",
    "\n",
    "    test_x_tensor = torch.from_numpy(np.array(X_test_vec)).type(torch.FloatTensor).to(device)\n",
    "    test_y_tensor = torch.from_numpy(np.array(y_test)).type(torch.FloatTensor).to(device)\n",
    "\n",
    "    # Build datasets\n",
    "    train_dataset = TensorDataset(train_x_tensor, train_y_tensor)\n",
    "    val_dataset = TensorDataset(val_x_tensor, val_y_tensor)\n",
    "    test_dataset = TensorDataset(test_x_tensor, test_y_tensor)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "403759a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "# The amazon dataset needs less max_features\n",
    "# rough math is 3_000_000 rows * float64 * max_features -> 3MB * 8B * max_features -> 24MB * max_features\n",
    "#   I initially took 100 features, therefore we need 2.4GB RAM\n",
    "#   Going with 300 features requires around 7.2GB GPU RAM \n",
    "tfidf_vectorizer_elec = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=100, max_df=0.5, use_idf=True, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5079e9f2",
   "metadata": {},
   "source": [
    "**Create DataLoaders**\n",
    "\n",
    "small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fe7cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_data_loaders(X_train, X_val, X_test, y_train, y_val, y_test, tfidf_vectorizer, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158a433",
   "metadata": {},
   "source": [
    "large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3acd84e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_l, val_loader_l, test_loader_l = create_data_loaders(X_train_l, X_val_l, X_test_l, y_train_l, y_val_l, y_test_l, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c17b2f0",
   "metadata": {},
   "source": [
    "amazon electronics review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e7fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_elec, val_loader_elec, test_loader_elec = create_data_loaders(X_train_elec, X_val_elec, X_test_elec, y_train_elec, y_val_elec, y_test_elec, tfidf_vectorizer_elec, handle_class_imbalance=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c0dcab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sentence embeddings using Word2Vec for vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95fe3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load pretrained word2vec model with dimension 100\n",
    "# word2vec_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "# embedding_dim = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc41033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(X_train) + list(X_val) + list(X_test)\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "sentences_l = list(X_train_l) + list(X_val_l) + list(X_test_l)\n",
    "# Tokenize sentences\n",
    "tokenized_sentences_l = [word_tokenize(sentence.lower()) for sentence in sentences_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0a5c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec \n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=100,       # dimension of vectors\n",
    "    window=5,              \n",
    "    min_count=1,           # min. appearance of a word in dataset\n",
    "    workers=4,             \n",
    "    sg=0                   # 1 = skip-gram, 0 = CBOW\n",
    ")\n",
    "\n",
    "word2vec_model_l = Word2Vec(\n",
    "    sentences=tokenized_sentences_l,\n",
    "    vector_size=100,       # dimension of vectors\n",
    "    window=5,              \n",
    "    min_count=1,           # min. appearance of a word in dataset\n",
    "    workers=4,             \n",
    "    sg=0                   # 1 = skip-gram, 0 = CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81f1b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert sentences into embeddings\n",
    "def sentence_to_vec(sentence, model, embedding_dim=100):\n",
    "    words = sentence.split()\n",
    "    vec = torch.zeros(embedding_dim).to(device)\n",
    "    word_count = 0\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            vec += torch.tensor(model.wv[word]).to(device)\n",
    "            word_count += 1\n",
    "    if word_count > 0:\n",
    "        vec /= word_count\n",
    "    return vec\n",
    "\n",
    "# Convert all sentences to embeddings for the datasets\n",
    "def create_dataset(X, y, word2vec_model, embedding_dim=100):\n",
    "    x_tensor = torch.stack([sentence_to_vec(sentence, word2vec_model, embedding_dim) for sentence in X]).to(device)\n",
    "    y_tensor = torch.tensor(np.array(y), dtype=torch.float32).to(device)  # Binary labels as float32\n",
    "    return x_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f178e",
   "metadata": {},
   "source": [
    "small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf772d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensor datasets\n",
    "train_x_tensor_w2v, train_y_tensor_w2v = create_dataset(X_train, y_train, word2vec_model)\n",
    "val_x_tensor_w2v, val_y_tensor_w2v = create_dataset(X_val, y_val, word2vec_model)\n",
    "test_x_tensor_w2v, test_y_tensor_w2v = create_dataset(X_test, y_test, word2vec_model)\n",
    "\n",
    "# Build datasets and DataLoaders\n",
    "train_loader_w2v = DataLoader(TensorDataset(train_x_tensor_w2v, train_y_tensor_w2v), batch_size=32, shuffle=True)\n",
    "val_loader_w2v = DataLoader(TensorDataset(val_x_tensor_w2v, val_y_tensor_w2v), batch_size=32, shuffle=False)\n",
    "test_loader_w2v = DataLoader(TensorDataset(test_x_tensor_w2v, test_y_tensor_w2v), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecc8b84",
   "metadata": {},
   "source": [
    "large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "577983f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensor datasets\n",
    "train_x_tensor_w2v_l, train_y_tensor_w2v_l = create_dataset(X_train_l, y_train_l, word2vec_model_l)\n",
    "val_x_tensor_w2v_l, val_y_tensor_w2v_l = create_dataset(X_val_l, y_val_l, word2vec_model_l)\n",
    "test_x_tensor_w2v_l, test_y_tensor_w2v_l = create_dataset(X_test_l, y_test_l, word2vec_model_l)\n",
    "\n",
    "# Build datasets and DataLoaders\n",
    "train_loader_w2v_l = DataLoader(TensorDataset(train_x_tensor_w2v_l, train_y_tensor_w2v_l), batch_size=32, shuffle=True)\n",
    "val_loader_w2v_l = DataLoader(TensorDataset(val_x_tensor_w2v_l, val_y_tensor_w2v_l), batch_size=32, shuffle=False)\n",
    "test_loader_w2v_l = DataLoader(TensorDataset(test_x_tensor_w2v_l, test_y_tensor_w2v_l), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d19d2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a55a4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ANN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0221af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, name = 'default'):\n",
    "        super(ANNClassifier, self).__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, 16)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.bn2 = nn.BatchNorm1d(8)\n",
    "        self.relu2 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.drop1 = nn.Dropout(0.7)\n",
    "\n",
    "        self.fc3 = nn.Linear(8, 1)  # Single neuron output\n",
    "\n",
    "        self.name = name + '.pth'\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu1(self.bn1(self.fc1(x)))\n",
    "        x = self.drop1(self.relu2(self.bn2(self.fc2(x))))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Sigmoid for binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f035da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleANNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, name = 'default'):\n",
    "        super(SimpleANNClassifier, self).__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, 8)\n",
    "        self.bn1 = nn.BatchNorm1d(8)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.drop1 = nn.Dropout(0.7)\n",
    "\n",
    "        self.fc2 = nn.Linear(8, 1)  # Single neuron output\n",
    "\n",
    "        self.name = name + '.pth'\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.drop1(self.relu1(self.bn1(self.fc1(x))))\n",
    "        x = torch.sigmoid(self.fc2(x))  # Sigmoid for binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "307da711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleANNClassifierMulti(nn.Module):\n",
    "    def __init__(self, input_size, name = 'default'):\n",
    "        super(SimpleANNClassifierMulti, self).__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, 8)\n",
    "        self.bn1 = nn.BatchNorm1d(8)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.drop1 = nn.Dropout(0.7)\n",
    "\n",
    "        self.fc2 = nn.Linear(8, 6)  # Ratings 0-5 \n",
    "\n",
    "        self.name = name + '.pth'\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.drop1(self.relu1(self.bn1(self.fc1(x))))\n",
    "        x = torch.softmax(self.fc2(x), 0)\n",
    "        print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9c24cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNClassifierLarge(nn.Module):\n",
    "    def __init__(self, input_size, name = 'default'):\n",
    "        super(ANNClassifierLarge, self).__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(64, 16)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "        self.relu3 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc4 = nn.Linear(16, 1)  # Single neuron output\n",
    "\n",
    "        self.name = name + '.pth'\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu1(self.bn1(self.fc1(x)))\n",
    "        x = self.drop1(self.relu2(self.bn2(self.fc2(x))))\n",
    "        x = self.drop2(self.relu3(self.bn3(self.fc3(x))))\n",
    "        x = torch.sigmoid(self.fc4(x))  # Sigmoid for binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a07df824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, mod=False):\n",
    "    train_loss_array = []\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)  # [batch, 1] for BCELoss\n",
    "            if(mod):\n",
    "                labels = mod(labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted = (outputs >= 0.5).int()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        training_loss = running_train_loss / len(train_loader)\n",
    "        train_loss_array.append(training_loss)\n",
    "\n",
    "        val_loss, val_acc, _, _ = test_model(loader=val_loader, model=model, criterion=criterion, mod=mod)\n",
    "\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, model.name)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {training_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    print(f\"Best validation accuracy: {best_accuracy}%\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_model(loader, model=None, criterion=None, input_size=None, mod=False):\n",
    "    if model is None:\n",
    "        model = ANNClassifier(input_size)\n",
    "        model.load_state_dict(torch.load('amazonclass.pth'))\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "            if(mod):\n",
    "                labels = mod(labels)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            predicted = (outputs >= 0.5).int()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(labels.squeeze().tolist())\n",
    "            all_preds.extend(predicted.squeeze().tolist())\n",
    "\n",
    "    loss = running_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return loss, accuracy, np.array(all_labels), np.array(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7780e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(all_labels=None, all_predictions=None, class_labels=None):\n",
    "    if all_labels is None or all_predictions is None:\n",
    "        print(\"Error: all_labels or all_predictions needs to be passed\")\n",
    "        return\n",
    "\n",
    "    if class_labels is None:\n",
    "        class_labels = [\"Negative (0)\", \"Positive (1)\"]  # Default\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = metrics.confusion_matrix(all_labels, all_predictions)\n",
    "    # print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    # Plot it with descriptive labels\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    cm_display.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53baeb7d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21948487",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Training and Evaluation of ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7b9d5c",
   "metadata": {},
   "source": [
    "## Train simple ANN Classifier on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1801385d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250, Train Loss: 0.7711, Val Loss: 0.6937, Train Acc: 49.20%, Val Acc: 47.00%\n",
      "Epoch 2/250, Train Loss: 0.7470, Val Loss: 0.6950, Train Acc: 51.20%, Val Acc: 49.00%\n",
      "Epoch 3/250, Train Loss: 0.6977, Val Loss: 0.6989, Train Acc: 53.00%, Val Acc: 51.50%\n",
      "Epoch 4/250, Train Loss: 0.6840, Val Loss: 0.6943, Train Acc: 58.40%, Val Acc: 52.50%\n",
      "Epoch 5/250, Train Loss: 0.6520, Val Loss: 0.6863, Train Acc: 62.20%, Val Acc: 57.00%\n",
      "Epoch 6/250, Train Loss: 0.6225, Val Loss: 0.6774, Train Acc: 66.20%, Val Acc: 56.50%\n",
      "Epoch 7/250, Train Loss: 0.6213, Val Loss: 0.6680, Train Acc: 64.00%, Val Acc: 59.50%\n",
      "Epoch 8/250, Train Loss: 0.6114, Val Loss: 0.6633, Train Acc: 66.40%, Val Acc: 61.00%\n",
      "Epoch 9/250, Train Loss: 0.5696, Val Loss: 0.6558, Train Acc: 70.20%, Val Acc: 62.00%\n",
      "Epoch 10/250, Train Loss: 0.5481, Val Loss: 0.6511, Train Acc: 74.40%, Val Acc: 63.50%\n",
      "Epoch 11/250, Train Loss: 0.5379, Val Loss: 0.6450, Train Acc: 73.00%, Val Acc: 63.50%\n",
      "Epoch 12/250, Train Loss: 0.5249, Val Loss: 0.6436, Train Acc: 77.80%, Val Acc: 65.50%\n",
      "Epoch 13/250, Train Loss: 0.5190, Val Loss: 0.6398, Train Acc: 76.20%, Val Acc: 64.00%\n",
      "Epoch 14/250, Train Loss: 0.5338, Val Loss: 0.6350, Train Acc: 76.00%, Val Acc: 66.50%\n",
      "Epoch 15/250, Train Loss: 0.5162, Val Loss: 0.6303, Train Acc: 78.60%, Val Acc: 65.00%\n",
      "Epoch 16/250, Train Loss: 0.5077, Val Loss: 0.6290, Train Acc: 79.20%, Val Acc: 63.00%\n",
      "Epoch 17/250, Train Loss: 0.4931, Val Loss: 0.6284, Train Acc: 82.80%, Val Acc: 65.50%\n",
      "Epoch 18/250, Train Loss: 0.4943, Val Loss: 0.6229, Train Acc: 78.00%, Val Acc: 66.00%\n",
      "Epoch 19/250, Train Loss: 0.4646, Val Loss: 0.6187, Train Acc: 83.80%, Val Acc: 67.00%\n",
      "Epoch 20/250, Train Loss: 0.4736, Val Loss: 0.6192, Train Acc: 84.00%, Val Acc: 67.00%\n",
      "Epoch 21/250, Train Loss: 0.4731, Val Loss: 0.6172, Train Acc: 84.00%, Val Acc: 68.00%\n",
      "Epoch 22/250, Train Loss: 0.4793, Val Loss: 0.6098, Train Acc: 83.60%, Val Acc: 68.50%\n",
      "Epoch 23/250, Train Loss: 0.4504, Val Loss: 0.6050, Train Acc: 82.60%, Val Acc: 68.00%\n",
      "Epoch 24/250, Train Loss: 0.4537, Val Loss: 0.6072, Train Acc: 83.60%, Val Acc: 69.50%\n",
      "Epoch 25/250, Train Loss: 0.4493, Val Loss: 0.6021, Train Acc: 84.00%, Val Acc: 68.00%\n",
      "Epoch 26/250, Train Loss: 0.4439, Val Loss: 0.6003, Train Acc: 83.00%, Val Acc: 68.50%\n",
      "Epoch 27/250, Train Loss: 0.4315, Val Loss: 0.5949, Train Acc: 85.60%, Val Acc: 69.50%\n",
      "Epoch 28/250, Train Loss: 0.4450, Val Loss: 0.5942, Train Acc: 85.40%, Val Acc: 68.50%\n",
      "Epoch 29/250, Train Loss: 0.4314, Val Loss: 0.5920, Train Acc: 85.60%, Val Acc: 71.50%\n",
      "Epoch 30/250, Train Loss: 0.4260, Val Loss: 0.5905, Train Acc: 85.40%, Val Acc: 71.00%\n",
      "Epoch 31/250, Train Loss: 0.4418, Val Loss: 0.5884, Train Acc: 85.40%, Val Acc: 69.50%\n",
      "Epoch 32/250, Train Loss: 0.4454, Val Loss: 0.5928, Train Acc: 85.40%, Val Acc: 73.00%\n",
      "Epoch 33/250, Train Loss: 0.4114, Val Loss: 0.5921, Train Acc: 87.20%, Val Acc: 73.00%\n",
      "Epoch 34/250, Train Loss: 0.4244, Val Loss: 0.5891, Train Acc: 84.40%, Val Acc: 72.00%\n",
      "Epoch 35/250, Train Loss: 0.4426, Val Loss: 0.5862, Train Acc: 86.40%, Val Acc: 70.00%\n",
      "Epoch 36/250, Train Loss: 0.4115, Val Loss: 0.5835, Train Acc: 85.60%, Val Acc: 69.50%\n",
      "Epoch 37/250, Train Loss: 0.4038, Val Loss: 0.5835, Train Acc: 86.20%, Val Acc: 70.50%\n",
      "Epoch 38/250, Train Loss: 0.4065, Val Loss: 0.5802, Train Acc: 85.40%, Val Acc: 69.50%\n",
      "Epoch 39/250, Train Loss: 0.3958, Val Loss: 0.5753, Train Acc: 89.60%, Val Acc: 70.00%\n",
      "Epoch 40/250, Train Loss: 0.3862, Val Loss: 0.5764, Train Acc: 87.60%, Val Acc: 69.00%\n",
      "Epoch 41/250, Train Loss: 0.3923, Val Loss: 0.5732, Train Acc: 86.40%, Val Acc: 69.50%\n",
      "Epoch 42/250, Train Loss: 0.4046, Val Loss: 0.5720, Train Acc: 87.20%, Val Acc: 69.50%\n",
      "Epoch 43/250, Train Loss: 0.4148, Val Loss: 0.5730, Train Acc: 85.60%, Val Acc: 72.00%\n",
      "Epoch 44/250, Train Loss: 0.4057, Val Loss: 0.5685, Train Acc: 85.00%, Val Acc: 70.00%\n",
      "Epoch 45/250, Train Loss: 0.4086, Val Loss: 0.5668, Train Acc: 86.00%, Val Acc: 72.00%\n",
      "Epoch 46/250, Train Loss: 0.3984, Val Loss: 0.5696, Train Acc: 87.40%, Val Acc: 72.50%\n",
      "Epoch 47/250, Train Loss: 0.3927, Val Loss: 0.5631, Train Acc: 85.80%, Val Acc: 71.50%\n",
      "Epoch 48/250, Train Loss: 0.4015, Val Loss: 0.5643, Train Acc: 86.80%, Val Acc: 72.50%\n",
      "Epoch 49/250, Train Loss: 0.3821, Val Loss: 0.5610, Train Acc: 85.80%, Val Acc: 72.50%\n",
      "Epoch 50/250, Train Loss: 0.3817, Val Loss: 0.5642, Train Acc: 88.80%, Val Acc: 73.00%\n",
      "Epoch 51/250, Train Loss: 0.3927, Val Loss: 0.5613, Train Acc: 85.00%, Val Acc: 72.00%\n",
      "Epoch 52/250, Train Loss: 0.3681, Val Loss: 0.5584, Train Acc: 86.20%, Val Acc: 73.00%\n",
      "Epoch 53/250, Train Loss: 0.3777, Val Loss: 0.5595, Train Acc: 84.40%, Val Acc: 72.00%\n",
      "Epoch 54/250, Train Loss: 0.3594, Val Loss: 0.5570, Train Acc: 85.40%, Val Acc: 72.00%\n",
      "Epoch 55/250, Train Loss: 0.3704, Val Loss: 0.5546, Train Acc: 87.40%, Val Acc: 72.00%\n",
      "Epoch 56/250, Train Loss: 0.3664, Val Loss: 0.5545, Train Acc: 85.60%, Val Acc: 72.50%\n",
      "Epoch 57/250, Train Loss: 0.3811, Val Loss: 0.5533, Train Acc: 85.20%, Val Acc: 72.50%\n",
      "Epoch 58/250, Train Loss: 0.3568, Val Loss: 0.5536, Train Acc: 86.20%, Val Acc: 72.50%\n",
      "Epoch 59/250, Train Loss: 0.3662, Val Loss: 0.5529, Train Acc: 86.60%, Val Acc: 71.50%\n",
      "Epoch 60/250, Train Loss: 0.3676, Val Loss: 0.5511, Train Acc: 85.40%, Val Acc: 71.50%\n",
      "Epoch 61/250, Train Loss: 0.3900, Val Loss: 0.5497, Train Acc: 83.20%, Val Acc: 72.00%\n",
      "Epoch 62/250, Train Loss: 0.3624, Val Loss: 0.5470, Train Acc: 87.00%, Val Acc: 72.50%\n",
      "Epoch 63/250, Train Loss: 0.3377, Val Loss: 0.5501, Train Acc: 86.00%, Val Acc: 73.00%\n",
      "Epoch 64/250, Train Loss: 0.3472, Val Loss: 0.5475, Train Acc: 86.60%, Val Acc: 73.00%\n",
      "Epoch 65/250, Train Loss: 0.3404, Val Loss: 0.5446, Train Acc: 88.80%, Val Acc: 73.00%\n",
      "Epoch 66/250, Train Loss: 0.3529, Val Loss: 0.5438, Train Acc: 87.60%, Val Acc: 73.00%\n",
      "Epoch 67/250, Train Loss: 0.3363, Val Loss: 0.5435, Train Acc: 87.00%, Val Acc: 73.00%\n",
      "Epoch 68/250, Train Loss: 0.3716, Val Loss: 0.5410, Train Acc: 84.80%, Val Acc: 73.50%\n",
      "Epoch 69/250, Train Loss: 0.3781, Val Loss: 0.5396, Train Acc: 81.80%, Val Acc: 72.50%\n",
      "Epoch 70/250, Train Loss: 0.3822, Val Loss: 0.5442, Train Acc: 83.00%, Val Acc: 73.00%\n",
      "Epoch 71/250, Train Loss: 0.3469, Val Loss: 0.5401, Train Acc: 84.20%, Val Acc: 74.00%\n",
      "Epoch 72/250, Train Loss: 0.3596, Val Loss: 0.5345, Train Acc: 86.40%, Val Acc: 71.50%\n",
      "Epoch 73/250, Train Loss: 0.3471, Val Loss: 0.5349, Train Acc: 86.20%, Val Acc: 73.50%\n",
      "Epoch 74/250, Train Loss: 0.3512, Val Loss: 0.5334, Train Acc: 87.40%, Val Acc: 71.50%\n",
      "Epoch 75/250, Train Loss: 0.3603, Val Loss: 0.5390, Train Acc: 85.80%, Val Acc: 72.50%\n",
      "Epoch 76/250, Train Loss: 0.3142, Val Loss: 0.5313, Train Acc: 86.80%, Val Acc: 73.50%\n",
      "Epoch 77/250, Train Loss: 0.3350, Val Loss: 0.5304, Train Acc: 86.60%, Val Acc: 73.50%\n",
      "Epoch 78/250, Train Loss: 0.3312, Val Loss: 0.5288, Train Acc: 87.40%, Val Acc: 73.00%\n",
      "Epoch 79/250, Train Loss: 0.3417, Val Loss: 0.5260, Train Acc: 86.80%, Val Acc: 74.00%\n",
      "Epoch 80/250, Train Loss: 0.3498, Val Loss: 0.5275, Train Acc: 85.20%, Val Acc: 73.00%\n",
      "Epoch 81/250, Train Loss: 0.3376, Val Loss: 0.5278, Train Acc: 84.80%, Val Acc: 74.00%\n",
      "Epoch 82/250, Train Loss: 0.3423, Val Loss: 0.5269, Train Acc: 86.00%, Val Acc: 73.50%\n",
      "Epoch 83/250, Train Loss: 0.3314, Val Loss: 0.5291, Train Acc: 85.60%, Val Acc: 73.00%\n",
      "Epoch 84/250, Train Loss: 0.3330, Val Loss: 0.5278, Train Acc: 87.60%, Val Acc: 74.00%\n",
      "Epoch 85/250, Train Loss: 0.3079, Val Loss: 0.5263, Train Acc: 87.80%, Val Acc: 74.00%\n",
      "Epoch 86/250, Train Loss: 0.3232, Val Loss: 0.5289, Train Acc: 87.20%, Val Acc: 71.00%\n",
      "Epoch 87/250, Train Loss: 0.3262, Val Loss: 0.5262, Train Acc: 86.60%, Val Acc: 72.50%\n",
      "Epoch 88/250, Train Loss: 0.3266, Val Loss: 0.5251, Train Acc: 88.60%, Val Acc: 73.00%\n",
      "Epoch 89/250, Train Loss: 0.3064, Val Loss: 0.5246, Train Acc: 85.20%, Val Acc: 72.50%\n",
      "Epoch 90/250, Train Loss: 0.3176, Val Loss: 0.5202, Train Acc: 87.20%, Val Acc: 73.00%\n",
      "Epoch 91/250, Train Loss: 0.3063, Val Loss: 0.5224, Train Acc: 88.60%, Val Acc: 70.50%\n",
      "Epoch 92/250, Train Loss: 0.3011, Val Loss: 0.5242, Train Acc: 87.00%, Val Acc: 71.00%\n",
      "Epoch 93/250, Train Loss: 0.3037, Val Loss: 0.5205, Train Acc: 84.60%, Val Acc: 73.00%\n",
      "Epoch 94/250, Train Loss: 0.3175, Val Loss: 0.5188, Train Acc: 88.00%, Val Acc: 73.50%\n",
      "Epoch 95/250, Train Loss: 0.3254, Val Loss: 0.5246, Train Acc: 86.20%, Val Acc: 72.00%\n",
      "Epoch 96/250, Train Loss: 0.3173, Val Loss: 0.5196, Train Acc: 86.00%, Val Acc: 72.50%\n",
      "Epoch 97/250, Train Loss: 0.3134, Val Loss: 0.5194, Train Acc: 86.40%, Val Acc: 74.00%\n",
      "Epoch 98/250, Train Loss: 0.3004, Val Loss: 0.5212, Train Acc: 88.80%, Val Acc: 71.50%\n",
      "Epoch 99/250, Train Loss: 0.2882, Val Loss: 0.5195, Train Acc: 87.20%, Val Acc: 73.00%\n",
      "Epoch 100/250, Train Loss: 0.2928, Val Loss: 0.5203, Train Acc: 87.40%, Val Acc: 73.00%\n",
      "Epoch 101/250, Train Loss: 0.3254, Val Loss: 0.5192, Train Acc: 84.80%, Val Acc: 73.50%\n",
      "Epoch 102/250, Train Loss: 0.2911, Val Loss: 0.5193, Train Acc: 86.80%, Val Acc: 73.00%\n",
      "Epoch 103/250, Train Loss: 0.2999, Val Loss: 0.5210, Train Acc: 87.80%, Val Acc: 74.00%\n",
      "Epoch 104/250, Train Loss: 0.2775, Val Loss: 0.5183, Train Acc: 89.60%, Val Acc: 74.00%\n",
      "Epoch 105/250, Train Loss: 0.2797, Val Loss: 0.5167, Train Acc: 87.60%, Val Acc: 72.50%\n",
      "Epoch 106/250, Train Loss: 0.2951, Val Loss: 0.5176, Train Acc: 88.00%, Val Acc: 74.00%\n",
      "Epoch 107/250, Train Loss: 0.2943, Val Loss: 0.5166, Train Acc: 86.40%, Val Acc: 74.50%\n",
      "Epoch 108/250, Train Loss: 0.3167, Val Loss: 0.5158, Train Acc: 87.20%, Val Acc: 73.00%\n",
      "Epoch 109/250, Train Loss: 0.3029, Val Loss: 0.5157, Train Acc: 86.60%, Val Acc: 74.50%\n",
      "Epoch 110/250, Train Loss: 0.2627, Val Loss: 0.5141, Train Acc: 89.80%, Val Acc: 73.50%\n",
      "Epoch 111/250, Train Loss: 0.2806, Val Loss: 0.5142, Train Acc: 88.20%, Val Acc: 74.50%\n",
      "Epoch 112/250, Train Loss: 0.2806, Val Loss: 0.5142, Train Acc: 87.00%, Val Acc: 74.50%\n",
      "Epoch 113/250, Train Loss: 0.2935, Val Loss: 0.5136, Train Acc: 89.00%, Val Acc: 74.00%\n",
      "Epoch 114/250, Train Loss: 0.2626, Val Loss: 0.5109, Train Acc: 89.60%, Val Acc: 74.00%\n",
      "Epoch 115/250, Train Loss: 0.2850, Val Loss: 0.5105, Train Acc: 88.00%, Val Acc: 74.00%\n",
      "Epoch 116/250, Train Loss: 0.2642, Val Loss: 0.5072, Train Acc: 90.80%, Val Acc: 74.00%\n",
      "Epoch 117/250, Train Loss: 0.2975, Val Loss: 0.5069, Train Acc: 86.40%, Val Acc: 74.50%\n",
      "Epoch 118/250, Train Loss: 0.2554, Val Loss: 0.5084, Train Acc: 88.20%, Val Acc: 74.50%\n",
      "Epoch 119/250, Train Loss: 0.2795, Val Loss: 0.5085, Train Acc: 87.40%, Val Acc: 75.00%\n",
      "Epoch 120/250, Train Loss: 0.2988, Val Loss: 0.5074, Train Acc: 87.60%, Val Acc: 74.50%\n",
      "Epoch 121/250, Train Loss: 0.2629, Val Loss: 0.5056, Train Acc: 89.60%, Val Acc: 74.50%\n",
      "Epoch 122/250, Train Loss: 0.3038, Val Loss: 0.5047, Train Acc: 88.20%, Val Acc: 73.50%\n",
      "Epoch 123/250, Train Loss: 0.2732, Val Loss: 0.5053, Train Acc: 88.20%, Val Acc: 75.00%\n",
      "Epoch 124/250, Train Loss: 0.2957, Val Loss: 0.5023, Train Acc: 88.20%, Val Acc: 74.50%\n",
      "Epoch 125/250, Train Loss: 0.2621, Val Loss: 0.5046, Train Acc: 90.20%, Val Acc: 74.50%\n",
      "Epoch 126/250, Train Loss: 0.2756, Val Loss: 0.5045, Train Acc: 88.20%, Val Acc: 74.00%\n",
      "Epoch 127/250, Train Loss: 0.2693, Val Loss: 0.5121, Train Acc: 89.60%, Val Acc: 74.50%\n",
      "Epoch 128/250, Train Loss: 0.2787, Val Loss: 0.5044, Train Acc: 88.40%, Val Acc: 74.50%\n",
      "Epoch 129/250, Train Loss: 0.2710, Val Loss: 0.5042, Train Acc: 89.60%, Val Acc: 75.00%\n",
      "Epoch 130/250, Train Loss: 0.2957, Val Loss: 0.5028, Train Acc: 88.80%, Val Acc: 76.00%\n",
      "Epoch 131/250, Train Loss: 0.2371, Val Loss: 0.5023, Train Acc: 90.60%, Val Acc: 75.50%\n",
      "Epoch 132/250, Train Loss: 0.2614, Val Loss: 0.5020, Train Acc: 90.40%, Val Acc: 75.50%\n",
      "Epoch 133/250, Train Loss: 0.2625, Val Loss: 0.5005, Train Acc: 90.00%, Val Acc: 74.50%\n",
      "Epoch 134/250, Train Loss: 0.2428, Val Loss: 0.5015, Train Acc: 90.80%, Val Acc: 75.00%\n",
      "Epoch 135/250, Train Loss: 0.2458, Val Loss: 0.5029, Train Acc: 91.40%, Val Acc: 75.50%\n",
      "Epoch 136/250, Train Loss: 0.2542, Val Loss: 0.5030, Train Acc: 90.40%, Val Acc: 75.50%\n",
      "Epoch 137/250, Train Loss: 0.2576, Val Loss: 0.5027, Train Acc: 89.40%, Val Acc: 74.50%\n",
      "Epoch 138/250, Train Loss: 0.2630, Val Loss: 0.5025, Train Acc: 89.20%, Val Acc: 74.50%\n",
      "Epoch 139/250, Train Loss: 0.2824, Val Loss: 0.5007, Train Acc: 88.00%, Val Acc: 75.00%\n",
      "Epoch 140/250, Train Loss: 0.2486, Val Loss: 0.5010, Train Acc: 91.00%, Val Acc: 74.50%\n",
      "Epoch 141/250, Train Loss: 0.2407, Val Loss: 0.5015, Train Acc: 90.60%, Val Acc: 75.50%\n",
      "Epoch 142/250, Train Loss: 0.2561, Val Loss: 0.5021, Train Acc: 89.40%, Val Acc: 74.50%\n",
      "Epoch 143/250, Train Loss: 0.2448, Val Loss: 0.5032, Train Acc: 89.60%, Val Acc: 74.50%\n",
      "Epoch 144/250, Train Loss: 0.2528, Val Loss: 0.5012, Train Acc: 89.00%, Val Acc: 75.00%\n",
      "Epoch 145/250, Train Loss: 0.2722, Val Loss: 0.5015, Train Acc: 88.80%, Val Acc: 74.00%\n",
      "Epoch 146/250, Train Loss: 0.2674, Val Loss: 0.4983, Train Acc: 87.00%, Val Acc: 75.00%\n",
      "Epoch 147/250, Train Loss: 0.2717, Val Loss: 0.5003, Train Acc: 89.00%, Val Acc: 74.50%\n",
      "Epoch 148/250, Train Loss: 0.2492, Val Loss: 0.5007, Train Acc: 88.00%, Val Acc: 74.00%\n",
      "Epoch 149/250, Train Loss: 0.2645, Val Loss: 0.5084, Train Acc: 88.40%, Val Acc: 75.50%\n",
      "Epoch 150/250, Train Loss: 0.2457, Val Loss: 0.5018, Train Acc: 90.80%, Val Acc: 74.50%\n",
      "Epoch 151/250, Train Loss: 0.2473, Val Loss: 0.5018, Train Acc: 91.20%, Val Acc: 74.50%\n",
      "Epoch 152/250, Train Loss: 0.2565, Val Loss: 0.5105, Train Acc: 89.40%, Val Acc: 74.00%\n",
      "Epoch 153/250, Train Loss: 0.2591, Val Loss: 0.5430, Train Acc: 89.20%, Val Acc: 73.00%\n",
      "Epoch 154/250, Train Loss: 0.2518, Val Loss: 0.4993, Train Acc: 88.80%, Val Acc: 75.00%\n",
      "Epoch 155/250, Train Loss: 0.2643, Val Loss: 0.5095, Train Acc: 90.80%, Val Acc: 73.50%\n",
      "Epoch 156/250, Train Loss: 0.2680, Val Loss: 0.5008, Train Acc: 89.80%, Val Acc: 75.00%\n",
      "Epoch 157/250, Train Loss: 0.2453, Val Loss: 0.4985, Train Acc: 88.60%, Val Acc: 75.50%\n",
      "Epoch 158/250, Train Loss: 0.2557, Val Loss: 0.4946, Train Acc: 89.80%, Val Acc: 74.50%\n",
      "Epoch 159/250, Train Loss: 0.2306, Val Loss: 0.5025, Train Acc: 91.60%, Val Acc: 76.00%\n",
      "Epoch 160/250, Train Loss: 0.2558, Val Loss: 0.5022, Train Acc: 88.80%, Val Acc: 75.50%\n",
      "Epoch 161/250, Train Loss: 0.2378, Val Loss: 0.5048, Train Acc: 90.00%, Val Acc: 75.50%\n",
      "Epoch 162/250, Train Loss: 0.2383, Val Loss: 0.4970, Train Acc: 89.80%, Val Acc: 76.00%\n",
      "Epoch 163/250, Train Loss: 0.2353, Val Loss: 0.4977, Train Acc: 90.40%, Val Acc: 76.00%\n",
      "Epoch 164/250, Train Loss: 0.2884, Val Loss: 0.5040, Train Acc: 89.20%, Val Acc: 75.50%\n",
      "Epoch 165/250, Train Loss: 0.2428, Val Loss: 0.4989, Train Acc: 90.80%, Val Acc: 76.00%\n",
      "Epoch 166/250, Train Loss: 0.2276, Val Loss: 0.4934, Train Acc: 88.60%, Val Acc: 75.50%\n",
      "Epoch 167/250, Train Loss: 0.2392, Val Loss: 0.4967, Train Acc: 89.60%, Val Acc: 75.50%\n",
      "Epoch 168/250, Train Loss: 0.2473, Val Loss: 0.4955, Train Acc: 89.20%, Val Acc: 76.50%\n",
      "Epoch 169/250, Train Loss: 0.2354, Val Loss: 0.4946, Train Acc: 87.40%, Val Acc: 77.00%\n",
      "Epoch 170/250, Train Loss: 0.2626, Val Loss: 0.4926, Train Acc: 90.80%, Val Acc: 76.00%\n",
      "Epoch 171/250, Train Loss: 0.2443, Val Loss: 0.4930, Train Acc: 90.20%, Val Acc: 75.50%\n",
      "Epoch 172/250, Train Loss: 0.2640, Val Loss: 0.4940, Train Acc: 88.00%, Val Acc: 76.50%\n",
      "Epoch 173/250, Train Loss: 0.2355, Val Loss: 0.4980, Train Acc: 90.00%, Val Acc: 75.00%\n",
      "Epoch 174/250, Train Loss: 0.2189, Val Loss: 0.4938, Train Acc: 91.60%, Val Acc: 76.00%\n",
      "Epoch 175/250, Train Loss: 0.2431, Val Loss: 0.4954, Train Acc: 91.80%, Val Acc: 74.50%\n",
      "Epoch 176/250, Train Loss: 0.2500, Val Loss: 0.4933, Train Acc: 88.20%, Val Acc: 76.00%\n",
      "Epoch 177/250, Train Loss: 0.2361, Val Loss: 0.4928, Train Acc: 92.20%, Val Acc: 76.00%\n",
      "Epoch 178/250, Train Loss: 0.2495, Val Loss: 0.4931, Train Acc: 89.40%, Val Acc: 77.00%\n",
      "Epoch 179/250, Train Loss: 0.2435, Val Loss: 0.4971, Train Acc: 89.20%, Val Acc: 75.00%\n",
      "Epoch 180/250, Train Loss: 0.2410, Val Loss: 0.4925, Train Acc: 89.20%, Val Acc: 76.50%\n",
      "Epoch 181/250, Train Loss: 0.2182, Val Loss: 0.4946, Train Acc: 91.40%, Val Acc: 77.00%\n",
      "Epoch 182/250, Train Loss: 0.2120, Val Loss: 0.4967, Train Acc: 89.60%, Val Acc: 77.00%\n",
      "Epoch 183/250, Train Loss: 0.2428, Val Loss: 0.5004, Train Acc: 89.20%, Val Acc: 76.50%\n",
      "Epoch 184/250, Train Loss: 0.2142, Val Loss: 0.4975, Train Acc: 91.40%, Val Acc: 77.50%\n",
      "Epoch 185/250, Train Loss: 0.2619, Val Loss: 0.5089, Train Acc: 91.00%, Val Acc: 74.50%\n",
      "Epoch 186/250, Train Loss: 0.2431, Val Loss: 0.5077, Train Acc: 90.20%, Val Acc: 74.50%\n",
      "Epoch 187/250, Train Loss: 0.2364, Val Loss: 0.5051, Train Acc: 89.80%, Val Acc: 75.50%\n",
      "Epoch 188/250, Train Loss: 0.2310, Val Loss: 0.5064, Train Acc: 90.00%, Val Acc: 76.00%\n",
      "Epoch 189/250, Train Loss: 0.2114, Val Loss: 0.5045, Train Acc: 93.20%, Val Acc: 77.00%\n",
      "Epoch 190/250, Train Loss: 0.2232, Val Loss: 0.5016, Train Acc: 90.20%, Val Acc: 77.00%\n",
      "Epoch 191/250, Train Loss: 0.2091, Val Loss: 0.5013, Train Acc: 92.60%, Val Acc: 76.50%\n",
      "Epoch 192/250, Train Loss: 0.2309, Val Loss: 0.5016, Train Acc: 89.40%, Val Acc: 75.50%\n",
      "Epoch 193/250, Train Loss: 0.2410, Val Loss: 0.5020, Train Acc: 90.40%, Val Acc: 75.00%\n",
      "Epoch 194/250, Train Loss: 0.2261, Val Loss: 0.5044, Train Acc: 93.60%, Val Acc: 75.50%\n",
      "Epoch 195/250, Train Loss: 0.2513, Val Loss: 0.5051, Train Acc: 91.60%, Val Acc: 75.50%\n",
      "Epoch 196/250, Train Loss: 0.2272, Val Loss: 0.5029, Train Acc: 91.60%, Val Acc: 76.50%\n",
      "Epoch 197/250, Train Loss: 0.2245, Val Loss: 0.5176, Train Acc: 90.60%, Val Acc: 75.00%\n",
      "Epoch 198/250, Train Loss: 0.2223, Val Loss: 0.5048, Train Acc: 92.20%, Val Acc: 76.50%\n",
      "Epoch 199/250, Train Loss: 0.2456, Val Loss: 0.5066, Train Acc: 88.00%, Val Acc: 76.50%\n",
      "Epoch 200/250, Train Loss: 0.2389, Val Loss: 0.5133, Train Acc: 90.00%, Val Acc: 75.00%\n",
      "Epoch 201/250, Train Loss: 0.2306, Val Loss: 0.5010, Train Acc: 91.20%, Val Acc: 76.50%\n",
      "Epoch 202/250, Train Loss: 0.2242, Val Loss: 0.5033, Train Acc: 90.80%, Val Acc: 76.00%\n",
      "Epoch 203/250, Train Loss: 0.2333, Val Loss: 0.5033, Train Acc: 90.80%, Val Acc: 77.00%\n",
      "Epoch 204/250, Train Loss: 0.2089, Val Loss: 0.4992, Train Acc: 92.40%, Val Acc: 77.00%\n",
      "Epoch 205/250, Train Loss: 0.2259, Val Loss: 0.4979, Train Acc: 91.60%, Val Acc: 77.00%\n",
      "Epoch 206/250, Train Loss: 0.2160, Val Loss: 0.5012, Train Acc: 90.60%, Val Acc: 77.50%\n",
      "Epoch 207/250, Train Loss: 0.2073, Val Loss: 0.5104, Train Acc: 94.40%, Val Acc: 75.00%\n",
      "Epoch 208/250, Train Loss: 0.2211, Val Loss: 0.5024, Train Acc: 90.60%, Val Acc: 77.00%\n",
      "Epoch 209/250, Train Loss: 0.2231, Val Loss: 0.5106, Train Acc: 90.60%, Val Acc: 75.00%\n",
      "Epoch 210/250, Train Loss: 0.2827, Val Loss: 0.5303, Train Acc: 88.80%, Val Acc: 75.00%\n",
      "Epoch 211/250, Train Loss: 0.2096, Val Loss: 0.5056, Train Acc: 91.40%, Val Acc: 77.00%\n",
      "Epoch 212/250, Train Loss: 0.2119, Val Loss: 0.5065, Train Acc: 91.20%, Val Acc: 75.50%\n",
      "Epoch 213/250, Train Loss: 0.2191, Val Loss: 0.5069, Train Acc: 91.60%, Val Acc: 76.00%\n",
      "Epoch 214/250, Train Loss: 0.2037, Val Loss: 0.5124, Train Acc: 92.00%, Val Acc: 76.00%\n",
      "Epoch 215/250, Train Loss: 0.1977, Val Loss: 0.5062, Train Acc: 91.00%, Val Acc: 77.00%\n",
      "Epoch 216/250, Train Loss: 0.2097, Val Loss: 0.5108, Train Acc: 90.60%, Val Acc: 77.00%\n",
      "Epoch 217/250, Train Loss: 0.2035, Val Loss: 0.5114, Train Acc: 92.20%, Val Acc: 76.00%\n",
      "Epoch 218/250, Train Loss: 0.2335, Val Loss: 0.5212, Train Acc: 91.80%, Val Acc: 76.00%\n",
      "Epoch 219/250, Train Loss: 0.2449, Val Loss: 0.5247, Train Acc: 90.00%, Val Acc: 76.50%\n",
      "Epoch 220/250, Train Loss: 0.2379, Val Loss: 0.5136, Train Acc: 90.60%, Val Acc: 77.00%\n",
      "Epoch 221/250, Train Loss: 0.2435, Val Loss: 0.5166, Train Acc: 91.60%, Val Acc: 75.50%\n",
      "Epoch 222/250, Train Loss: 0.2116, Val Loss: 0.5150, Train Acc: 90.40%, Val Acc: 77.00%\n",
      "Epoch 223/250, Train Loss: 0.2169, Val Loss: 0.5201, Train Acc: 91.80%, Val Acc: 75.50%\n",
      "Epoch 224/250, Train Loss: 0.2108, Val Loss: 0.5140, Train Acc: 92.20%, Val Acc: 77.00%\n",
      "Epoch 225/250, Train Loss: 0.1988, Val Loss: 0.5200, Train Acc: 92.00%, Val Acc: 76.50%\n",
      "Epoch 226/250, Train Loss: 0.2544, Val Loss: 0.5270, Train Acc: 91.20%, Val Acc: 76.50%\n",
      "Epoch 227/250, Train Loss: 0.2206, Val Loss: 0.5223, Train Acc: 92.00%, Val Acc: 74.50%\n",
      "Epoch 228/250, Train Loss: 0.2090, Val Loss: 0.5205, Train Acc: 91.60%, Val Acc: 75.50%\n",
      "Epoch 229/250, Train Loss: 0.1830, Val Loss: 0.5106, Train Acc: 93.00%, Val Acc: 76.00%\n",
      "Epoch 230/250, Train Loss: 0.2060, Val Loss: 0.5174, Train Acc: 91.40%, Val Acc: 75.00%\n",
      "Epoch 231/250, Train Loss: 0.2184, Val Loss: 0.5236, Train Acc: 91.00%, Val Acc: 77.00%\n",
      "Epoch 232/250, Train Loss: 0.2037, Val Loss: 0.5144, Train Acc: 92.00%, Val Acc: 76.00%\n",
      "Epoch 233/250, Train Loss: 0.1965, Val Loss: 0.5221, Train Acc: 92.20%, Val Acc: 75.50%\n",
      "Epoch 234/250, Train Loss: 0.2089, Val Loss: 0.5208, Train Acc: 90.80%, Val Acc: 74.50%\n",
      "Epoch 235/250, Train Loss: 0.1973, Val Loss: 0.5176, Train Acc: 92.60%, Val Acc: 75.50%\n",
      "Epoch 236/250, Train Loss: 0.2386, Val Loss: 0.5273, Train Acc: 88.20%, Val Acc: 77.00%\n",
      "Epoch 237/250, Train Loss: 0.1890, Val Loss: 0.5226, Train Acc: 93.00%, Val Acc: 74.50%\n",
      "Epoch 238/250, Train Loss: 0.2235, Val Loss: 0.5335, Train Acc: 92.40%, Val Acc: 74.00%\n",
      "Epoch 239/250, Train Loss: 0.2442, Val Loss: 0.5344, Train Acc: 92.00%, Val Acc: 76.00%\n",
      "Epoch 240/250, Train Loss: 0.2060, Val Loss: 0.5289, Train Acc: 91.60%, Val Acc: 76.00%\n",
      "Epoch 241/250, Train Loss: 0.2036, Val Loss: 0.5325, Train Acc: 91.00%, Val Acc: 74.50%\n",
      "Epoch 242/250, Train Loss: 0.2059, Val Loss: 0.5254, Train Acc: 93.00%, Val Acc: 75.50%\n",
      "Epoch 243/250, Train Loss: 0.2039, Val Loss: 0.5327, Train Acc: 91.40%, Val Acc: 75.50%\n",
      "Epoch 244/250, Train Loss: 0.2311, Val Loss: 0.5319, Train Acc: 90.60%, Val Acc: 76.00%\n",
      "Epoch 245/250, Train Loss: 0.1855, Val Loss: 0.5366, Train Acc: 91.00%, Val Acc: 75.50%\n",
      "Epoch 246/250, Train Loss: 0.1780, Val Loss: 0.5437, Train Acc: 93.80%, Val Acc: 75.00%\n",
      "Epoch 247/250, Train Loss: 0.2117, Val Loss: 0.5380, Train Acc: 93.00%, Val Acc: 76.50%\n",
      "Epoch 248/250, Train Loss: 0.1956, Val Loss: 0.5348, Train Acc: 92.20%, Val Acc: 75.50%\n",
      "Epoch 249/250, Train Loss: 0.2262, Val Loss: 0.5369, Train Acc: 91.80%, Val Acc: 75.50%\n",
      "Epoch 250/250, Train Loss: 0.1945, Val Loss: 0.5291, Train Acc: 93.00%, Val Acc: 76.50%\n",
      "Best validation accuracy: 77.5%\n"
     ]
    }
   ],
   "source": [
    "model_name ='amazonclasssimple'\n",
    "input_size = next(iter(train_loader))[0].shape[1]\n",
    "model = SimpleANNClassifier(input_size, model_name)\n",
    "model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "trained_model = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b668100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing dataset: 81.33333333333333%\n",
      "F1-Score: 0.82\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load best model\n",
    "model = SimpleANNClassifier(input_size, model_name)\n",
    "model.load_state_dict(torch.load(model.name, weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate best model\n",
    "_, test_acc, labels, preds = test_model(model=model, loader=test_loader, criterion=criterion, input_size=input_size)\n",
    "print(f'Accuracy on testing dataset: {test_acc}%')\n",
    "print(f'F1-Score: {f1_score(labels, preds):.2f}')\n",
    "plot_confusion_matrix(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b2e79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train simple ANN Classifier on large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c5e2b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train Loss: 0.6273, Val Loss: 0.5445, Train Acc: 67.80%, Val Acc: 84.24%\n",
      "Epoch 2/25, Train Loss: 0.5471, Val Loss: 0.4964, Train Acc: 77.66%, Val Acc: 87.00%\n",
      "Epoch 3/25, Train Loss: 0.4963, Val Loss: 0.4544, Train Acc: 80.17%, Val Acc: 87.38%\n",
      "Epoch 4/25, Train Loss: 0.4557, Val Loss: 0.4208, Train Acc: 82.15%, Val Acc: 88.08%\n",
      "Epoch 5/25, Train Loss: 0.4209, Val Loss: 0.3871, Train Acc: 83.45%, Val Acc: 87.34%\n",
      "Epoch 6/25, Train Loss: 0.3936, Val Loss: 0.3646, Train Acc: 84.04%, Val Acc: 88.14%\n",
      "Epoch 7/25, Train Loss: 0.3680, Val Loss: 0.3455, Train Acc: 84.54%, Val Acc: 88.36%\n",
      "Epoch 8/25, Train Loss: 0.3541, Val Loss: 0.3326, Train Acc: 84.45%, Val Acc: 88.42%\n",
      "Epoch 9/25, Train Loss: 0.3371, Val Loss: 0.3237, Train Acc: 84.96%, Val Acc: 88.56%\n",
      "Epoch 10/25, Train Loss: 0.3225, Val Loss: 0.3084, Train Acc: 85.53%, Val Acc: 88.28%\n",
      "Epoch 11/25, Train Loss: 0.3089, Val Loss: 0.2996, Train Acc: 85.65%, Val Acc: 88.38%\n",
      "Epoch 12/25, Train Loss: 0.2946, Val Loss: 0.2949, Train Acc: 86.03%, Val Acc: 88.32%\n",
      "Epoch 13/25, Train Loss: 0.2837, Val Loss: 0.2911, Train Acc: 87.00%, Val Acc: 88.30%\n",
      "Epoch 14/25, Train Loss: 0.2804, Val Loss: 0.2945, Train Acc: 86.26%, Val Acc: 87.88%\n",
      "Epoch 15/25, Train Loss: 0.2695, Val Loss: 0.2895, Train Acc: 86.60%, Val Acc: 88.08%\n",
      "Epoch 16/25, Train Loss: 0.2583, Val Loss: 0.2873, Train Acc: 87.34%, Val Acc: 88.38%\n",
      "Epoch 17/25, Train Loss: 0.2502, Val Loss: 0.2886, Train Acc: 87.55%, Val Acc: 88.34%\n",
      "Epoch 18/25, Train Loss: 0.2533, Val Loss: 0.2959, Train Acc: 87.14%, Val Acc: 88.32%\n",
      "Epoch 19/25, Train Loss: 0.2449, Val Loss: 0.2976, Train Acc: 87.28%, Val Acc: 88.60%\n",
      "Epoch 20/25, Train Loss: 0.2414, Val Loss: 0.3007, Train Acc: 87.46%, Val Acc: 88.22%\n",
      "Epoch 21/25, Train Loss: 0.2385, Val Loss: 0.3062, Train Acc: 87.46%, Val Acc: 88.06%\n",
      "Epoch 22/25, Train Loss: 0.2279, Val Loss: 0.3148, Train Acc: 88.30%, Val Acc: 87.92%\n",
      "Epoch 23/25, Train Loss: 0.2345, Val Loss: 0.3214, Train Acc: 87.42%, Val Acc: 87.64%\n",
      "Epoch 24/25, Train Loss: 0.2274, Val Loss: 0.3288, Train Acc: 87.50%, Val Acc: 87.80%\n",
      "Epoch 25/25, Train Loss: 0.2212, Val Loss: 0.3287, Train Acc: 88.46%, Val Acc: 87.74%\n",
      "Best validation accuracy: 88.6%\n"
     ]
    }
   ],
   "source": [
    "model_name ='amazonclasssimplelarge'\n",
    "input_size = next(iter(train_loader_l))[0].shape[1]\n",
    "model = SimpleANNClassifier(input_size, model_name)\n",
    "model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "trained_model = train_model(model, criterion, optimizer, train_loader_l, val_loader_l, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95cf86e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing dataset: 88.24%\n",
      "F1-Score: 0.90\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load best model\n",
    "model = SimpleANNClassifier(input_size, model_name)\n",
    "model.load_state_dict(torch.load(model.name, weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate best model\n",
    "_, test_acc, labels, preds = test_model(model=model, loader=test_loader_l, criterion=criterion, input_size=input_size)\n",
    "print(f'Accuracy on testing dataset: {test_acc}%')\n",
    "print(f'F1-Score: {f1_score(labels, preds):.2f}')\n",
    "plot_confusion_matrix(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a71ff4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train ANN Classifier on large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "67ea571b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train Loss: 0.6000, Val Loss: 0.4585, Train Acc: 67.78%, Val Acc: 83.08%\n",
      "Epoch 2/25, Train Loss: 0.4509, Val Loss: 0.3651, Train Acc: 80.27%, Val Acc: 86.40%\n",
      "Epoch 3/25, Train Loss: 0.3333, Val Loss: 0.3285, Train Acc: 85.19%, Val Acc: 86.72%\n",
      "Epoch 4/25, Train Loss: 0.2571, Val Loss: 0.3200, Train Acc: 88.21%, Val Acc: 86.78%\n",
      "Epoch 5/25, Train Loss: 0.2186, Val Loss: 0.3339, Train Acc: 88.94%, Val Acc: 86.68%\n",
      "Epoch 6/25, Train Loss: 0.1961, Val Loss: 0.3592, Train Acc: 89.44%, Val Acc: 86.32%\n",
      "Epoch 7/25, Train Loss: 0.1960, Val Loss: 0.3935, Train Acc: 89.22%, Val Acc: 85.60%\n",
      "Epoch 8/25, Train Loss: 0.1901, Val Loss: 0.4099, Train Acc: 89.51%, Val Acc: 86.42%\n",
      "Epoch 9/25, Train Loss: 0.1889, Val Loss: 0.4536, Train Acc: 89.48%, Val Acc: 85.86%\n",
      "Epoch 10/25, Train Loss: 0.1941, Val Loss: 0.4556, Train Acc: 88.99%, Val Acc: 85.80%\n",
      "Epoch 11/25, Train Loss: 0.1857, Val Loss: 0.4490, Train Acc: 89.88%, Val Acc: 86.04%\n",
      "Epoch 12/25, Train Loss: 0.1850, Val Loss: 0.4486, Train Acc: 89.35%, Val Acc: 86.28%\n",
      "Epoch 13/25, Train Loss: 0.1877, Val Loss: 0.4900, Train Acc: 89.70%, Val Acc: 85.76%\n",
      "Epoch 14/25, Train Loss: 0.1756, Val Loss: 0.4916, Train Acc: 89.96%, Val Acc: 86.46%\n",
      "Epoch 15/25, Train Loss: 0.1816, Val Loss: 0.4861, Train Acc: 89.32%, Val Acc: 86.40%\n",
      "Epoch 16/25, Train Loss: 0.1708, Val Loss: 0.4970, Train Acc: 89.93%, Val Acc: 86.16%\n",
      "Epoch 17/25, Train Loss: 0.1741, Val Loss: 0.5557, Train Acc: 89.65%, Val Acc: 86.02%\n",
      "Epoch 18/25, Train Loss: 0.1778, Val Loss: 0.5441, Train Acc: 89.77%, Val Acc: 85.90%\n",
      "Epoch 19/25, Train Loss: 0.1772, Val Loss: 0.5402, Train Acc: 89.54%, Val Acc: 85.74%\n",
      "Epoch 20/25, Train Loss: 0.1792, Val Loss: 0.5616, Train Acc: 89.38%, Val Acc: 86.44%\n",
      "Epoch 21/25, Train Loss: 0.1721, Val Loss: 0.5694, Train Acc: 90.26%, Val Acc: 86.50%\n",
      "Epoch 22/25, Train Loss: 0.1683, Val Loss: 0.6451, Train Acc: 90.17%, Val Acc: 86.16%\n",
      "Epoch 23/25, Train Loss: 0.1700, Val Loss: 0.6331, Train Acc: 89.98%, Val Acc: 86.18%\n",
      "Epoch 24/25, Train Loss: 0.1669, Val Loss: 0.6374, Train Acc: 90.34%, Val Acc: 86.24%\n",
      "Epoch 25/25, Train Loss: 0.1738, Val Loss: 0.6249, Train Acc: 89.50%, Val Acc: 86.30%\n",
      "Best validation accuracy: 86.78%\n"
     ]
    }
   ],
   "source": [
    "model_name_l = 'amazonclasslarge'\n",
    "input_size = next(iter(train_loader_l))[0].shape[1]\n",
    "model = ANNClassifier(input_size, model_name_l)\n",
    "model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "\n",
    "trained_model = train_model(model, criterion, optimizer, train_loader_l, val_loader_l, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a6a8198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing dataset: 87.28%\n",
      "F1-Score: 0.90\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load best model\n",
    "model = ANNClassifier(input_size, model_name_l)\n",
    "model.load_state_dict(torch.load(model.name, weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate best model\n",
    "_, test_acc, labels, preds = test_model(model=model, loader=test_loader_l, criterion=criterion, input_size=input_size)\n",
    "print(f'Accuracy on testing dataset: {test_acc}%')\n",
    "print(f'F1-Score: {f1_score(labels, preds):.2f}')\n",
    "plot_confusion_matrix(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d52e41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train ANN Classifier on Amazon Electronics Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f8d39c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.3726, Val Loss: 0.3550, Train Acc: 83.49%, Val Acc: 84.30%\n",
      "Epoch 2/5, Train Loss: 0.3647, Val Loss: 0.3522, Train Acc: 83.91%, Val Acc: 84.51%\n",
      "Epoch 3/5, Train Loss: 0.3626, Val Loss: 0.3506, Train Acc: 84.01%, Val Acc: 84.53%\n",
      "Epoch 4/5, Train Loss: 0.3612, Val Loss: 0.3512, Train Acc: 84.09%, Val Acc: 84.47%\n",
      "Epoch 5/5, Train Loss: 0.3606, Val Loss: 0.3500, Train Acc: 84.12%, Val Acc: 84.57%\n",
      "Best validation accuracy: 84.57459557287628%\n"
     ]
    }
   ],
   "source": [
    "model_name_elec = 'amazonElectronics'\n",
    "input_size = next(iter(train_loader_elec))[0].shape[1]\n",
    "model = ANNClassifierLarge(input_size, model_name_elec)\n",
    "model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "\n",
    "# This isn't really great for performance, but we need to be a bit\n",
    "# more flexible with out data. Also, all of those operations are\n",
    "# performed on the GPU, so this should be fine ig\n",
    "def bin_mapper(labels):\n",
    "    return (labels >= 4).float()\n",
    "\n",
    "trained_model = train_model(model, criterion, optimizer, train_loader_elec, val_loader_elec, num_epochs=5, mod=bin_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32e87819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing dataset: 84.53893994079561%\n",
      "F1-Score: 0.91\n",
      "Precision-Score: 0.86\n",
      "Recall-Score: 0.97\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load best model\n",
    "model_name_elec = 'amazonElectronics'\n",
    "input_size = next(iter(train_loader_elec))[0].shape[1]\n",
    "model = ANNClassifierLarge(input_size, model_name_elec)\n",
    "model.load_state_dict(torch.load(model.name, weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "\n",
    "def bin_mapper(labels):\n",
    "    return (labels >= 4).float()\n",
    "\n",
    "# Evaluate best model\n",
    "_, test_acc, labels, preds = test_model(model=model, loader=test_loader_elec, criterion=criterion, input_size=input_size, mod=bin_mapper)\n",
    "print(f'Accuracy on testing dataset: {test_acc}%')\n",
    "print(f'F1-Score: {f1_score(labels, preds):.2f}')\n",
    "print(f'Precision-Score: {precision_score(labels, preds):.2f}')\n",
    "print(f'Recall-Score: {recall_score(labels, preds):.2f}')\n",
    "plot_confusion_matrix(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920172c",
   "metadata": {},
   "source": [
    "Reasoning: The amazon model is heavily biased towards TP. This is due to the provided dataset has a lot 4 and 5 star ratings:\n",
    "\n",
    "`{5.0: 4_322_520, 3.0: 504_712, 4.0: 1_137_229, 2.0: 306_659, 1.0: 467_117}`\n",
    "\n",
    "That might be the reason why the model has a lot of FP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ee583",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88581d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train ANN with word embeddings\n",
    "small dataset and small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "83e32c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Train Loss: 0.7357, Val Loss: 0.7018, Train Acc: 48.00%, Val Acc: 47.00%\n",
      "Epoch 2/500, Train Loss: 0.7059, Val Loss: 0.7020, Train Acc: 53.40%, Val Acc: 47.00%\n",
      "Epoch 3/500, Train Loss: 0.7062, Val Loss: 0.7004, Train Acc: 52.20%, Val Acc: 47.50%\n",
      "Epoch 4/500, Train Loss: 0.7233, Val Loss: 0.7387, Train Acc: 51.40%, Val Acc: 48.50%\n",
      "Epoch 5/500, Train Loss: 0.7400, Val Loss: 0.6955, Train Acc: 49.00%, Val Acc: 46.00%\n",
      "Epoch 6/500, Train Loss: 0.7214, Val Loss: 0.6971, Train Acc: 45.40%, Val Acc: 45.50%\n",
      "Epoch 7/500, Train Loss: 0.7446, Val Loss: 0.6902, Train Acc: 46.80%, Val Acc: 52.00%\n",
      "Epoch 8/500, Train Loss: 0.7037, Val Loss: 0.6904, Train Acc: 52.00%, Val Acc: 52.00%\n",
      "Epoch 9/500, Train Loss: 0.7151, Val Loss: 0.7037, Train Acc: 49.40%, Val Acc: 50.00%\n",
      "Epoch 10/500, Train Loss: 0.7124, Val Loss: 0.7067, Train Acc: 50.60%, Val Acc: 51.00%\n",
      "Epoch 11/500, Train Loss: 0.7179, Val Loss: 0.7032, Train Acc: 50.80%, Val Acc: 47.00%\n",
      "Epoch 12/500, Train Loss: 0.7263, Val Loss: 0.6956, Train Acc: 48.40%, Val Acc: 48.50%\n",
      "Epoch 13/500, Train Loss: 0.7038, Val Loss: 0.6952, Train Acc: 51.00%, Val Acc: 46.50%\n",
      "Epoch 14/500, Train Loss: 0.7220, Val Loss: 0.6967, Train Acc: 51.00%, Val Acc: 46.50%\n",
      "Epoch 15/500, Train Loss: 0.7117, Val Loss: 0.6935, Train Acc: 50.80%, Val Acc: 49.00%\n",
      "Epoch 16/500, Train Loss: 0.7180, Val Loss: 0.6976, Train Acc: 47.80%, Val Acc: 47.00%\n",
      "Epoch 17/500, Train Loss: 0.6943, Val Loss: 0.7009, Train Acc: 51.40%, Val Acc: 47.50%\n",
      "Epoch 18/500, Train Loss: 0.7021, Val Loss: 0.6934, Train Acc: 50.20%, Val Acc: 47.00%\n",
      "Epoch 19/500, Train Loss: 0.7071, Val Loss: 0.6925, Train Acc: 50.40%, Val Acc: 47.50%\n",
      "Epoch 20/500, Train Loss: 0.7035, Val Loss: 0.6862, Train Acc: 50.20%, Val Acc: 53.00%\n",
      "Epoch 21/500, Train Loss: 0.6906, Val Loss: 0.6863, Train Acc: 53.20%, Val Acc: 56.50%\n",
      "Epoch 22/500, Train Loss: 0.6873, Val Loss: 0.6858, Train Acc: 53.40%, Val Acc: 57.50%\n",
      "Epoch 23/500, Train Loss: 0.6972, Val Loss: 0.6836, Train Acc: 51.60%, Val Acc: 54.00%\n",
      "Epoch 24/500, Train Loss: 0.6873, Val Loss: 0.7129, Train Acc: 55.60%, Val Acc: 48.50%\n",
      "Epoch 25/500, Train Loss: 0.6992, Val Loss: 0.6956, Train Acc: 48.80%, Val Acc: 47.50%\n",
      "Epoch 26/500, Train Loss: 0.6942, Val Loss: 0.7024, Train Acc: 50.80%, Val Acc: 48.00%\n",
      "Epoch 27/500, Train Loss: 0.6897, Val Loss: 0.7277, Train Acc: 52.40%, Val Acc: 47.50%\n",
      "Epoch 28/500, Train Loss: 0.6979, Val Loss: 0.6998, Train Acc: 48.80%, Val Acc: 45.00%\n",
      "Epoch 29/500, Train Loss: 0.6885, Val Loss: 0.6957, Train Acc: 52.20%, Val Acc: 46.50%\n",
      "Epoch 30/500, Train Loss: 0.7048, Val Loss: 0.6853, Train Acc: 49.20%, Val Acc: 55.00%\n",
      "Epoch 31/500, Train Loss: 0.7049, Val Loss: 0.6862, Train Acc: 47.80%, Val Acc: 53.00%\n",
      "Epoch 32/500, Train Loss: 0.7054, Val Loss: 0.7332, Train Acc: 48.40%, Val Acc: 48.50%\n",
      "Epoch 33/500, Train Loss: 0.6931, Val Loss: 0.7122, Train Acc: 51.80%, Val Acc: 47.50%\n",
      "Epoch 34/500, Train Loss: 0.6914, Val Loss: 0.7015, Train Acc: 53.40%, Val Acc: 44.50%\n",
      "Epoch 35/500, Train Loss: 0.6967, Val Loss: 0.6979, Train Acc: 48.60%, Val Acc: 44.00%\n",
      "Epoch 36/500, Train Loss: 0.6945, Val Loss: 0.6925, Train Acc: 49.60%, Val Acc: 50.50%\n",
      "Epoch 37/500, Train Loss: 0.6783, Val Loss: 0.7158, Train Acc: 53.60%, Val Acc: 53.00%\n",
      "Epoch 38/500, Train Loss: 0.6985, Val Loss: 0.6840, Train Acc: 50.60%, Val Acc: 53.00%\n",
      "Epoch 39/500, Train Loss: 0.6959, Val Loss: 0.6893, Train Acc: 49.20%, Val Acc: 55.00%\n",
      "Epoch 40/500, Train Loss: 0.6952, Val Loss: 0.6908, Train Acc: 52.00%, Val Acc: 50.50%\n",
      "Epoch 41/500, Train Loss: 0.6919, Val Loss: 0.6909, Train Acc: 50.80%, Val Acc: 47.50%\n",
      "Epoch 42/500, Train Loss: 0.6949, Val Loss: 0.6959, Train Acc: 52.80%, Val Acc: 47.50%\n",
      "Epoch 43/500, Train Loss: 0.6889, Val Loss: 0.6929, Train Acc: 53.20%, Val Acc: 45.50%\n",
      "Epoch 44/500, Train Loss: 0.6952, Val Loss: 0.6858, Train Acc: 52.60%, Val Acc: 53.00%\n",
      "Epoch 45/500, Train Loss: 0.6957, Val Loss: 0.6860, Train Acc: 50.80%, Val Acc: 53.00%\n",
      "Epoch 46/500, Train Loss: 0.6935, Val Loss: 0.6899, Train Acc: 51.80%, Val Acc: 57.00%\n",
      "Epoch 47/500, Train Loss: 0.6948, Val Loss: 0.6910, Train Acc: 47.80%, Val Acc: 54.00%\n",
      "Epoch 48/500, Train Loss: 0.6930, Val Loss: 0.6914, Train Acc: 53.00%, Val Acc: 50.50%\n",
      "Epoch 49/500, Train Loss: 0.6972, Val Loss: 0.6915, Train Acc: 49.60%, Val Acc: 46.50%\n",
      "Epoch 50/500, Train Loss: 0.6894, Val Loss: 0.6925, Train Acc: 52.40%, Val Acc: 48.00%\n",
      "Epoch 51/500, Train Loss: 0.6871, Val Loss: 0.6921, Train Acc: 56.60%, Val Acc: 51.50%\n",
      "Epoch 52/500, Train Loss: 0.6965, Val Loss: 0.6918, Train Acc: 52.20%, Val Acc: 52.00%\n",
      "Epoch 53/500, Train Loss: 0.6904, Val Loss: 0.6935, Train Acc: 55.00%, Val Acc: 50.00%\n",
      "Epoch 54/500, Train Loss: 0.6866, Val Loss: 0.7495, Train Acc: 53.20%, Val Acc: 47.00%\n",
      "Epoch 55/500, Train Loss: 0.6967, Val Loss: 0.7032, Train Acc: 48.60%, Val Acc: 44.00%\n",
      "Epoch 56/500, Train Loss: 0.6976, Val Loss: 0.6939, Train Acc: 51.20%, Val Acc: 44.50%\n",
      "Epoch 57/500, Train Loss: 0.6829, Val Loss: 0.6915, Train Acc: 53.80%, Val Acc: 46.00%\n",
      "Epoch 58/500, Train Loss: 0.6950, Val Loss: 0.6924, Train Acc: 47.60%, Val Acc: 45.00%\n",
      "Epoch 59/500, Train Loss: 0.6939, Val Loss: 0.6922, Train Acc: 51.80%, Val Acc: 45.50%\n",
      "Epoch 60/500, Train Loss: 0.6872, Val Loss: 0.6995, Train Acc: 53.80%, Val Acc: 48.50%\n",
      "Epoch 61/500, Train Loss: 0.6802, Val Loss: 0.6916, Train Acc: 55.80%, Val Acc: 46.50%\n",
      "Epoch 62/500, Train Loss: 0.6863, Val Loss: 0.6915, Train Acc: 52.40%, Val Acc: 46.00%\n",
      "Epoch 63/500, Train Loss: 0.6808, Val Loss: 0.6905, Train Acc: 54.40%, Val Acc: 45.00%\n",
      "Epoch 64/500, Train Loss: 0.6921, Val Loss: 0.7437, Train Acc: 53.20%, Val Acc: 47.50%\n",
      "Epoch 65/500, Train Loss: 0.6922, Val Loss: 0.7039, Train Acc: 50.60%, Val Acc: 47.50%\n",
      "Epoch 66/500, Train Loss: 0.6957, Val Loss: 0.6887, Train Acc: 51.40%, Val Acc: 54.00%\n",
      "Epoch 67/500, Train Loss: 0.6836, Val Loss: 0.6842, Train Acc: 50.60%, Val Acc: 53.00%\n",
      "Epoch 68/500, Train Loss: 0.6856, Val Loss: 0.6814, Train Acc: 52.00%, Val Acc: 56.50%\n",
      "Epoch 69/500, Train Loss: 0.6825, Val Loss: 0.7022, Train Acc: 53.80%, Val Acc: 48.00%\n",
      "Epoch 70/500, Train Loss: 0.6817, Val Loss: 0.6827, Train Acc: 52.40%, Val Acc: 53.50%\n",
      "Epoch 71/500, Train Loss: 0.6951, Val Loss: 0.6907, Train Acc: 54.20%, Val Acc: 49.00%\n",
      "Epoch 72/500, Train Loss: 0.6892, Val Loss: 0.6838, Train Acc: 53.00%, Val Acc: 53.00%\n",
      "Epoch 73/500, Train Loss: 0.6839, Val Loss: 0.6888, Train Acc: 55.80%, Val Acc: 53.50%\n",
      "Epoch 74/500, Train Loss: 0.6861, Val Loss: 0.6905, Train Acc: 59.40%, Val Acc: 49.00%\n",
      "Epoch 75/500, Train Loss: 0.6832, Val Loss: 0.6858, Train Acc: 55.00%, Val Acc: 63.50%\n",
      "Epoch 76/500, Train Loss: 0.6961, Val Loss: 0.6792, Train Acc: 53.00%, Val Acc: 66.00%\n",
      "Epoch 77/500, Train Loss: 0.6884, Val Loss: 0.6910, Train Acc: 49.20%, Val Acc: 50.50%\n",
      "Epoch 78/500, Train Loss: 0.6905, Val Loss: 0.6902, Train Acc: 53.40%, Val Acc: 51.00%\n",
      "Epoch 79/500, Train Loss: 0.6821, Val Loss: 0.6900, Train Acc: 52.60%, Val Acc: 52.50%\n",
      "Epoch 80/500, Train Loss: 0.6942, Val Loss: 0.6886, Train Acc: 50.80%, Val Acc: 52.00%\n",
      "Epoch 81/500, Train Loss: 0.6968, Val Loss: 0.6898, Train Acc: 50.20%, Val Acc: 48.50%\n",
      "Epoch 82/500, Train Loss: 0.6946, Val Loss: 0.6882, Train Acc: 52.00%, Val Acc: 51.00%\n",
      "Epoch 83/500, Train Loss: 0.6895, Val Loss: 0.6872, Train Acc: 52.40%, Val Acc: 52.50%\n",
      "Epoch 84/500, Train Loss: 0.6871, Val Loss: 0.6820, Train Acc: 53.80%, Val Acc: 58.50%\n",
      "Epoch 85/500, Train Loss: 0.6786, Val Loss: 0.7148, Train Acc: 58.00%, Val Acc: 48.50%\n",
      "Epoch 86/500, Train Loss: 0.6784, Val Loss: 0.6873, Train Acc: 57.80%, Val Acc: 48.50%\n",
      "Epoch 87/500, Train Loss: 0.6891, Val Loss: 0.6817, Train Acc: 53.40%, Val Acc: 53.00%\n",
      "Epoch 88/500, Train Loss: 0.6869, Val Loss: 0.6827, Train Acc: 54.40%, Val Acc: 53.00%\n",
      "Epoch 89/500, Train Loss: 0.6787, Val Loss: 0.6882, Train Acc: 52.80%, Val Acc: 47.00%\n",
      "Epoch 90/500, Train Loss: 0.6866, Val Loss: 0.6830, Train Acc: 55.60%, Val Acc: 53.00%\n",
      "Epoch 91/500, Train Loss: 0.6859, Val Loss: 0.6867, Train Acc: 51.20%, Val Acc: 53.50%\n",
      "Epoch 92/500, Train Loss: 0.6818, Val Loss: 0.6876, Train Acc: 55.20%, Val Acc: 55.50%\n",
      "Epoch 93/500, Train Loss: 0.6912, Val Loss: 0.6809, Train Acc: 53.40%, Val Acc: 53.50%\n",
      "Epoch 94/500, Train Loss: 0.6893, Val Loss: 0.6838, Train Acc: 52.00%, Val Acc: 53.00%\n",
      "Epoch 95/500, Train Loss: 0.6969, Val Loss: 0.6991, Train Acc: 50.40%, Val Acc: 53.50%\n",
      "Epoch 96/500, Train Loss: 0.7092, Val Loss: 0.6882, Train Acc: 49.00%, Val Acc: 56.50%\n",
      "Epoch 97/500, Train Loss: 0.6969, Val Loss: 0.6988, Train Acc: 51.60%, Val Acc: 48.00%\n",
      "Epoch 98/500, Train Loss: 0.6851, Val Loss: 0.6854, Train Acc: 57.20%, Val Acc: 55.50%\n",
      "Epoch 99/500, Train Loss: 0.6935, Val Loss: 0.6830, Train Acc: 51.40%, Val Acc: 59.00%\n",
      "Epoch 100/500, Train Loss: 0.6888, Val Loss: 0.6998, Train Acc: 53.40%, Val Acc: 50.50%\n",
      "Epoch 101/500, Train Loss: 0.6811, Val Loss: 0.6869, Train Acc: 55.00%, Val Acc: 51.00%\n",
      "Epoch 102/500, Train Loss: 0.6816, Val Loss: 0.6807, Train Acc: 56.60%, Val Acc: 53.00%\n",
      "Epoch 103/500, Train Loss: 0.6800, Val Loss: 0.6771, Train Acc: 53.60%, Val Acc: 61.50%\n",
      "Epoch 104/500, Train Loss: 0.6824, Val Loss: 0.6795, Train Acc: 53.60%, Val Acc: 61.50%\n",
      "Epoch 105/500, Train Loss: 0.6739, Val Loss: 0.6814, Train Acc: 57.40%, Val Acc: 54.00%\n",
      "Epoch 106/500, Train Loss: 0.6868, Val Loss: 0.6853, Train Acc: 52.80%, Val Acc: 51.50%\n",
      "Epoch 107/500, Train Loss: 0.6820, Val Loss: 0.6886, Train Acc: 52.80%, Val Acc: 50.50%\n",
      "Epoch 108/500, Train Loss: 0.6844, Val Loss: 0.6797, Train Acc: 57.80%, Val Acc: 55.00%\n",
      "Epoch 109/500, Train Loss: 0.6801, Val Loss: 0.6716, Train Acc: 55.80%, Val Acc: 58.00%\n",
      "Epoch 110/500, Train Loss: 0.6791, Val Loss: 0.6799, Train Acc: 55.20%, Val Acc: 53.00%\n",
      "Epoch 111/500, Train Loss: 0.6881, Val Loss: 0.6779, Train Acc: 53.80%, Val Acc: 53.00%\n",
      "Epoch 112/500, Train Loss: 0.6857, Val Loss: 0.6876, Train Acc: 53.20%, Val Acc: 50.00%\n",
      "Epoch 113/500, Train Loss: 0.6964, Val Loss: 0.6809, Train Acc: 54.20%, Val Acc: 61.00%\n",
      "Epoch 114/500, Train Loss: 0.6897, Val Loss: 0.7101, Train Acc: 51.80%, Val Acc: 51.00%\n",
      "Epoch 115/500, Train Loss: 0.6807, Val Loss: 0.6792, Train Acc: 57.20%, Val Acc: 56.50%\n",
      "Epoch 116/500, Train Loss: 0.6794, Val Loss: 0.6801, Train Acc: 57.00%, Val Acc: 55.50%\n",
      "Epoch 117/500, Train Loss: 0.6775, Val Loss: 0.6799, Train Acc: 58.80%, Val Acc: 55.00%\n",
      "Epoch 118/500, Train Loss: 0.6766, Val Loss: 0.6805, Train Acc: 54.40%, Val Acc: 53.00%\n",
      "Epoch 119/500, Train Loss: 0.6754, Val Loss: 0.6804, Train Acc: 57.40%, Val Acc: 57.50%\n",
      "Epoch 120/500, Train Loss: 0.6829, Val Loss: 0.6785, Train Acc: 54.40%, Val Acc: 61.00%\n",
      "Epoch 121/500, Train Loss: 0.6779, Val Loss: 0.6840, Train Acc: 56.80%, Val Acc: 53.00%\n",
      "Epoch 122/500, Train Loss: 0.6829, Val Loss: 0.6754, Train Acc: 54.00%, Val Acc: 58.00%\n",
      "Epoch 123/500, Train Loss: 0.6779, Val Loss: 0.6647, Train Acc: 56.80%, Val Acc: 64.00%\n",
      "Epoch 124/500, Train Loss: 0.6769, Val Loss: 0.6793, Train Acc: 56.20%, Val Acc: 58.50%\n",
      "Epoch 125/500, Train Loss: 0.6774, Val Loss: 0.6802, Train Acc: 56.80%, Val Acc: 55.50%\n",
      "Epoch 126/500, Train Loss: 0.6990, Val Loss: 0.6821, Train Acc: 48.80%, Val Acc: 54.50%\n",
      "Epoch 127/500, Train Loss: 0.6971, Val Loss: 0.6886, Train Acc: 49.20%, Val Acc: 56.50%\n",
      "Epoch 128/500, Train Loss: 0.6884, Val Loss: 0.6913, Train Acc: 50.60%, Val Acc: 48.50%\n",
      "Epoch 129/500, Train Loss: 0.6891, Val Loss: 0.6910, Train Acc: 53.80%, Val Acc: 47.00%\n",
      "Epoch 130/500, Train Loss: 0.6828, Val Loss: 0.6815, Train Acc: 53.40%, Val Acc: 55.00%\n",
      "Epoch 131/500, Train Loss: 0.6833, Val Loss: 0.6867, Train Acc: 55.40%, Val Acc: 53.00%\n",
      "Epoch 132/500, Train Loss: 0.6922, Val Loss: 0.6851, Train Acc: 52.20%, Val Acc: 56.00%\n",
      "Epoch 133/500, Train Loss: 0.6812, Val Loss: 0.6890, Train Acc: 51.80%, Val Acc: 53.00%\n",
      "Epoch 134/500, Train Loss: 0.6810, Val Loss: 0.6808, Train Acc: 55.40%, Val Acc: 60.50%\n",
      "Epoch 135/500, Train Loss: 0.6805, Val Loss: 0.6881, Train Acc: 50.40%, Val Acc: 55.00%\n",
      "Epoch 136/500, Train Loss: 0.6770, Val Loss: 0.7378, Train Acc: 57.20%, Val Acc: 47.50%\n",
      "Epoch 137/500, Train Loss: 0.6826, Val Loss: 0.6803, Train Acc: 51.80%, Val Acc: 53.00%\n",
      "Epoch 138/500, Train Loss: 0.6843, Val Loss: 0.7855, Train Acc: 55.20%, Val Acc: 49.50%\n",
      "Epoch 139/500, Train Loss: 0.6851, Val Loss: 0.6854, Train Acc: 51.60%, Val Acc: 53.00%\n",
      "Epoch 140/500, Train Loss: 0.6738, Val Loss: 0.6757, Train Acc: 58.00%, Val Acc: 53.00%\n",
      "Epoch 141/500, Train Loss: 0.6802, Val Loss: 0.6723, Train Acc: 53.20%, Val Acc: 55.00%\n",
      "Epoch 142/500, Train Loss: 0.6774, Val Loss: 0.6960, Train Acc: 53.20%, Val Acc: 49.50%\n",
      "Epoch 143/500, Train Loss: 0.6731, Val Loss: 0.6730, Train Acc: 54.80%, Val Acc: 59.00%\n",
      "Epoch 144/500, Train Loss: 0.6763, Val Loss: 0.6906, Train Acc: 56.20%, Val Acc: 53.50%\n",
      "Epoch 145/500, Train Loss: 0.6807, Val Loss: 0.6825, Train Acc: 53.80%, Val Acc: 53.50%\n",
      "Epoch 146/500, Train Loss: 0.6887, Val Loss: 0.6747, Train Acc: 51.20%, Val Acc: 62.00%\n",
      "Epoch 147/500, Train Loss: 0.6716, Val Loss: 0.6777, Train Acc: 56.00%, Val Acc: 58.50%\n",
      "Epoch 148/500, Train Loss: 0.6943, Val Loss: 0.6674, Train Acc: 53.20%, Val Acc: 61.00%\n",
      "Epoch 149/500, Train Loss: 0.6920, Val Loss: 0.7325, Train Acc: 52.40%, Val Acc: 48.50%\n",
      "Epoch 150/500, Train Loss: 0.6686, Val Loss: 0.7448, Train Acc: 57.40%, Val Acc: 50.00%\n",
      "Epoch 151/500, Train Loss: 0.6676, Val Loss: 0.6880, Train Acc: 54.80%, Val Acc: 55.00%\n",
      "Epoch 152/500, Train Loss: 0.6805, Val Loss: 0.6718, Train Acc: 55.80%, Val Acc: 53.00%\n",
      "Epoch 153/500, Train Loss: 0.6693, Val Loss: 0.6611, Train Acc: 57.60%, Val Acc: 66.50%\n",
      "Epoch 154/500, Train Loss: 0.6868, Val Loss: 0.6827, Train Acc: 52.00%, Val Acc: 60.00%\n",
      "Epoch 155/500, Train Loss: 0.6758, Val Loss: 0.6760, Train Acc: 56.20%, Val Acc: 53.50%\n",
      "Epoch 156/500, Train Loss: 0.6650, Val Loss: 0.6731, Train Acc: 60.00%, Val Acc: 62.50%\n",
      "Epoch 157/500, Train Loss: 0.6794, Val Loss: 0.6764, Train Acc: 58.20%, Val Acc: 61.50%\n",
      "Epoch 158/500, Train Loss: 0.6841, Val Loss: 0.6705, Train Acc: 54.00%, Val Acc: 55.00%\n",
      "Epoch 159/500, Train Loss: 0.6624, Val Loss: 0.6824, Train Acc: 59.20%, Val Acc: 55.00%\n",
      "Epoch 160/500, Train Loss: 0.6636, Val Loss: 0.6512, Train Acc: 61.00%, Val Acc: 62.50%\n",
      "Epoch 161/500, Train Loss: 0.6589, Val Loss: 0.6788, Train Acc: 58.00%, Val Acc: 55.50%\n",
      "Epoch 162/500, Train Loss: 0.6881, Val Loss: 0.6864, Train Acc: 55.60%, Val Acc: 59.00%\n",
      "Epoch 163/500, Train Loss: 0.6895, Val Loss: 0.6862, Train Acc: 51.60%, Val Acc: 48.50%\n",
      "Epoch 164/500, Train Loss: 0.6756, Val Loss: 0.6806, Train Acc: 58.40%, Val Acc: 51.50%\n",
      "Epoch 165/500, Train Loss: 0.6680, Val Loss: 0.6953, Train Acc: 56.00%, Val Acc: 52.50%\n",
      "Epoch 166/500, Train Loss: 0.6751, Val Loss: 0.7696, Train Acc: 55.80%, Val Acc: 50.00%\n",
      "Epoch 167/500, Train Loss: 0.6646, Val Loss: 0.6578, Train Acc: 55.40%, Val Acc: 60.50%\n",
      "Epoch 168/500, Train Loss: 0.6584, Val Loss: 0.6773, Train Acc: 58.80%, Val Acc: 53.50%\n",
      "Epoch 169/500, Train Loss: 0.6770, Val Loss: 0.6662, Train Acc: 57.40%, Val Acc: 58.50%\n",
      "Epoch 170/500, Train Loss: 0.6861, Val Loss: 0.7013, Train Acc: 55.20%, Val Acc: 51.50%\n",
      "Epoch 171/500, Train Loss: 0.6672, Val Loss: 0.6550, Train Acc: 57.00%, Val Acc: 60.50%\n",
      "Epoch 172/500, Train Loss: 0.6733, Val Loss: 0.6659, Train Acc: 54.60%, Val Acc: 58.00%\n",
      "Epoch 173/500, Train Loss: 0.6732, Val Loss: 0.6673, Train Acc: 56.40%, Val Acc: 59.00%\n",
      "Epoch 174/500, Train Loss: 0.6829, Val Loss: 0.6726, Train Acc: 51.00%, Val Acc: 58.50%\n",
      "Epoch 175/500, Train Loss: 0.6671, Val Loss: 0.7654, Train Acc: 55.60%, Val Acc: 49.00%\n",
      "Epoch 176/500, Train Loss: 0.6936, Val Loss: 0.6779, Train Acc: 52.60%, Val Acc: 53.00%\n",
      "Epoch 177/500, Train Loss: 0.6605, Val Loss: 0.6770, Train Acc: 57.00%, Val Acc: 55.00%\n",
      "Epoch 178/500, Train Loss: 0.6820, Val Loss: 0.6728, Train Acc: 54.00%, Val Acc: 59.00%\n",
      "Epoch 179/500, Train Loss: 0.6652, Val Loss: 0.7130, Train Acc: 56.00%, Val Acc: 50.50%\n",
      "Epoch 180/500, Train Loss: 0.6762, Val Loss: 0.6483, Train Acc: 55.60%, Val Acc: 59.00%\n",
      "Epoch 181/500, Train Loss: 0.6663, Val Loss: 0.6717, Train Acc: 56.40%, Val Acc: 61.00%\n",
      "Epoch 182/500, Train Loss: 0.6591, Val Loss: 0.6742, Train Acc: 56.40%, Val Acc: 55.00%\n",
      "Epoch 183/500, Train Loss: 0.6678, Val Loss: 0.6783, Train Acc: 57.20%, Val Acc: 54.00%\n",
      "Epoch 184/500, Train Loss: 0.6578, Val Loss: 0.6562, Train Acc: 57.20%, Val Acc: 62.00%\n",
      "Epoch 185/500, Train Loss: 0.6550, Val Loss: 0.6663, Train Acc: 59.20%, Val Acc: 58.50%\n",
      "Epoch 186/500, Train Loss: 0.6654, Val Loss: 0.6674, Train Acc: 58.80%, Val Acc: 55.00%\n",
      "Epoch 187/500, Train Loss: 0.6634, Val Loss: 0.6654, Train Acc: 60.60%, Val Acc: 55.00%\n",
      "Epoch 188/500, Train Loss: 0.6552, Val Loss: 0.6965, Train Acc: 55.80%, Val Acc: 55.00%\n",
      "Epoch 189/500, Train Loss: 0.6315, Val Loss: 0.6497, Train Acc: 58.80%, Val Acc: 56.50%\n",
      "Epoch 190/500, Train Loss: 0.6582, Val Loss: 0.7153, Train Acc: 58.60%, Val Acc: 48.50%\n",
      "Epoch 191/500, Train Loss: 0.6820, Val Loss: 0.6622, Train Acc: 50.20%, Val Acc: 54.00%\n",
      "Epoch 192/500, Train Loss: 0.6889, Val Loss: 0.6776, Train Acc: 53.40%, Val Acc: 57.50%\n",
      "Epoch 193/500, Train Loss: 0.6930, Val Loss: 0.6921, Train Acc: 55.80%, Val Acc: 53.50%\n",
      "Epoch 194/500, Train Loss: 0.6737, Val Loss: 0.6849, Train Acc: 56.20%, Val Acc: 50.00%\n",
      "Epoch 195/500, Train Loss: 0.7096, Val Loss: 0.6748, Train Acc: 55.80%, Val Acc: 53.50%\n",
      "Epoch 196/500, Train Loss: 0.6818, Val Loss: 0.6485, Train Acc: 56.00%, Val Acc: 60.50%\n",
      "Epoch 197/500, Train Loss: 0.6685, Val Loss: 0.6522, Train Acc: 57.60%, Val Acc: 67.50%\n",
      "Epoch 198/500, Train Loss: 0.6654, Val Loss: 0.6869, Train Acc: 52.60%, Val Acc: 50.50%\n",
      "Epoch 199/500, Train Loss: 0.6587, Val Loss: 0.6769, Train Acc: 55.80%, Val Acc: 51.50%\n",
      "Epoch 200/500, Train Loss: 0.6783, Val Loss: 0.6821, Train Acc: 55.60%, Val Acc: 53.00%\n",
      "Epoch 201/500, Train Loss: 0.6838, Val Loss: 0.7019, Train Acc: 54.40%, Val Acc: 46.00%\n",
      "Epoch 202/500, Train Loss: 0.6815, Val Loss: 0.6938, Train Acc: 54.40%, Val Acc: 49.00%\n",
      "Epoch 203/500, Train Loss: 0.6780, Val Loss: 0.6816, Train Acc: 56.60%, Val Acc: 53.50%\n",
      "Epoch 204/500, Train Loss: 0.6668, Val Loss: 0.6805, Train Acc: 57.20%, Val Acc: 54.00%\n",
      "Epoch 205/500, Train Loss: 0.6573, Val Loss: 0.6765, Train Acc: 55.20%, Val Acc: 57.00%\n",
      "Epoch 206/500, Train Loss: 0.6647, Val Loss: 0.7135, Train Acc: 55.20%, Val Acc: 53.50%\n",
      "Epoch 207/500, Train Loss: 0.6716, Val Loss: 0.6647, Train Acc: 58.00%, Val Acc: 60.50%\n",
      "Epoch 208/500, Train Loss: 0.6702, Val Loss: 0.6733, Train Acc: 55.00%, Val Acc: 61.50%\n",
      "Epoch 209/500, Train Loss: 0.6687, Val Loss: 0.7644, Train Acc: 56.60%, Val Acc: 49.50%\n",
      "Epoch 210/500, Train Loss: 0.6572, Val Loss: 0.6780, Train Acc: 58.20%, Val Acc: 53.00%\n",
      "Epoch 211/500, Train Loss: 0.6591, Val Loss: 0.6352, Train Acc: 57.40%, Val Acc: 62.50%\n",
      "Epoch 212/500, Train Loss: 0.6583, Val Loss: 0.6443, Train Acc: 56.00%, Val Acc: 63.00%\n",
      "Epoch 213/500, Train Loss: 0.6699, Val Loss: 0.6942, Train Acc: 54.80%, Val Acc: 53.00%\n",
      "Epoch 214/500, Train Loss: 0.6569, Val Loss: 0.6718, Train Acc: 59.80%, Val Acc: 55.50%\n",
      "Epoch 215/500, Train Loss: 0.6639, Val Loss: 0.6601, Train Acc: 58.00%, Val Acc: 61.50%\n",
      "Epoch 216/500, Train Loss: 0.6566, Val Loss: 0.6958, Train Acc: 59.00%, Val Acc: 54.50%\n",
      "Epoch 217/500, Train Loss: 0.6701, Val Loss: 0.6525, Train Acc: 56.00%, Val Acc: 61.00%\n",
      "Epoch 218/500, Train Loss: 0.6581, Val Loss: 0.6583, Train Acc: 61.60%, Val Acc: 59.00%\n",
      "Epoch 219/500, Train Loss: 0.6554, Val Loss: 0.7086, Train Acc: 60.40%, Val Acc: 50.50%\n",
      "Epoch 220/500, Train Loss: 0.6575, Val Loss: 0.6300, Train Acc: 56.80%, Val Acc: 64.50%\n",
      "Epoch 221/500, Train Loss: 0.6558, Val Loss: 0.6778, Train Acc: 59.80%, Val Acc: 54.50%\n",
      "Epoch 222/500, Train Loss: 0.6476, Val Loss: 0.6734, Train Acc: 64.20%, Val Acc: 58.50%\n",
      "Epoch 223/500, Train Loss: 0.6552, Val Loss: 0.7152, Train Acc: 58.20%, Val Acc: 51.50%\n",
      "Epoch 224/500, Train Loss: 0.6607, Val Loss: 0.6286, Train Acc: 58.80%, Val Acc: 66.50%\n",
      "Epoch 225/500, Train Loss: 0.6546, Val Loss: 0.6561, Train Acc: 62.40%, Val Acc: 61.50%\n",
      "Epoch 226/500, Train Loss: 0.6495, Val Loss: 0.6376, Train Acc: 58.20%, Val Acc: 60.50%\n",
      "Epoch 227/500, Train Loss: 0.6671, Val Loss: 0.6723, Train Acc: 55.20%, Val Acc: 49.50%\n",
      "Epoch 228/500, Train Loss: 0.6507, Val Loss: 0.6601, Train Acc: 59.20%, Val Acc: 63.00%\n",
      "Epoch 229/500, Train Loss: 0.6719, Val Loss: 0.6563, Train Acc: 58.20%, Val Acc: 59.50%\n",
      "Epoch 230/500, Train Loss: 0.6692, Val Loss: 0.8476, Train Acc: 58.60%, Val Acc: 49.00%\n",
      "Epoch 231/500, Train Loss: 0.6514, Val Loss: 0.6527, Train Acc: 59.80%, Val Acc: 60.00%\n",
      "Epoch 232/500, Train Loss: 0.6433, Val Loss: 0.6707, Train Acc: 60.00%, Val Acc: 56.50%\n",
      "Epoch 233/500, Train Loss: 0.6393, Val Loss: 0.6268, Train Acc: 60.20%, Val Acc: 63.50%\n",
      "Epoch 234/500, Train Loss: 0.6319, Val Loss: 0.6448, Train Acc: 61.80%, Val Acc: 60.00%\n",
      "Epoch 235/500, Train Loss: 0.6633, Val Loss: 0.6371, Train Acc: 59.00%, Val Acc: 61.50%\n",
      "Epoch 236/500, Train Loss: 0.6747, Val Loss: 0.6719, Train Acc: 57.00%, Val Acc: 61.50%\n",
      "Epoch 237/500, Train Loss: 0.6764, Val Loss: 0.6661, Train Acc: 53.80%, Val Acc: 61.50%\n",
      "Epoch 238/500, Train Loss: 0.6620, Val Loss: 0.8623, Train Acc: 58.00%, Val Acc: 48.00%\n",
      "Epoch 239/500, Train Loss: 0.6976, Val Loss: 0.7140, Train Acc: 54.40%, Val Acc: 52.50%\n",
      "Epoch 240/500, Train Loss: 0.6719, Val Loss: 0.6716, Train Acc: 55.00%, Val Acc: 54.00%\n",
      "Epoch 241/500, Train Loss: 0.6470, Val Loss: 0.6657, Train Acc: 63.00%, Val Acc: 53.00%\n",
      "Epoch 242/500, Train Loss: 0.6608, Val Loss: 0.6718, Train Acc: 59.60%, Val Acc: 54.50%\n",
      "Epoch 243/500, Train Loss: 0.6559, Val Loss: 0.6800, Train Acc: 61.40%, Val Acc: 55.00%\n",
      "Epoch 244/500, Train Loss: 0.6756, Val Loss: 0.6528, Train Acc: 54.40%, Val Acc: 67.50%\n",
      "Epoch 245/500, Train Loss: 0.6587, Val Loss: 0.6644, Train Acc: 58.20%, Val Acc: 61.00%\n",
      "Epoch 246/500, Train Loss: 0.6820, Val Loss: 0.7080, Train Acc: 55.60%, Val Acc: 53.50%\n",
      "Epoch 247/500, Train Loss: 0.6617, Val Loss: 0.6588, Train Acc: 57.20%, Val Acc: 60.50%\n",
      "Epoch 248/500, Train Loss: 0.6623, Val Loss: 0.6839, Train Acc: 60.00%, Val Acc: 50.50%\n",
      "Epoch 249/500, Train Loss: 0.6790, Val Loss: 0.6818, Train Acc: 54.60%, Val Acc: 58.50%\n",
      "Epoch 250/500, Train Loss: 0.6768, Val Loss: 0.6593, Train Acc: 54.00%, Val Acc: 57.50%\n",
      "Epoch 251/500, Train Loss: 0.6469, Val Loss: 0.6544, Train Acc: 60.40%, Val Acc: 56.00%\n",
      "Epoch 252/500, Train Loss: 0.6420, Val Loss: 0.6403, Train Acc: 57.20%, Val Acc: 62.00%\n",
      "Epoch 253/500, Train Loss: 0.6324, Val Loss: 0.6653, Train Acc: 61.60%, Val Acc: 56.50%\n",
      "Epoch 254/500, Train Loss: 0.6508, Val Loss: 0.7536, Train Acc: 60.20%, Val Acc: 53.50%\n",
      "Epoch 255/500, Train Loss: 0.6591, Val Loss: 0.7086, Train Acc: 59.60%, Val Acc: 53.50%\n",
      "Epoch 256/500, Train Loss: 0.6669, Val Loss: 0.7079, Train Acc: 59.40%, Val Acc: 54.00%\n",
      "Epoch 257/500, Train Loss: 0.6500, Val Loss: 0.6927, Train Acc: 60.20%, Val Acc: 49.00%\n",
      "Epoch 258/500, Train Loss: 0.6556, Val Loss: 0.6815, Train Acc: 60.20%, Val Acc: 53.00%\n",
      "Epoch 259/500, Train Loss: 0.6591, Val Loss: 0.6694, Train Acc: 59.20%, Val Acc: 57.50%\n",
      "Epoch 260/500, Train Loss: 0.6569, Val Loss: 0.6674, Train Acc: 57.20%, Val Acc: 55.50%\n",
      "Epoch 261/500, Train Loss: 0.6439, Val Loss: 0.6566, Train Acc: 60.40%, Val Acc: 65.00%\n",
      "Epoch 262/500, Train Loss: 0.6560, Val Loss: 0.9354, Train Acc: 59.80%, Val Acc: 48.50%\n",
      "Epoch 263/500, Train Loss: 0.6584, Val Loss: 0.6480, Train Acc: 60.40%, Val Acc: 60.00%\n",
      "Epoch 264/500, Train Loss: 0.6421, Val Loss: 0.6652, Train Acc: 62.60%, Val Acc: 60.50%\n",
      "Epoch 265/500, Train Loss: 0.6560, Val Loss: 0.6740, Train Acc: 56.60%, Val Acc: 53.50%\n",
      "Epoch 266/500, Train Loss: 0.6455, Val Loss: 0.6341, Train Acc: 59.40%, Val Acc: 61.50%\n",
      "Epoch 267/500, Train Loss: 0.6560, Val Loss: 0.6587, Train Acc: 60.00%, Val Acc: 67.00%\n",
      "Epoch 268/500, Train Loss: 0.6652, Val Loss: 0.6748, Train Acc: 58.60%, Val Acc: 59.00%\n",
      "Epoch 269/500, Train Loss: 0.6575, Val Loss: 0.6311, Train Acc: 57.60%, Val Acc: 61.00%\n",
      "Epoch 270/500, Train Loss: 0.6470, Val Loss: 0.6810, Train Acc: 59.60%, Val Acc: 50.00%\n",
      "Epoch 271/500, Train Loss: 0.6445, Val Loss: 0.6273, Train Acc: 59.60%, Val Acc: 60.50%\n",
      "Epoch 272/500, Train Loss: 0.6527, Val Loss: 0.6783, Train Acc: 65.00%, Val Acc: 60.50%\n",
      "Epoch 273/500, Train Loss: 0.6474, Val Loss: 0.7199, Train Acc: 61.80%, Val Acc: 57.00%\n",
      "Epoch 274/500, Train Loss: 0.6668, Val Loss: 0.8365, Train Acc: 60.20%, Val Acc: 49.00%\n",
      "Epoch 275/500, Train Loss: 0.6989, Val Loss: 0.6614, Train Acc: 55.60%, Val Acc: 59.50%\n",
      "Epoch 276/500, Train Loss: 0.6545, Val Loss: 0.6726, Train Acc: 60.60%, Val Acc: 53.50%\n",
      "Epoch 277/500, Train Loss: 0.6705, Val Loss: 0.6871, Train Acc: 53.80%, Val Acc: 50.00%\n",
      "Epoch 278/500, Train Loss: 0.6494, Val Loss: 0.6532, Train Acc: 60.80%, Val Acc: 61.50%\n",
      "Epoch 279/500, Train Loss: 0.6506, Val Loss: 0.6326, Train Acc: 54.80%, Val Acc: 63.00%\n",
      "Epoch 280/500, Train Loss: 0.6333, Val Loss: 0.6200, Train Acc: 63.80%, Val Acc: 66.00%\n",
      "Epoch 281/500, Train Loss: 0.6512, Val Loss: 0.6739, Train Acc: 59.20%, Val Acc: 59.00%\n",
      "Epoch 282/500, Train Loss: 0.6474, Val Loss: 0.8638, Train Acc: 58.00%, Val Acc: 49.00%\n",
      "Epoch 283/500, Train Loss: 0.6526, Val Loss: 0.6423, Train Acc: 62.80%, Val Acc: 61.00%\n",
      "Epoch 284/500, Train Loss: 0.6537, Val Loss: 0.7360, Train Acc: 58.80%, Val Acc: 52.50%\n",
      "Epoch 285/500, Train Loss: 0.6526, Val Loss: 0.6595, Train Acc: 60.80%, Val Acc: 57.50%\n",
      "Epoch 286/500, Train Loss: 0.6688, Val Loss: 0.6349, Train Acc: 56.00%, Val Acc: 60.50%\n",
      "Epoch 287/500, Train Loss: 0.6241, Val Loss: 0.6454, Train Acc: 64.40%, Val Acc: 61.50%\n",
      "Epoch 288/500, Train Loss: 0.6264, Val Loss: 0.6482, Train Acc: 61.80%, Val Acc: 60.50%\n",
      "Epoch 289/500, Train Loss: 0.6579, Val Loss: 0.6274, Train Acc: 57.80%, Val Acc: 64.00%\n",
      "Epoch 290/500, Train Loss: 0.6443, Val Loss: 0.6641, Train Acc: 59.80%, Val Acc: 56.00%\n",
      "Epoch 291/500, Train Loss: 0.6556, Val Loss: 0.6441, Train Acc: 60.20%, Val Acc: 61.50%\n",
      "Epoch 292/500, Train Loss: 0.6351, Val Loss: 0.6620, Train Acc: 62.40%, Val Acc: 59.00%\n",
      "Epoch 293/500, Train Loss: 0.6338, Val Loss: 0.6492, Train Acc: 62.20%, Val Acc: 64.00%\n",
      "Epoch 294/500, Train Loss: 0.6409, Val Loss: 0.6338, Train Acc: 62.00%, Val Acc: 61.50%\n",
      "Epoch 295/500, Train Loss: 0.6364, Val Loss: 0.6470, Train Acc: 60.60%, Val Acc: 58.00%\n",
      "Epoch 296/500, Train Loss: 0.6428, Val Loss: 0.7007, Train Acc: 61.80%, Val Acc: 52.50%\n",
      "Epoch 297/500, Train Loss: 0.6478, Val Loss: 0.6223, Train Acc: 59.80%, Val Acc: 63.50%\n",
      "Epoch 298/500, Train Loss: 0.6506, Val Loss: 0.6750, Train Acc: 62.20%, Val Acc: 52.50%\n",
      "Epoch 299/500, Train Loss: 0.6641, Val Loss: 0.6345, Train Acc: 57.60%, Val Acc: 64.50%\n",
      "Epoch 300/500, Train Loss: 0.6360, Val Loss: 0.6743, Train Acc: 60.80%, Val Acc: 53.50%\n",
      "Epoch 301/500, Train Loss: 0.6410, Val Loss: 0.6353, Train Acc: 60.40%, Val Acc: 62.00%\n",
      "Epoch 302/500, Train Loss: 0.6291, Val Loss: 0.6354, Train Acc: 64.40%, Val Acc: 61.50%\n",
      "Epoch 303/500, Train Loss: 0.6268, Val Loss: 0.6706, Train Acc: 62.40%, Val Acc: 56.50%\n",
      "Epoch 304/500, Train Loss: 0.6384, Val Loss: 0.6536, Train Acc: 61.60%, Val Acc: 61.00%\n",
      "Epoch 305/500, Train Loss: 0.6581, Val Loss: 0.7023, Train Acc: 59.60%, Val Acc: 51.50%\n",
      "Epoch 306/500, Train Loss: 0.6492, Val Loss: 0.6441, Train Acc: 61.60%, Val Acc: 62.50%\n",
      "Epoch 307/500, Train Loss: 0.6493, Val Loss: 0.6300, Train Acc: 60.80%, Val Acc: 63.50%\n",
      "Epoch 308/500, Train Loss: 0.6510, Val Loss: 0.6143, Train Acc: 60.20%, Val Acc: 63.50%\n",
      "Epoch 309/500, Train Loss: 0.6463, Val Loss: 0.6637, Train Acc: 60.40%, Val Acc: 58.50%\n",
      "Epoch 310/500, Train Loss: 0.6290, Val Loss: 0.7215, Train Acc: 61.80%, Val Acc: 58.50%\n",
      "Epoch 311/500, Train Loss: 0.6598, Val Loss: 0.6433, Train Acc: 57.00%, Val Acc: 58.00%\n",
      "Epoch 312/500, Train Loss: 0.6570, Val Loss: 0.6428, Train Acc: 59.20%, Val Acc: 58.50%\n",
      "Epoch 313/500, Train Loss: 0.6515, Val Loss: 0.6837, Train Acc: 62.40%, Val Acc: 51.50%\n",
      "Epoch 314/500, Train Loss: 0.6473, Val Loss: 0.7260, Train Acc: 62.20%, Val Acc: 56.50%\n",
      "Epoch 315/500, Train Loss: 0.6251, Val Loss: 0.6356, Train Acc: 65.40%, Val Acc: 68.50%\n",
      "Epoch 316/500, Train Loss: 0.6437, Val Loss: 0.7126, Train Acc: 59.80%, Val Acc: 59.50%\n",
      "Epoch 317/500, Train Loss: 0.6373, Val Loss: 0.6555, Train Acc: 60.60%, Val Acc: 60.00%\n",
      "Epoch 318/500, Train Loss: 0.6891, Val Loss: 0.6888, Train Acc: 61.00%, Val Acc: 49.00%\n",
      "Epoch 319/500, Train Loss: 0.6742, Val Loss: 0.6422, Train Acc: 57.40%, Val Acc: 61.00%\n",
      "Epoch 320/500, Train Loss: 0.6473, Val Loss: 0.7510, Train Acc: 59.80%, Val Acc: 50.50%\n",
      "Epoch 321/500, Train Loss: 0.6297, Val Loss: 0.7679, Train Acc: 59.60%, Val Acc: 58.00%\n",
      "Epoch 322/500, Train Loss: 0.6521, Val Loss: 0.6369, Train Acc: 56.00%, Val Acc: 60.50%\n",
      "Epoch 323/500, Train Loss: 0.6170, Val Loss: 0.6441, Train Acc: 65.40%, Val Acc: 59.00%\n",
      "Epoch 324/500, Train Loss: 0.6331, Val Loss: 0.6286, Train Acc: 62.00%, Val Acc: 62.00%\n",
      "Epoch 325/500, Train Loss: 0.6471, Val Loss: 0.6845, Train Acc: 60.00%, Val Acc: 50.00%\n",
      "Epoch 326/500, Train Loss: 0.6776, Val Loss: 0.6774, Train Acc: 59.60%, Val Acc: 58.50%\n",
      "Epoch 327/500, Train Loss: 0.6541, Val Loss: 0.6859, Train Acc: 59.40%, Val Acc: 56.00%\n",
      "Epoch 328/500, Train Loss: 0.6409, Val Loss: 0.6391, Train Acc: 63.20%, Val Acc: 68.50%\n",
      "Epoch 329/500, Train Loss: 0.6437, Val Loss: 0.6619, Train Acc: 60.40%, Val Acc: 59.00%\n",
      "Epoch 330/500, Train Loss: 0.6205, Val Loss: 0.6182, Train Acc: 63.40%, Val Acc: 66.00%\n",
      "Epoch 331/500, Train Loss: 0.6509, Val Loss: 0.6312, Train Acc: 61.00%, Val Acc: 62.50%\n",
      "Epoch 332/500, Train Loss: 0.6358, Val Loss: 0.6417, Train Acc: 61.40%, Val Acc: 58.50%\n",
      "Epoch 333/500, Train Loss: 0.6450, Val Loss: 0.6519, Train Acc: 59.60%, Val Acc: 58.00%\n",
      "Epoch 334/500, Train Loss: 0.6528, Val Loss: 0.8551, Train Acc: 59.20%, Val Acc: 49.00%\n",
      "Epoch 335/500, Train Loss: 0.6812, Val Loss: 0.6571, Train Acc: 56.80%, Val Acc: 60.00%\n",
      "Epoch 336/500, Train Loss: 0.6378, Val Loss: 0.6463, Train Acc: 63.00%, Val Acc: 61.50%\n",
      "Epoch 337/500, Train Loss: 0.6193, Val Loss: 0.6596, Train Acc: 63.00%, Val Acc: 57.00%\n",
      "Epoch 338/500, Train Loss: 0.6314, Val Loss: 0.6173, Train Acc: 63.00%, Val Acc: 67.00%\n",
      "Epoch 339/500, Train Loss: 0.6290, Val Loss: 0.6475, Train Acc: 62.20%, Val Acc: 59.50%\n",
      "Epoch 340/500, Train Loss: 0.6510, Val Loss: 0.6285, Train Acc: 62.80%, Val Acc: 63.00%\n",
      "Epoch 341/500, Train Loss: 0.6550, Val Loss: 0.6511, Train Acc: 60.80%, Val Acc: 60.50%\n",
      "Epoch 342/500, Train Loss: 0.6624, Val Loss: 0.6389, Train Acc: 59.60%, Val Acc: 58.50%\n",
      "Epoch 343/500, Train Loss: 0.6611, Val Loss: 0.6314, Train Acc: 58.80%, Val Acc: 60.00%\n",
      "Epoch 344/500, Train Loss: 0.6371, Val Loss: 0.6716, Train Acc: 61.40%, Val Acc: 56.00%\n",
      "Epoch 345/500, Train Loss: 0.6437, Val Loss: 0.6463, Train Acc: 59.40%, Val Acc: 57.00%\n",
      "Epoch 346/500, Train Loss: 0.6375, Val Loss: 0.6308, Train Acc: 61.80%, Val Acc: 62.50%\n",
      "Epoch 347/500, Train Loss: 0.6245, Val Loss: 0.6696, Train Acc: 64.20%, Val Acc: 53.00%\n",
      "Epoch 348/500, Train Loss: 0.6312, Val Loss: 0.6511, Train Acc: 63.00%, Val Acc: 56.50%\n",
      "Epoch 349/500, Train Loss: 0.6392, Val Loss: 0.6081, Train Acc: 62.80%, Val Acc: 66.50%\n",
      "Epoch 350/500, Train Loss: 0.6385, Val Loss: 0.6963, Train Acc: 62.80%, Val Acc: 57.50%\n",
      "Epoch 351/500, Train Loss: 0.6352, Val Loss: 0.6337, Train Acc: 63.00%, Val Acc: 66.50%\n",
      "Epoch 352/500, Train Loss: 0.6329, Val Loss: 0.6123, Train Acc: 62.60%, Val Acc: 67.00%\n",
      "Epoch 353/500, Train Loss: 0.6444, Val Loss: 0.6195, Train Acc: 59.80%, Val Acc: 65.00%\n",
      "Epoch 354/500, Train Loss: 0.6506, Val Loss: 0.6465, Train Acc: 58.80%, Val Acc: 61.00%\n",
      "Epoch 355/500, Train Loss: 0.6364, Val Loss: 0.6737, Train Acc: 59.20%, Val Acc: 55.50%\n",
      "Epoch 356/500, Train Loss: 0.6661, Val Loss: 0.6293, Train Acc: 60.60%, Val Acc: 59.00%\n",
      "Epoch 357/500, Train Loss: 0.6117, Val Loss: 0.6714, Train Acc: 63.00%, Val Acc: 61.00%\n",
      "Epoch 358/500, Train Loss: 0.6444, Val Loss: 0.6289, Train Acc: 57.20%, Val Acc: 66.00%\n",
      "Epoch 359/500, Train Loss: 0.6326, Val Loss: 0.6325, Train Acc: 61.20%, Val Acc: 68.00%\n",
      "Epoch 360/500, Train Loss: 0.6287, Val Loss: 0.6649, Train Acc: 62.80%, Val Acc: 57.50%\n",
      "Epoch 361/500, Train Loss: 0.6283, Val Loss: 0.6465, Train Acc: 62.80%, Val Acc: 60.50%\n",
      "Epoch 362/500, Train Loss: 0.6216, Val Loss: 0.6427, Train Acc: 62.40%, Val Acc: 67.50%\n",
      "Epoch 363/500, Train Loss: 0.6430, Val Loss: 0.6977, Train Acc: 60.20%, Val Acc: 58.50%\n",
      "Epoch 364/500, Train Loss: 0.6558, Val Loss: 0.6669, Train Acc: 60.00%, Val Acc: 55.50%\n",
      "Epoch 365/500, Train Loss: 0.6288, Val Loss: 0.6129, Train Acc: 59.80%, Val Acc: 65.50%\n",
      "Epoch 366/500, Train Loss: 0.6073, Val Loss: 0.6406, Train Acc: 64.20%, Val Acc: 65.00%\n",
      "Epoch 367/500, Train Loss: 0.6640, Val Loss: 0.6919, Train Acc: 59.60%, Val Acc: 53.50%\n",
      "Epoch 368/500, Train Loss: 0.6434, Val Loss: 0.6646, Train Acc: 60.60%, Val Acc: 56.50%\n",
      "Epoch 369/500, Train Loss: 0.6371, Val Loss: 0.6427, Train Acc: 63.40%, Val Acc: 61.00%\n",
      "Epoch 370/500, Train Loss: 0.6385, Val Loss: 0.6889, Train Acc: 64.20%, Val Acc: 50.50%\n",
      "Epoch 371/500, Train Loss: 0.6216, Val Loss: 0.6351, Train Acc: 62.60%, Val Acc: 64.00%\n",
      "Epoch 372/500, Train Loss: 0.6249, Val Loss: 0.6038, Train Acc: 64.40%, Val Acc: 67.00%\n",
      "Epoch 373/500, Train Loss: 0.6408, Val Loss: 0.6373, Train Acc: 61.60%, Val Acc: 60.00%\n",
      "Epoch 374/500, Train Loss: 0.6381, Val Loss: 0.7048, Train Acc: 60.60%, Val Acc: 59.00%\n",
      "Epoch 375/500, Train Loss: 0.6198, Val Loss: 0.6895, Train Acc: 63.40%, Val Acc: 58.00%\n",
      "Epoch 376/500, Train Loss: 0.6561, Val Loss: 0.6501, Train Acc: 60.40%, Val Acc: 58.00%\n",
      "Epoch 377/500, Train Loss: 0.6232, Val Loss: 0.6520, Train Acc: 62.80%, Val Acc: 67.50%\n",
      "Epoch 378/500, Train Loss: 0.6336, Val Loss: 0.7990, Train Acc: 61.20%, Val Acc: 58.50%\n",
      "Epoch 379/500, Train Loss: 0.6739, Val Loss: 0.8983, Train Acc: 61.40%, Val Acc: 50.50%\n",
      "Epoch 380/500, Train Loss: 0.6735, Val Loss: 0.6871, Train Acc: 56.20%, Val Acc: 54.00%\n",
      "Epoch 381/500, Train Loss: 0.6544, Val Loss: 0.6678, Train Acc: 61.20%, Val Acc: 58.00%\n",
      "Epoch 382/500, Train Loss: 0.6296, Val Loss: 0.6740, Train Acc: 61.80%, Val Acc: 51.50%\n",
      "Epoch 383/500, Train Loss: 0.6380, Val Loss: 0.6314, Train Acc: 60.80%, Val Acc: 61.00%\n",
      "Epoch 384/500, Train Loss: 0.6265, Val Loss: 0.6096, Train Acc: 60.40%, Val Acc: 67.00%\n",
      "Epoch 385/500, Train Loss: 0.6102, Val Loss: 0.6197, Train Acc: 66.80%, Val Acc: 65.00%\n",
      "Epoch 386/500, Train Loss: 0.6438, Val Loss: 0.7098, Train Acc: 60.00%, Val Acc: 58.50%\n",
      "Epoch 387/500, Train Loss: 0.6341, Val Loss: 0.6421, Train Acc: 63.00%, Val Acc: 58.00%\n",
      "Epoch 388/500, Train Loss: 0.6726, Val Loss: 0.6262, Train Acc: 56.80%, Val Acc: 64.00%\n",
      "Epoch 389/500, Train Loss: 0.6563, Val Loss: 0.6459, Train Acc: 59.20%, Val Acc: 56.00%\n",
      "Epoch 390/500, Train Loss: 0.6492, Val Loss: 0.6394, Train Acc: 61.20%, Val Acc: 61.50%\n",
      "Epoch 391/500, Train Loss: 0.6430, Val Loss: 0.6714, Train Acc: 59.80%, Val Acc: 53.00%\n",
      "Epoch 392/500, Train Loss: 0.6532, Val Loss: 0.6016, Train Acc: 59.00%, Val Acc: 67.00%\n",
      "Epoch 393/500, Train Loss: 0.6289, Val Loss: 0.6406, Train Acc: 63.20%, Val Acc: 61.00%\n",
      "Epoch 394/500, Train Loss: 0.6260, Val Loss: 0.7977, Train Acc: 63.00%, Val Acc: 50.00%\n",
      "Epoch 395/500, Train Loss: 0.6390, Val Loss: 0.6353, Train Acc: 63.20%, Val Acc: 60.50%\n",
      "Epoch 396/500, Train Loss: 0.6177, Val Loss: 0.6393, Train Acc: 63.00%, Val Acc: 60.00%\n",
      "Epoch 397/500, Train Loss: 0.6481, Val Loss: 0.6222, Train Acc: 56.80%, Val Acc: 66.50%\n",
      "Epoch 398/500, Train Loss: 0.6301, Val Loss: 0.6335, Train Acc: 62.20%, Val Acc: 66.50%\n",
      "Epoch 399/500, Train Loss: 0.6278, Val Loss: 0.6077, Train Acc: 63.00%, Val Acc: 64.00%\n",
      "Epoch 400/500, Train Loss: 0.6135, Val Loss: 0.6220, Train Acc: 61.40%, Val Acc: 64.00%\n",
      "Epoch 401/500, Train Loss: 0.6416, Val Loss: 0.6414, Train Acc: 59.60%, Val Acc: 62.50%\n",
      "Epoch 402/500, Train Loss: 0.6524, Val Loss: 0.7806, Train Acc: 61.20%, Val Acc: 50.50%\n",
      "Epoch 403/500, Train Loss: 0.6513, Val Loss: 0.6627, Train Acc: 61.20%, Val Acc: 58.50%\n",
      "Epoch 404/500, Train Loss: 0.6246, Val Loss: 0.6868, Train Acc: 63.80%, Val Acc: 58.00%\n",
      "Epoch 405/500, Train Loss: 0.6409, Val Loss: 0.6549, Train Acc: 62.20%, Val Acc: 57.00%\n",
      "Epoch 406/500, Train Loss: 0.6475, Val Loss: 0.7143, Train Acc: 60.20%, Val Acc: 51.00%\n",
      "Epoch 407/500, Train Loss: 0.6381, Val Loss: 0.7821, Train Acc: 62.40%, Val Acc: 58.50%\n",
      "Epoch 408/500, Train Loss: 0.6475, Val Loss: 0.6142, Train Acc: 60.00%, Val Acc: 66.50%\n",
      "Epoch 409/500, Train Loss: 0.6466, Val Loss: 0.6606, Train Acc: 59.40%, Val Acc: 53.50%\n",
      "Epoch 410/500, Train Loss: 0.6342, Val Loss: 0.6409, Train Acc: 60.00%, Val Acc: 60.00%\n",
      "Epoch 411/500, Train Loss: 0.6315, Val Loss: 0.6193, Train Acc: 60.20%, Val Acc: 66.00%\n",
      "Epoch 412/500, Train Loss: 0.6266, Val Loss: 0.6077, Train Acc: 63.00%, Val Acc: 66.50%\n",
      "Epoch 413/500, Train Loss: 0.6318, Val Loss: 0.6476, Train Acc: 60.60%, Val Acc: 61.50%\n",
      "Epoch 414/500, Train Loss: 0.6397, Val Loss: 0.6206, Train Acc: 58.40%, Val Acc: 64.00%\n",
      "Epoch 415/500, Train Loss: 0.6225, Val Loss: 0.6666, Train Acc: 64.20%, Val Acc: 58.00%\n",
      "Epoch 416/500, Train Loss: 0.6562, Val Loss: 0.7513, Train Acc: 63.00%, Val Acc: 50.50%\n",
      "Epoch 417/500, Train Loss: 0.6683, Val Loss: 0.6471, Train Acc: 58.40%, Val Acc: 60.50%\n",
      "Epoch 418/500, Train Loss: 0.6204, Val Loss: 0.6196, Train Acc: 60.60%, Val Acc: 63.50%\n",
      "Epoch 419/500, Train Loss: 0.6286, Val Loss: 0.6061, Train Acc: 60.80%, Val Acc: 68.50%\n",
      "Epoch 420/500, Train Loss: 0.6463, Val Loss: 0.6882, Train Acc: 60.60%, Val Acc: 54.00%\n",
      "Epoch 421/500, Train Loss: 0.6479, Val Loss: 0.6357, Train Acc: 61.20%, Val Acc: 62.50%\n",
      "Epoch 422/500, Train Loss: 0.6360, Val Loss: 0.6223, Train Acc: 61.80%, Val Acc: 66.50%\n",
      "Epoch 423/500, Train Loss: 0.6366, Val Loss: 0.6897, Train Acc: 61.00%, Val Acc: 56.00%\n",
      "Epoch 424/500, Train Loss: 0.6546, Val Loss: 0.6702, Train Acc: 56.80%, Val Acc: 55.50%\n",
      "Epoch 425/500, Train Loss: 0.6530, Val Loss: 0.7166, Train Acc: 59.60%, Val Acc: 56.00%\n",
      "Epoch 426/500, Train Loss: 0.6461, Val Loss: 0.6382, Train Acc: 62.20%, Val Acc: 64.50%\n",
      "Epoch 427/500, Train Loss: 0.6511, Val Loss: 0.6629, Train Acc: 61.00%, Val Acc: 51.50%\n",
      "Epoch 428/500, Train Loss: 0.6605, Val Loss: 0.6540, Train Acc: 60.00%, Val Acc: 60.00%\n",
      "Epoch 429/500, Train Loss: 0.6615, Val Loss: 0.6455, Train Acc: 62.60%, Val Acc: 59.50%\n",
      "Epoch 430/500, Train Loss: 0.6386, Val Loss: 0.6395, Train Acc: 64.00%, Val Acc: 59.00%\n",
      "Epoch 431/500, Train Loss: 0.6476, Val Loss: 0.6460, Train Acc: 61.00%, Val Acc: 59.00%\n",
      "Epoch 432/500, Train Loss: 0.6429, Val Loss: 0.6364, Train Acc: 59.40%, Val Acc: 63.50%\n",
      "Epoch 433/500, Train Loss: 0.6240, Val Loss: 0.6680, Train Acc: 63.00%, Val Acc: 51.50%\n",
      "Epoch 434/500, Train Loss: 0.6185, Val Loss: 0.6854, Train Acc: 61.80%, Val Acc: 59.00%\n",
      "Epoch 435/500, Train Loss: 0.6370, Val Loss: 0.6600, Train Acc: 60.80%, Val Acc: 59.00%\n",
      "Epoch 436/500, Train Loss: 0.6635, Val Loss: 0.6282, Train Acc: 57.00%, Val Acc: 64.00%\n",
      "Epoch 437/500, Train Loss: 0.6235, Val Loss: 0.6180, Train Acc: 63.60%, Val Acc: 62.50%\n",
      "Epoch 438/500, Train Loss: 0.6267, Val Loss: 0.6359, Train Acc: 64.00%, Val Acc: 60.50%\n",
      "Epoch 439/500, Train Loss: 0.6285, Val Loss: 0.6409, Train Acc: 61.00%, Val Acc: 61.00%\n",
      "Epoch 440/500, Train Loss: 0.6335, Val Loss: 0.6132, Train Acc: 62.00%, Val Acc: 64.00%\n",
      "Epoch 441/500, Train Loss: 0.6217, Val Loss: 0.6794, Train Acc: 65.60%, Val Acc: 55.00%\n",
      "Epoch 442/500, Train Loss: 0.6644, Val Loss: 0.6806, Train Acc: 63.20%, Val Acc: 56.50%\n",
      "Epoch 443/500, Train Loss: 0.6646, Val Loss: 0.6367, Train Acc: 60.40%, Val Acc: 64.50%\n",
      "Epoch 444/500, Train Loss: 0.6335, Val Loss: 0.6728, Train Acc: 63.60%, Val Acc: 54.00%\n",
      "Epoch 445/500, Train Loss: 0.6157, Val Loss: 0.6338, Train Acc: 65.20%, Val Acc: 62.50%\n",
      "Epoch 446/500, Train Loss: 0.6538, Val Loss: 0.6115, Train Acc: 60.80%, Val Acc: 66.00%\n",
      "Epoch 447/500, Train Loss: 0.6243, Val Loss: 0.6241, Train Acc: 67.60%, Val Acc: 62.00%\n",
      "Epoch 448/500, Train Loss: 0.6454, Val Loss: 0.6441, Train Acc: 61.40%, Val Acc: 58.50%\n",
      "Epoch 449/500, Train Loss: 0.6274, Val Loss: 0.6924, Train Acc: 60.00%, Val Acc: 57.00%\n",
      "Epoch 450/500, Train Loss: 0.6252, Val Loss: 0.6034, Train Acc: 62.80%, Val Acc: 65.50%\n",
      "Epoch 451/500, Train Loss: 0.6163, Val Loss: 0.6427, Train Acc: 64.60%, Val Acc: 58.50%\n",
      "Epoch 452/500, Train Loss: 0.6359, Val Loss: 0.6159, Train Acc: 63.60%, Val Acc: 67.00%\n",
      "Epoch 453/500, Train Loss: 0.6457, Val Loss: 0.6229, Train Acc: 60.40%, Val Acc: 65.00%\n",
      "Epoch 454/500, Train Loss: 0.5989, Val Loss: 0.6730, Train Acc: 64.00%, Val Acc: 60.00%\n",
      "Epoch 455/500, Train Loss: 0.6314, Val Loss: 0.6130, Train Acc: 61.60%, Val Acc: 67.00%\n",
      "Epoch 456/500, Train Loss: 0.6461, Val Loss: 0.6447, Train Acc: 61.20%, Val Acc: 59.50%\n",
      "Epoch 457/500, Train Loss: 0.6516, Val Loss: 0.7180, Train Acc: 60.80%, Val Acc: 52.50%\n",
      "Epoch 458/500, Train Loss: 0.6282, Val Loss: 0.6177, Train Acc: 62.60%, Val Acc: 70.00%\n",
      "Epoch 459/500, Train Loss: 0.6083, Val Loss: 0.6327, Train Acc: 65.20%, Val Acc: 61.50%\n",
      "Epoch 460/500, Train Loss: 0.6280, Val Loss: 0.6197, Train Acc: 59.80%, Val Acc: 63.50%\n",
      "Epoch 461/500, Train Loss: 0.6296, Val Loss: 0.6187, Train Acc: 62.20%, Val Acc: 67.00%\n",
      "Epoch 462/500, Train Loss: 0.6290, Val Loss: 0.6244, Train Acc: 62.80%, Val Acc: 67.00%\n",
      "Epoch 463/500, Train Loss: 0.6164, Val Loss: 0.6103, Train Acc: 64.00%, Val Acc: 67.00%\n",
      "Epoch 464/500, Train Loss: 0.6473, Val Loss: 0.6333, Train Acc: 61.20%, Val Acc: 62.00%\n",
      "Epoch 465/500, Train Loss: 0.6209, Val Loss: 0.6077, Train Acc: 60.00%, Val Acc: 67.50%\n",
      "Epoch 466/500, Train Loss: 0.6184, Val Loss: 0.6139, Train Acc: 63.20%, Val Acc: 62.50%\n",
      "Epoch 467/500, Train Loss: 0.6400, Val Loss: 0.6181, Train Acc: 60.40%, Val Acc: 63.50%\n",
      "Epoch 468/500, Train Loss: 0.6248, Val Loss: 0.6468, Train Acc: 62.60%, Val Acc: 59.50%\n",
      "Epoch 469/500, Train Loss: 0.6257, Val Loss: 0.6282, Train Acc: 65.40%, Val Acc: 60.50%\n",
      "Epoch 470/500, Train Loss: 0.6268, Val Loss: 0.5985, Train Acc: 62.00%, Val Acc: 66.50%\n",
      "Epoch 471/500, Train Loss: 0.6410, Val Loss: 0.6454, Train Acc: 62.00%, Val Acc: 61.00%\n",
      "Epoch 472/500, Train Loss: 0.6068, Val Loss: 0.7666, Train Acc: 62.20%, Val Acc: 59.50%\n",
      "Epoch 473/500, Train Loss: 0.6282, Val Loss: 0.6481, Train Acc: 63.00%, Val Acc: 60.00%\n",
      "Epoch 474/500, Train Loss: 0.6452, Val Loss: 0.6060, Train Acc: 61.00%, Val Acc: 67.00%\n",
      "Epoch 475/500, Train Loss: 0.6441, Val Loss: 0.6672, Train Acc: 60.20%, Val Acc: 59.50%\n",
      "Epoch 476/500, Train Loss: 0.6696, Val Loss: 0.6784, Train Acc: 58.40%, Val Acc: 53.00%\n",
      "Epoch 477/500, Train Loss: 0.6188, Val Loss: 0.6248, Train Acc: 64.00%, Val Acc: 64.00%\n",
      "Epoch 478/500, Train Loss: 0.6075, Val Loss: 0.6684, Train Acc: 62.20%, Val Acc: 59.00%\n",
      "Epoch 479/500, Train Loss: 0.6327, Val Loss: 0.6283, Train Acc: 62.00%, Val Acc: 60.50%\n",
      "Epoch 480/500, Train Loss: 0.6301, Val Loss: 0.7370, Train Acc: 64.20%, Val Acc: 57.50%\n",
      "Epoch 481/500, Train Loss: 0.6258, Val Loss: 0.6478, Train Acc: 67.80%, Val Acc: 60.00%\n",
      "Epoch 482/500, Train Loss: 0.6511, Val Loss: 0.6402, Train Acc: 61.00%, Val Acc: 63.50%\n",
      "Epoch 483/500, Train Loss: 0.6434, Val Loss: 0.6473, Train Acc: 59.20%, Val Acc: 62.00%\n",
      "Epoch 484/500, Train Loss: 0.6259, Val Loss: 0.6842, Train Acc: 61.80%, Val Acc: 54.50%\n",
      "Epoch 485/500, Train Loss: 0.6341, Val Loss: 0.6434, Train Acc: 59.80%, Val Acc: 60.50%\n",
      "Epoch 486/500, Train Loss: 0.6654, Val Loss: 0.6119, Train Acc: 60.60%, Val Acc: 62.50%\n",
      "Epoch 487/500, Train Loss: 0.6369, Val Loss: 0.6326, Train Acc: 62.40%, Val Acc: 59.50%\n",
      "Epoch 488/500, Train Loss: 0.6528, Val Loss: 0.6422, Train Acc: 56.20%, Val Acc: 60.50%\n",
      "Epoch 489/500, Train Loss: 0.6481, Val Loss: 0.7844, Train Acc: 54.80%, Val Acc: 51.50%\n",
      "Epoch 490/500, Train Loss: 0.6424, Val Loss: 0.6738, Train Acc: 61.60%, Val Acc: 58.50%\n",
      "Epoch 491/500, Train Loss: 0.6414, Val Loss: 0.6244, Train Acc: 57.40%, Val Acc: 62.00%\n",
      "Epoch 492/500, Train Loss: 0.6345, Val Loss: 0.6626, Train Acc: 63.00%, Val Acc: 57.00%\n",
      "Epoch 493/500, Train Loss: 0.6323, Val Loss: 0.6403, Train Acc: 63.00%, Val Acc: 57.50%\n",
      "Epoch 494/500, Train Loss: 0.6131, Val Loss: 0.6229, Train Acc: 65.40%, Val Acc: 61.50%\n",
      "Epoch 495/500, Train Loss: 0.6321, Val Loss: 0.6199, Train Acc: 61.00%, Val Acc: 62.50%\n",
      "Epoch 496/500, Train Loss: 0.6158, Val Loss: 0.6145, Train Acc: 64.40%, Val Acc: 64.00%\n",
      "Epoch 497/500, Train Loss: 0.6277, Val Loss: 0.6226, Train Acc: 59.20%, Val Acc: 63.00%\n",
      "Epoch 498/500, Train Loss: 0.6651, Val Loss: 0.6389, Train Acc: 57.20%, Val Acc: 61.50%\n",
      "Epoch 499/500, Train Loss: 0.6381, Val Loss: 0.6833, Train Acc: 62.40%, Val Acc: 53.00%\n",
      "Epoch 500/500, Train Loss: 0.6364, Val Loss: 0.6354, Train Acc: 61.40%, Val Acc: 65.50%\n",
      "Best validation accuracy: 70.0%\n"
     ]
    }
   ],
   "source": [
    "model_name ='amazonclasssimplew2v'\n",
    "input_size = train_x_tensor_w2v.shape[1]\n",
    "model = SimpleANNClassifier(input_size, model_name)\n",
    "model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "trained_model = train_model(model, criterion, optimizer, train_loader_w2v, val_loader_w2v, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e3cf246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing dataset: 66.33333333333333%\n",
      "F1-Score: 0.64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load best model\n",
    "model = SimpleANNClassifier(input_size, model_name)\n",
    "model.load_state_dict(torch.load(model.name, weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate best model\n",
    "_, test_acc, labels, preds = test_model(model=model, loader=test_loader_w2v, criterion=criterion, input_size=input_size)\n",
    "print(f'Accuracy on testing dataset: {test_acc}%')\n",
    "print(f'F1-Score: {f1_score(labels, preds):.2f}')\n",
    "plot_confusion_matrix(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a68b9c1",
   "metadata": {},
   "source": [
    "larger dataset and larger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e212c148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.5057, Val Loss: 0.4617, Train Acc: 74.79%, Val Acc: 76.26%\n",
      "Epoch 2/50, Train Loss: 0.4550, Val Loss: 0.4558, Train Acc: 78.30%, Val Acc: 78.18%\n",
      "Epoch 3/50, Train Loss: 0.4443, Val Loss: 0.4422, Train Acc: 79.14%, Val Acc: 78.26%\n",
      "Epoch 4/50, Train Loss: 0.4369, Val Loss: 0.4343, Train Acc: 79.62%, Val Acc: 78.14%\n",
      "Epoch 5/50, Train Loss: 0.4279, Val Loss: 0.4314, Train Acc: 79.76%, Val Acc: 79.16%\n",
      "Epoch 6/50, Train Loss: 0.4267, Val Loss: 0.4455, Train Acc: 79.81%, Val Acc: 77.80%\n",
      "Epoch 7/50, Train Loss: 0.4254, Val Loss: 0.4785, Train Acc: 80.06%, Val Acc: 77.04%\n",
      "Epoch 8/50, Train Loss: 0.4195, Val Loss: 0.4444, Train Acc: 80.18%, Val Acc: 78.68%\n",
      "Epoch 9/50, Train Loss: 0.4178, Val Loss: 0.4305, Train Acc: 80.36%, Val Acc: 79.34%\n",
      "Epoch 10/50, Train Loss: 0.4105, Val Loss: 0.4372, Train Acc: 80.94%, Val Acc: 78.50%\n",
      "Epoch 11/50, Train Loss: 0.4063, Val Loss: 0.4377, Train Acc: 80.98%, Val Acc: 78.98%\n",
      "Epoch 12/50, Train Loss: 0.4055, Val Loss: 0.4488, Train Acc: 81.60%, Val Acc: 78.14%\n",
      "Epoch 13/50, Train Loss: 0.4010, Val Loss: 0.4339, Train Acc: 80.87%, Val Acc: 79.22%\n",
      "Epoch 14/50, Train Loss: 0.3981, Val Loss: 0.4375, Train Acc: 81.03%, Val Acc: 79.32%\n",
      "Epoch 15/50, Train Loss: 0.3879, Val Loss: 0.4386, Train Acc: 81.62%, Val Acc: 79.22%\n",
      "Epoch 16/50, Train Loss: 0.3927, Val Loss: 0.4681, Train Acc: 81.42%, Val Acc: 76.80%\n",
      "Epoch 17/50, Train Loss: 0.3882, Val Loss: 0.4397, Train Acc: 81.82%, Val Acc: 78.52%\n",
      "Epoch 18/50, Train Loss: 0.3846, Val Loss: 0.4622, Train Acc: 81.73%, Val Acc: 77.86%\n",
      "Epoch 19/50, Train Loss: 0.3855, Val Loss: 0.4340, Train Acc: 82.17%, Val Acc: 79.60%\n",
      "Epoch 20/50, Train Loss: 0.3760, Val Loss: 0.4348, Train Acc: 82.13%, Val Acc: 79.60%\n",
      "Epoch 21/50, Train Loss: 0.3767, Val Loss: 0.4590, Train Acc: 82.51%, Val Acc: 78.26%\n",
      "Epoch 22/50, Train Loss: 0.3760, Val Loss: 0.4648, Train Acc: 82.61%, Val Acc: 78.78%\n",
      "Epoch 23/50, Train Loss: 0.3662, Val Loss: 0.4483, Train Acc: 83.18%, Val Acc: 78.12%\n",
      "Epoch 24/50, Train Loss: 0.3754, Val Loss: 0.4388, Train Acc: 82.40%, Val Acc: 79.12%\n",
      "Epoch 25/50, Train Loss: 0.3606, Val Loss: 0.4478, Train Acc: 83.50%, Val Acc: 78.64%\n",
      "Epoch 26/50, Train Loss: 0.3556, Val Loss: 0.4584, Train Acc: 83.68%, Val Acc: 78.12%\n",
      "Epoch 27/50, Train Loss: 0.3490, Val Loss: 0.4853, Train Acc: 83.86%, Val Acc: 78.12%\n",
      "Epoch 28/50, Train Loss: 0.3563, Val Loss: 0.4991, Train Acc: 83.66%, Val Acc: 76.54%\n",
      "Epoch 29/50, Train Loss: 0.3516, Val Loss: 0.4628, Train Acc: 84.02%, Val Acc: 78.34%\n",
      "Epoch 30/50, Train Loss: 0.3455, Val Loss: 0.4610, Train Acc: 84.36%, Val Acc: 78.00%\n",
      "Epoch 31/50, Train Loss: 0.3410, Val Loss: 0.4667, Train Acc: 84.36%, Val Acc: 78.00%\n",
      "Epoch 32/50, Train Loss: 0.3443, Val Loss: 0.4584, Train Acc: 84.38%, Val Acc: 78.94%\n",
      "Epoch 33/50, Train Loss: 0.3378, Val Loss: 0.4735, Train Acc: 84.45%, Val Acc: 78.76%\n",
      "Epoch 34/50, Train Loss: 0.3364, Val Loss: 0.4679, Train Acc: 84.67%, Val Acc: 78.56%\n",
      "Epoch 35/50, Train Loss: 0.3317, Val Loss: 0.4650, Train Acc: 84.60%, Val Acc: 78.90%\n",
      "Epoch 36/50, Train Loss: 0.3319, Val Loss: 0.5008, Train Acc: 84.94%, Val Acc: 78.58%\n",
      "Epoch 37/50, Train Loss: 0.3271, Val Loss: 0.4655, Train Acc: 85.39%, Val Acc: 78.74%\n",
      "Epoch 38/50, Train Loss: 0.3231, Val Loss: 0.5756, Train Acc: 85.26%, Val Acc: 74.68%\n",
      "Epoch 39/50, Train Loss: 0.3256, Val Loss: 0.5295, Train Acc: 85.13%, Val Acc: 77.30%\n",
      "Epoch 40/50, Train Loss: 0.3196, Val Loss: 0.4857, Train Acc: 85.60%, Val Acc: 78.32%\n",
      "Epoch 41/50, Train Loss: 0.3178, Val Loss: 0.5178, Train Acc: 85.50%, Val Acc: 78.14%\n",
      "Epoch 42/50, Train Loss: 0.3155, Val Loss: 0.4826, Train Acc: 85.61%, Val Acc: 78.46%\n",
      "Epoch 43/50, Train Loss: 0.3124, Val Loss: 0.4997, Train Acc: 85.48%, Val Acc: 77.48%\n",
      "Epoch 44/50, Train Loss: 0.3073, Val Loss: 0.4938, Train Acc: 86.02%, Val Acc: 78.14%\n",
      "Epoch 45/50, Train Loss: 0.3126, Val Loss: 0.4879, Train Acc: 86.02%, Val Acc: 77.90%\n",
      "Epoch 46/50, Train Loss: 0.3038, Val Loss: 0.4825, Train Acc: 86.06%, Val Acc: 78.32%\n",
      "Epoch 47/50, Train Loss: 0.3042, Val Loss: 0.4996, Train Acc: 86.57%, Val Acc: 78.20%\n",
      "Epoch 48/50, Train Loss: 0.2993, Val Loss: 0.5191, Train Acc: 86.60%, Val Acc: 78.00%\n",
      "Epoch 49/50, Train Loss: 0.2950, Val Loss: 0.5021, Train Acc: 87.17%, Val Acc: 77.40%\n",
      "Epoch 50/50, Train Loss: 0.2924, Val Loss: 0.5783, Train Acc: 87.10%, Val Acc: 76.08%\n",
      "Best validation accuracy: 79.6%\n"
     ]
    }
   ],
   "source": [
    "model_name ='amazonclasssimplew2vlarge'\n",
    "input_size = train_x_tensor_w2v_l.shape[1]\n",
    "model = ANNClassifierLarge(input_size, model_name)\n",
    "model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "trained_model = train_model(model, criterion, optimizer, train_loader_w2v_l, val_loader_w2v_l, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d921b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing dataset: 79.17333333333333%\n",
      "F1-Score: 0.83\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load best model\n",
    "model = ANNClassifierLarge(input_size, model_name)\n",
    "model.load_state_dict(torch.load(model.name, weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate best model\n",
    "_, test_acc, labels, preds = test_model(model=model, loader=test_loader_w2v_l, criterion=criterion, input_size=input_size)\n",
    "print(f'Accuracy on testing dataset: {test_acc}%')\n",
    "print(f'F1-Score: {f1_score(labels, preds):.2f}')\n",
    "plot_confusion_matrix(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc33bb21",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48190c",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b5da0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RoBERTa\n",
    "\n",
    "In this part we perform feature extraction on the RoBERTa transformer model for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "093e1b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 21:37:52.788208: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-21 21:37:52.802760: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747856272.822054  138574 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747856272.827855  138574 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747856272.842171  138574 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747856272.842188  138574 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747856272.842190  138574 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747856272.842191  138574 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-21 21:37:52.847673: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28bd20",
   "metadata": {},
   "source": [
    "#### Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40325f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0008b295b9af430cb216ed05cd94579f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ec0a79c7354bd7b11ebb6bb6f6e21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8bf930fa9548098dd1cbd2ca247396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34637d3154424c0f8004cf4863022e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641c80fffe4446c7a59534194c666d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7f9da4657d4d5db6c5c0132bac66fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def create_dataset(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    train_dataset = Dataset.from_dict({\"sentence\": X_train, \"labels\": y_train})\n",
    "    val_dataset = Dataset.from_dict({\"sentence\": X_val, \"labels\": y_val})\n",
    "    test_dataset = Dataset.from_dict({\"sentence\": X_test, \"labels\": y_test})\n",
    "    return train_dataset.map(tokenize, batched=True), val_dataset.map(tokenize, batched=True), test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "    return {\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds)\n",
    "    }\n",
    "\n",
    "# Create small and large dataset\n",
    "train_dataset, val_dataset, test_dataset = create_dataset(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "train_dataset_l, val_dataset_l, test_dataset_l = create_dataset(X_train_l, y_train_l, X_val_l, y_val_l, X_test_l, y_test_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e9a5a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 01:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.298316</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.917127</td>\n",
       "      <td>0.882979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.571897</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.254291</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=0.2952616252596416, metrics={'train_runtime': 112.7971, 'train_samples_per_second': 13.298, 'train_steps_per_second': 1.676, 'total_flos': 30062493630000.0, 'train_loss': 0.2952616252596416, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8\n",
    "    ),\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7042399e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20251715183258057, 'eval_precision': 0.9625, 'eval_accuracy': 0.9666666666666667, 'eval_f1': 0.9685534591194969, 'eval_recall': 0.9746835443037974, 'eval_runtime': 3.3241, 'eval_samples_per_second': 90.25, 'eval_steps_per_second': 11.432, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a361fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Playground\n",
    "Cause it is fancy to try around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95722d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n",
      "Probabilities: [[0.0043547 0.9956453]]\n"
     ]
    }
   ],
   "source": [
    "# It is really fancy to try it a bit out\n",
    "text = \"does what it promised\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Probabilities: {probabilities.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07220b52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Large Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14d0d0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4689' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4689/4689 18:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.329300</td>\n",
       "      <td>0.276609</td>\n",
       "      <td>0.935792</td>\n",
       "      <td>0.917800</td>\n",
       "      <td>0.932568</td>\n",
       "      <td>0.929366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.219700</td>\n",
       "      <td>0.253355</td>\n",
       "      <td>0.947941</td>\n",
       "      <td>0.932200</td>\n",
       "      <td>0.944362</td>\n",
       "      <td>0.940811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>0.298459</td>\n",
       "      <td>0.942221</td>\n",
       "      <td>0.936400</td>\n",
       "      <td>0.948343</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4689, training_loss=0.24563814017990807, metrics={'train_runtime': 1082.5085, 'train_samples_per_second': 34.642, 'train_steps_per_second': 4.332, 'total_flos': 9866664576000000.0, 'train_loss': 0.24563814017990807, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_l = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "trainer_l = Trainer(\n",
    "    model=model_l,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./results_large\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs_large\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8\n",
    "    ),\n",
    "    train_dataset = train_dataset_l,\n",
    "    eval_dataset = val_dataset_l,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer_l.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63145cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='938' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [938/938 00:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2807043194770813, 'eval_precision': 0.9447918913090361, 'eval_accuracy': 0.9389333333333333, 'eval_f1': 0.9503253796095444, 'eval_recall': 0.9559240672048877, 'eval_runtime': 54.6927, 'eval_samples_per_second': 137.13, 'eval_steps_per_second': 17.15, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer_l.evaluate(\n",
    "    eval_dataset=test_dataset_l,\n",
    ")\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c63c4-4cd0-4782-97a7-7e358a94fdbf",
   "metadata": {},
   "source": [
    "### WandB optimisation of RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "920e3c5a-2397-4dd5-9cee-b92a18010ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 23:58:58.233282: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-21 23:58:58.248536: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747864738.269315  147385 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747864738.275321  147385 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747864738.290939  147385 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747864738.290955  147385 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747864738.290957  147385 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747864738.290958  147385 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-21 23:58:58.296552: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "370597bd-9ca7-4eac-8777-882b7ac88210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28034ff77c1c4acd83418f06b3b7e74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9d89dae29847609cd820525b6f25c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d9f5e151ae48dd86632cf28631dc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def create_dataset(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    train_dataset = Dataset.from_dict({\"sentence\": X_train, \"labels\": y_train})\n",
    "    val_dataset = Dataset.from_dict({\"sentence\": X_val, \"labels\": y_val})\n",
    "    test_dataset = Dataset.from_dict({\"sentence\": X_test, \"labels\": y_test})\n",
    "    return train_dataset.map(tokenize, batched=True), val_dataset.map(tokenize, batched=True), test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "    return {\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds)\n",
    "    }\n",
    "\n",
    "# Create small and large dataset\n",
    "train_dataset, val_dataset, test_dataset = create_dataset(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "#train_dataset_l, val_dataset_l, test_dataset_l = create_dataset(X_train_l, y_train_l, X_val_l, y_val_l, X_test_l, y_test_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451e5f1-15cf-4611-8da5-2fe9daa3a9c5",
   "metadata": {},
   "source": [
    "Small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d460ef0f-3867-4bcd-8607-f3aad7149efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "810d4c15-2845-4e14-bf48-d5409c864197",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"eval/f1\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"min\": 1e-6,\n",
    "            \"max\": 5e-5\n",
    "        },\n",
    "        \"per_device_train_batch_size\": {\n",
    "            \"values\": [8, 16, 32]\n",
    "        },\n",
    "        \"num_train_epochs\": {\n",
    "            \"values\": [2, 3, 4]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d049389d-a1a7-40bc-b383-9b5240403759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: wecneufa\n",
      "Sweep URL: https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"roberta-sweep-lab1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66284ddd-f88f-41f0-83bf-403f8a83c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    with wandb.init() as run:\n",
    "        config = run.config\n",
    "\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            logging_dir=\"./logs\",\n",
    "            report_to=\"wandb\",\n",
    "            learning_rate=config.learning_rate,\n",
    "            per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "            num_train_epochs=config.num_train_epochs,\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # Save model \n",
    "        model_dir = f\"./models/{wandb.run.name}\"\n",
    "        trainer.save_model(model_dir)\n",
    "        wandb.save(f\"{model_dir}/*\") \n",
    "\n",
    "        del trainer\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d02f14a-51f9-4d3f-910d-5df2611e9275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: isndre95 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.185334974456161e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_225406-isndre95</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/isndre95' target=\"_blank\">decent-sweep-1</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/isndre95' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/isndre95</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 01:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.611400</td>\n",
       "      <td>0.418820</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.240700</td>\n",
       "      <td>0.257434</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.943005</td>\n",
       "      <td>0.968085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.188200</td>\n",
       "      <td>0.256254</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>█▁█</td></tr><tr><td>eval/f1</td><td>▇▁█</td></tr><tr><td>eval/loss</td><td>█▁▁</td></tr><tr><td>eval/precision</td><td>█▁▆</td></tr><tr><td>eval/recall</td><td>▁█▅</td></tr><tr><td>eval/runtime</td><td>▁▅█</td></tr><tr><td>eval/samples_per_second</td><td>█▄▁</td></tr><tr><td>eval/steps_per_second</td><td>█▄▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▁▁▂▂▃▁▇█▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▆▅▅▄▄▃▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>████▇▇▅▃▂▃▃▃▃▂▃▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.95</td></tr><tr><td>eval/f1</td><td>0.94737</td></tr><tr><td>eval/loss</td><td>0.25625</td></tr><tr><td>eval/precision</td><td>0.9375</td></tr><tr><td>eval/recall</td><td>0.95745</td></tr><tr><td>eval/runtime</td><td>2.0398</td></tr><tr><td>eval/samples_per_second</td><td>98.05</td></tr><tr><td>eval/steps_per_second</td><td>12.256</td></tr><tr><td>total_flos</td><td>394666583040000.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>189</td></tr><tr><td>train/grad_norm</td><td>0.12638</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1882</td></tr><tr><td>train_loss</td><td>0.35765</td></tr><tr><td>train_runtime</td><td>77.0513</td></tr><tr><td>train_samples_per_second</td><td>19.468</td></tr><tr><td>train_steps_per_second</td><td>2.453</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decent-sweep-1</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/isndre95' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/isndre95</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_225406-isndre95/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: us7ipbia with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4.917464376348869e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_225600-us7ipbia</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/us7ipbia' target=\"_blank\">clean-sweep-2</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/us7ipbia' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/us7ipbia</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 01:43, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.323200</td>\n",
       "      <td>0.317384</td>\n",
       "      <td>0.926316</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.931217</td>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.292700</td>\n",
       "      <td>0.358674</td>\n",
       "      <td>0.871287</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>0.902564</td>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.128200</td>\n",
       "      <td>0.291469</td>\n",
       "      <td>0.977011</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.939227</td>\n",
       "      <td>0.904255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.138500</td>\n",
       "      <td>0.295266</td>\n",
       "      <td>0.936842</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.941799</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▆▁██</td></tr><tr><td>eval/f1</td><td>▆▁██</td></tr><tr><td>eval/loss</td><td>▄█▁▁</td></tr><tr><td>eval/precision</td><td>▅▁█▅</td></tr><tr><td>eval/recall</td><td>▆▆▁█</td></tr><tr><td>eval/runtime</td><td>▁▆▇█</td></tr><tr><td>eval/samples_per_second</td><td>█▃▂▁</td></tr><tr><td>eval/steps_per_second</td><td>█▃▂▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▅▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>▇▇▅▄▄▄█▃▃▅▃▃▅▂▄▅▂▂▃▃▁▃▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.945</td></tr><tr><td>eval/f1</td><td>0.9418</td></tr><tr><td>eval/loss</td><td>0.29527</td></tr><tr><td>eval/precision</td><td>0.93684</td></tr><tr><td>eval/recall</td><td>0.94681</td></tr><tr><td>eval/runtime</td><td>2.0539</td></tr><tr><td>eval/samples_per_second</td><td>97.377</td></tr><tr><td>eval/steps_per_second</td><td>12.172</td></tr><tr><td>total_flos</td><td>526222110720000.0</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>252</td></tr><tr><td>train/grad_norm</td><td>0.07304</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1385</td></tr><tr><td>train_loss</td><td>0.30176</td></tr><tr><td>train_runtime</td><td>103.873</td></tr><tr><td>train_samples_per_second</td><td>19.254</td></tr><tr><td>train_steps_per_second</td><td>2.426</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-sweep-2</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/us7ipbia' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/us7ipbia</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_225600-us7ipbia/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 83wbkcwc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.096975465549019e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_225821-83wbkcwc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/83wbkcwc' target=\"_blank\">smooth-sweep-3</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/83wbkcwc' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/83wbkcwc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 01:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.406900</td>\n",
       "      <td>0.227441</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.947917</td>\n",
       "      <td>0.968085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.253711</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.184600</td>\n",
       "      <td>0.253309</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁██</td></tr><tr><td>eval/f1</td><td>▁██</td></tr><tr><td>eval/loss</td><td>▁██</td></tr><tr><td>eval/precision</td><td>▁██</td></tr><tr><td>eval/recall</td><td>█▁▁</td></tr><tr><td>eval/runtime</td><td>▁▇█</td></tr><tr><td>eval/samples_per_second</td><td>█▂▁</td></tr><tr><td>eval/steps_per_second</td><td>█▂▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▁▁▁▁▁█▃▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▆▅▅▄▄▃▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▆▄▂▅▅▂▃▂▃▁▄▁▂▂▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.96</td></tr><tr><td>eval/f1</td><td>0.95745</td></tr><tr><td>eval/loss</td><td>0.25331</td></tr><tr><td>eval/precision</td><td>0.95745</td></tr><tr><td>eval/recall</td><td>0.95745</td></tr><tr><td>eval/runtime</td><td>2.0519</td></tr><tr><td>eval/samples_per_second</td><td>97.469</td></tr><tr><td>eval/steps_per_second</td><td>12.184</td></tr><tr><td>total_flos</td><td>394666583040000.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>189</td></tr><tr><td>train/grad_norm</td><td>0.06253</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1846</td></tr><tr><td>train_loss</td><td>0.24117</td></tr><tr><td>train_runtime</td><td>77.9679</td></tr><tr><td>train_samples_per_second</td><td>19.239</td></tr><tr><td>train_steps_per_second</td><td>2.424</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smooth-sweep-3</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/83wbkcwc' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/83wbkcwc</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_225821-83wbkcwc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cvs0003s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.828490382974806e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_230015-cvs0003s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/cvs0003s' target=\"_blank\">dashing-sweep-4</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/cvs0003s' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/cvs0003s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 01:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.221451</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.925532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.265164</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.433979</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.968085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.362083</td>\n",
       "      <td>0.883495</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.923858</td>\n",
       "      <td>0.968085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>█▁▃▅</td></tr><tr><td>eval/f1</td><td>█▁▄▅</td></tr><tr><td>eval/loss</td><td>▁▂█▆</td></tr><tr><td>eval/precision</td><td>█▁▁▂</td></tr><tr><td>eval/recall</td><td>▁▅██</td></tr><tr><td>eval/runtime</td><td>▁▅██</td></tr><tr><td>eval/samples_per_second</td><td>█▄▁▁</td></tr><tr><td>eval/steps_per_second</td><td>█▄▁▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▃▄▄▅▅▆▆▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▃▄▄▅▅▆▆▆▇███</td></tr><tr><td>train/grad_norm</td><td>▁▂▁▁▄█▆▆▂▁▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▅▅▄▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▇▃▃▃▂▄▄▁▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.925</td></tr><tr><td>eval/f1</td><td>0.92386</td></tr><tr><td>eval/loss</td><td>0.36208</td></tr><tr><td>eval/precision</td><td>0.8835</td></tr><tr><td>eval/recall</td><td>0.96809</td></tr><tr><td>eval/runtime</td><td>2.0524</td></tr><tr><td>eval/samples_per_second</td><td>97.449</td></tr><tr><td>eval/steps_per_second</td><td>12.181</td></tr><tr><td>total_flos</td><td>526222110720000.0</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>128</td></tr><tr><td>train/grad_norm</td><td>0.27953</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.109</td></tr><tr><td>train_loss</td><td>0.27538</td></tr><tr><td>train_runtime</td><td>99.3193</td></tr><tr><td>train_samples_per_second</td><td>20.137</td></tr><tr><td>train_steps_per_second</td><td>1.289</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dashing-sweep-4</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/cvs0003s' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/cvs0003s</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_230015-cvs0003s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 354ggpcy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2.7507103932419822e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_230229-354ggpcy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/354ggpcy' target=\"_blank\">different-sweep-5</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/354ggpcy' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/354ggpcy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 01:43, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.503800</td>\n",
       "      <td>0.206535</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>0.319048</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.145600</td>\n",
       "      <td>0.232690</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.343996</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▆▁█▁</td></tr><tr><td>eval/f1</td><td>▅▁█▁</td></tr><tr><td>eval/loss</td><td>▁▇▂█</td></tr><tr><td>eval/precision</td><td>█▁▆▁</td></tr><tr><td>eval/recall</td><td>▁███</td></tr><tr><td>eval/runtime</td><td>▁▄██</td></tr><tr><td>eval/samples_per_second</td><td>█▅▁▁</td></tr><tr><td>eval/steps_per_second</td><td>█▅▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▁▁▂▁▁▂█▁▂▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▆▄▂▆▆▂▂▂▂▂▅▂▃▁▁▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.94</td></tr><tr><td>eval/f1</td><td>0.9375</td></tr><tr><td>eval/loss</td><td>0.344</td></tr><tr><td>eval/precision</td><td>0.91837</td></tr><tr><td>eval/recall</td><td>0.95745</td></tr><tr><td>eval/runtime</td><td>2.059</td></tr><tr><td>eval/samples_per_second</td><td>97.135</td></tr><tr><td>eval/steps_per_second</td><td>12.142</td></tr><tr><td>total_flos</td><td>526222110720000.0</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>252</td></tr><tr><td>train/grad_norm</td><td>0.04586</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0011</td></tr><tr><td>train_loss</td><td>0.20074</td></tr><tr><td>train_runtime</td><td>104.0579</td></tr><tr><td>train_samples_per_second</td><td>19.22</td></tr><tr><td>train_steps_per_second</td><td>2.422</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">different-sweep-5</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/354ggpcy' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/354ggpcy</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_230229-354ggpcy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: agwef8p6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2.573010616660904e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_230450-agwef8p6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/agwef8p6' target=\"_blank\">wobbly-sweep-6</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/agwef8p6' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/agwef8p6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.540700</td>\n",
       "      <td>0.203563</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.151600</td>\n",
       "      <td>0.172564</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.209999</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.951872</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁██</td></tr><tr><td>eval/f1</td><td>▁██</td></tr><tr><td>eval/loss</td><td>▇▁█</td></tr><tr><td>eval/precision</td><td>▁▂█</td></tr><tr><td>eval/recall</td><td>▁█▅</td></tr><tr><td>eval/runtime</td><td>▁▅█</td></tr><tr><td>eval/samples_per_second</td><td>█▄▁</td></tr><tr><td>eval/steps_per_second</td><td>█▄▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▃▄▅▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▃▄▅▅▆▇███</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▅█▁▂▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▄▄▃▂▁</td></tr><tr><td>train/loss</td><td>██▆▃▃▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.955</td></tr><tr><td>eval/f1</td><td>0.95187</td></tr><tr><td>eval/loss</td><td>0.21</td></tr><tr><td>eval/precision</td><td>0.95699</td></tr><tr><td>eval/recall</td><td>0.94681</td></tr><tr><td>eval/runtime</td><td>2.0533</td></tr><tr><td>eval/samples_per_second</td><td>97.403</td></tr><tr><td>eval/steps_per_second</td><td>12.175</td></tr><tr><td>total_flos</td><td>394666583040000.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>96</td></tr><tr><td>train/grad_norm</td><td>0.79551</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0189</td></tr><tr><td>train_loss</td><td>0.3038</td></tr><tr><td>train_runtime</td><td>74.329</td></tr><tr><td>train_samples_per_second</td><td>20.181</td></tr><tr><td>train_steps_per_second</td><td>1.292</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wobbly-sweep-6</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/agwef8p6' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/agwef8p6</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_230450-agwef8p6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p6sebaro with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2.8874019251693835e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_230641-p6sebaro</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/p6sebaro' target=\"_blank\">laced-sweep-7</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/p6sebaro' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/p6sebaro</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 01:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>0.212822</td>\n",
       "      <td>0.917526</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.931937</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.171300</td>\n",
       "      <td>0.270593</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.942408</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.092900</td>\n",
       "      <td>0.226091</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.305337</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆█▃</td></tr><tr><td>eval/f1</td><td>▁▆█▄</td></tr><tr><td>eval/loss</td><td>▁▅▂█</td></tr><tr><td>eval/precision</td><td>▁▃█▁</td></tr><tr><td>eval/recall</td><td>▁█▁█</td></tr><tr><td>eval/runtime</td><td>▁▅█▇</td></tr><tr><td>eval/samples_per_second</td><td>█▄▁▂</td></tr><tr><td>eval/steps_per_second</td><td>█▄▁▂</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▃▄▄▅▅▆▆▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▃▄▄▅▅▆▆▆▇███</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁█▂▁▁▄▁▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▅▅▄▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>██▅▃▃▂▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.94</td></tr><tr><td>eval/f1</td><td>0.9375</td></tr><tr><td>eval/loss</td><td>0.30534</td></tr><tr><td>eval/precision</td><td>0.91837</td></tr><tr><td>eval/recall</td><td>0.95745</td></tr><tr><td>eval/runtime</td><td>2.0504</td></tr><tr><td>eval/samples_per_second</td><td>97.541</td></tr><tr><td>eval/steps_per_second</td><td>12.193</td></tr><tr><td>total_flos</td><td>526222110720000.0</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>128</td></tr><tr><td>train/grad_norm</td><td>0.08128</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0605</td></tr><tr><td>train_loss</td><td>0.25566</td></tr><tr><td>train_runtime</td><td>98.9334</td></tr><tr><td>train_samples_per_second</td><td>20.216</td></tr><tr><td>train_steps_per_second</td><td>1.294</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">laced-sweep-7</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/p6sebaro' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/p6sebaro</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_230641-p6sebaro/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cu2y0wpj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2.9973920503521943e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_230852-cu2y0wpj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/cu2y0wpj' target=\"_blank\">lemon-sweep-8</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/cu2y0wpj' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/cu2y0wpj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.447500</td>\n",
       "      <td>0.155788</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.951351</td>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.101600</td>\n",
       "      <td>0.241730</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.932642</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.185561</td>\n",
       "      <td>0.967391</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▇▁█</td></tr><tr><td>eval/f1</td><td>▆▁█</td></tr><tr><td>eval/loss</td><td>▁█▃</td></tr><tr><td>eval/precision</td><td>█▁█</td></tr><tr><td>eval/recall</td><td>▁█▅</td></tr><tr><td>eval/runtime</td><td>▁▃█</td></tr><tr><td>eval/samples_per_second</td><td>█▆▁</td></tr><tr><td>eval/steps_per_second</td><td>█▅▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▃▄▅▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▃▄▅▅▆▇███</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁█▁▁▄▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▄▄▃▂▁</td></tr><tr><td>train/loss</td><td>██▅▃▃▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.96</td></tr><tr><td>eval/f1</td><td>0.95699</td></tr><tr><td>eval/loss</td><td>0.18556</td></tr><tr><td>eval/precision</td><td>0.96739</td></tr><tr><td>eval/recall</td><td>0.94681</td></tr><tr><td>eval/runtime</td><td>2.0575</td></tr><tr><td>eval/samples_per_second</td><td>97.204</td></tr><tr><td>eval/steps_per_second</td><td>12.151</td></tr><tr><td>total_flos</td><td>394666583040000.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>96</td></tr><tr><td>train/grad_norm</td><td>0.64561</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0077</td></tr><tr><td>train_loss</td><td>0.28977</td></tr><tr><td>train_runtime</td><td>74.5845</td></tr><tr><td>train_samples_per_second</td><td>20.111</td></tr><tr><td>train_steps_per_second</td><td>1.287</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lemon-sweep-8</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/cu2y0wpj' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/cu2y0wpj</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_230852-cu2y0wpj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 64smzt72 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.393093929925325e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_231044-64smzt72</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/64smzt72' target=\"_blank\">dandy-sweep-9</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/64smzt72' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/64smzt72</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.350400</td>\n",
       "      <td>0.207008</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.926316</td>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.188300</td>\n",
       "      <td>0.162124</td>\n",
       "      <td>0.967391</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.184019</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.961749</td>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▇█</td></tr><tr><td>eval/f1</td><td>▁▇█</td></tr><tr><td>eval/loss</td><td>█▁▄</td></tr><tr><td>eval/precision</td><td>▁▆█</td></tr><tr><td>eval/recall</td><td>▁█▁</td></tr><tr><td>eval/runtime</td><td>▁▅█</td></tr><tr><td>eval/samples_per_second</td><td>█▄▁</td></tr><tr><td>eval/steps_per_second</td><td>█▄▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▃▄▅▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▃▄▅▅▆▇███</td></tr><tr><td>train/grad_norm</td><td>▁▂▁▁▇█▁▃▂</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▇▄▂▃▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.965</td></tr><tr><td>eval/f1</td><td>0.96175</td></tr><tr><td>eval/loss</td><td>0.18402</td></tr><tr><td>eval/precision</td><td>0.98876</td></tr><tr><td>eval/recall</td><td>0.93617</td></tr><tr><td>eval/runtime</td><td>2.0504</td></tr><tr><td>eval/samples_per_second</td><td>97.541</td></tr><tr><td>eval/steps_per_second</td><td>12.193</td></tr><tr><td>total_flos</td><td>394666583040000.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>96</td></tr><tr><td>train/grad_norm</td><td>5.05258</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0537</td></tr><tr><td>train_loss</td><td>0.28991</td></tr><tr><td>train_runtime</td><td>74.4134</td></tr><tr><td>train_samples_per_second</td><td>20.158</td></tr><tr><td>train_steps_per_second</td><td>1.29</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dandy-sweep-9</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/64smzt72' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/64smzt72</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_231044-64smzt72/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7pv9zaoi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.624221878420868e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_231241-7pv9zaoi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/7pv9zaoi' target=\"_blank\">sage-sweep-10</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/7pv9zaoi' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/7pv9zaoi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.371500</td>\n",
       "      <td>0.192829</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.263675</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.932642</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.152944</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.967391</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▃▁█</td></tr><tr><td>eval/f1</td><td>▃▁█</td></tr><tr><td>eval/loss</td><td>▄█▁</td></tr><tr><td>eval/precision</td><td>▄▁█</td></tr><tr><td>eval/recall</td><td>▁█▅</td></tr><tr><td>eval/runtime</td><td>▁▄█</td></tr><tr><td>eval/samples_per_second</td><td>█▅▁</td></tr><tr><td>eval/steps_per_second</td><td>█▅▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▃▄▅▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▃▄▅▅▆▇███</td></tr><tr><td>train/grad_norm</td><td>▂▂▂▁▁▁▁▁█</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▇▄▂▂▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.97</td></tr><tr><td>eval/f1</td><td>0.96739</td></tr><tr><td>eval/loss</td><td>0.15294</td></tr><tr><td>eval/precision</td><td>0.98889</td></tr><tr><td>eval/recall</td><td>0.94681</td></tr><tr><td>eval/runtime</td><td>2.0629</td></tr><tr><td>eval/samples_per_second</td><td>96.953</td></tr><tr><td>eval/steps_per_second</td><td>12.119</td></tr><tr><td>total_flos</td><td>394666583040000.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>96</td></tr><tr><td>train/grad_norm</td><td>17.7867</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0762</td></tr><tr><td>train_loss</td><td>0.30403</td></tr><tr><td>train_runtime</td><td>74.1018</td></tr><tr><td>train_samples_per_second</td><td>20.242</td></tr><tr><td>train_steps_per_second</td><td>1.296</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sage-sweep-10</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/7pv9zaoi' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/7pv9zaoi</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_231241-7pv9zaoi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ru7oykiz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.911411491292472e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_231432-ru7oykiz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/ru7oykiz' target=\"_blank\">effortless-sweep-11</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/ru7oykiz' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/ru7oykiz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.287900</td>\n",
       "      <td>0.219897</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.925532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>0.251675</td>\n",
       "      <td>0.917526</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.931937</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.193748</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▃▁█</td></tr><tr><td>eval/f1</td><td>▃▁█</td></tr><tr><td>eval/loss</td><td>▄█▁</td></tr><tr><td>eval/precision</td><td>█▁█</td></tr><tr><td>eval/recall</td><td>▁██</td></tr><tr><td>eval/runtime</td><td>▁▅█</td></tr><tr><td>eval/samples_per_second</td><td>█▄▁</td></tr><tr><td>eval/steps_per_second</td><td>█▄▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▃▄▅▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▃▄▅▅▆▇███</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▁▅█▁▃</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▄▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▇▃▃▂▃▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.95</td></tr><tr><td>eval/f1</td><td>0.94681</td></tr><tr><td>eval/loss</td><td>0.19375</td></tr><tr><td>eval/precision</td><td>0.94681</td></tr><tr><td>eval/recall</td><td>0.94681</td></tr><tr><td>eval/runtime</td><td>2.0535</td></tr><tr><td>eval/samples_per_second</td><td>97.393</td></tr><tr><td>eval/steps_per_second</td><td>12.174</td></tr><tr><td>total_flos</td><td>394666583040000.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>96</td></tr><tr><td>train/grad_norm</td><td>31.06029</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0805</td></tr><tr><td>train_loss</td><td>0.31042</td></tr><tr><td>train_runtime</td><td>74.1472</td></tr><tr><td>train_samples_per_second</td><td>20.23</td></tr><tr><td>train_steps_per_second</td><td>1.295</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">effortless-sweep-11</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/ru7oykiz' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/ru7oykiz</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_231432-ru7oykiz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7zeprpaz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.346198159898318e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_231619-7zeprpaz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/7zeprpaz' target=\"_blank\">chocolate-sweep-12</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/7zeprpaz' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/7zeprpaz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 01:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>0.196118</td>\n",
       "      <td>0.926316</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.931217</td>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.262600</td>\n",
       "      <td>0.511779</td>\n",
       "      <td>0.798246</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.968085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.070400</td>\n",
       "      <td>0.257730</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.940541</td>\n",
       "      <td>0.925532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.117500</td>\n",
       "      <td>0.241323</td>\n",
       "      <td>0.936842</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.941799</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▇▁██</td></tr><tr><td>eval/f1</td><td>▇▁██</td></tr><tr><td>eval/loss</td><td>▁█▂▂</td></tr><tr><td>eval/precision</td><td>▇▁█▇</td></tr><tr><td>eval/recall</td><td>▃█▁▅</td></tr><tr><td>eval/runtime</td><td>▁▆▇█</td></tr><tr><td>eval/samples_per_second</td><td>█▃▂▁</td></tr><tr><td>eval/steps_per_second</td><td>█▃▂▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▃▄▄▅▅▆▆▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▃▄▄▅▅▆▆▆▇███</td></tr><tr><td>train/grad_norm</td><td>▁▂▁▁▅▂▆▁█▃▂▁</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▅▅▄▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▇▄▂▃▃▅▃▁▃▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.945</td></tr><tr><td>eval/f1</td><td>0.9418</td></tr><tr><td>eval/loss</td><td>0.24132</td></tr><tr><td>eval/precision</td><td>0.93684</td></tr><tr><td>eval/recall</td><td>0.94681</td></tr><tr><td>eval/runtime</td><td>2.0576</td></tr><tr><td>eval/samples_per_second</td><td>97.199</td></tr><tr><td>eval/steps_per_second</td><td>12.15</td></tr><tr><td>total_flos</td><td>526222110720000.0</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>128</td></tr><tr><td>train/grad_norm</td><td>0.14846</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1175</td></tr><tr><td>train_loss</td><td>0.28346</td></tr><tr><td>train_runtime</td><td>99.3789</td></tr><tr><td>train_samples_per_second</td><td>20.125</td></tr><tr><td>train_steps_per_second</td><td>1.288</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chocolate-sweep-12</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/7zeprpaz' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/7zeprpaz</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_231619-7zeprpaz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hqem671y with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.6688337174975593e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_231833-hqem671y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/hqem671y' target=\"_blank\">restful-sweep-13</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/hqem671y' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/hqem671y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:48, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.680800</td>\n",
       "      <td>0.651525</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.455285</td>\n",
       "      <td>0.297872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.346500</td>\n",
       "      <td>0.259068</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁█</td></tr><tr><td>eval/f1</td><td>▁█</td></tr><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/precision</td><td>█▁</td></tr><tr><td>eval/recall</td><td>▁█</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▂▄▄▅▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▄▄▅▆▇██</td></tr><tr><td>train/grad_norm</td><td>▁▁▂█▆▇</td></tr><tr><td>train/learning_rate</td><td>█▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>███▇▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.95</td></tr><tr><td>eval/f1</td><td>0.94681</td></tr><tr><td>eval/loss</td><td>0.25907</td></tr><tr><td>eval/precision</td><td>0.94681</td></tr><tr><td>eval/recall</td><td>0.94681</td></tr><tr><td>eval/runtime</td><td>2.0498</td></tr><tr><td>eval/samples_per_second</td><td>97.569</td></tr><tr><td>eval/steps_per_second</td><td>12.196</td></tr><tr><td>total_flos</td><td>263111055360000.0</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>64</td></tr><tr><td>train/grad_norm</td><td>7.89129</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3465</td></tr><tr><td>train_loss</td><td>0.57678</td></tr><tr><td>train_runtime</td><td>49.4097</td></tr><tr><td>train_samples_per_second</td><td>20.239</td></tr><tr><td>train_steps_per_second</td><td>1.295</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">restful-sweep-13</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/hqem671y' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/hqem671y</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_231833-hqem671y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mkqxpq7l with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.91995917801367e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_231957-mkqxpq7l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/mkqxpq7l' target=\"_blank\">honest-sweep-14</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/mkqxpq7l' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/mkqxpq7l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.538100</td>\n",
       "      <td>0.261417</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>0.179155</td>\n",
       "      <td>0.967391</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.951872</td>\n",
       "      <td>0.946809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁█▅</td></tr><tr><td>eval/f1</td><td>▁█▄</td></tr><tr><td>eval/loss</td><td>█▁▃</td></tr><tr><td>eval/precision</td><td>▁█▄</td></tr><tr><td>eval/recall</td><td>▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▅█</td></tr><tr><td>eval/samples_per_second</td><td>█▄▁</td></tr><tr><td>eval/steps_per_second</td><td>█▄▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▃▄▅▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▃▄▅▅▆▇███</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▁▄▁█▂</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>██▆▄▃▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.955</td></tr><tr><td>eval/f1</td><td>0.95187</td></tr><tr><td>eval/loss</td><td>0.2029</td></tr><tr><td>eval/precision</td><td>0.95699</td></tr><tr><td>eval/recall</td><td>0.94681</td></tr><tr><td>eval/runtime</td><td>2.0544</td></tr><tr><td>eval/samples_per_second</td><td>97.353</td></tr><tr><td>eval/steps_per_second</td><td>12.169</td></tr><tr><td>total_flos</td><td>394666583040000.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>96</td></tr><tr><td>train/grad_norm</td><td>13.83809</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0691</td></tr><tr><td>train_loss</td><td>0.33327</td></tr><tr><td>train_runtime</td><td>74.5784</td></tr><tr><td>train_samples_per_second</td><td>20.113</td></tr><tr><td>train_steps_per_second</td><td>1.287</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">honest-sweep-14</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/mkqxpq7l' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/mkqxpq7l</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_231957-mkqxpq7l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: okuatdvp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.2462787213180473e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_232149-okuatdvp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/okuatdvp' target=\"_blank\">hopeful-sweep-15</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/okuatdvp' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/okuatdvp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/32 : < :, Epoch 0.06/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_142970/1539672656.py\", line 30, in train\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2240, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2555, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3745, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3810, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 1202, in forward\n",
      "    outputs = self.roberta(\n",
      "              ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 869, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 618, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 549, in forward\n",
      "    layer_output = apply_chunking_to_forward(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\", line 253, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 561, in feed_forward_chunk\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 459, in forward\n",
      "    hidden_states = self.dense(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 83.81 MiB is free. Process 934104 has 10.66 GiB memory in use. Of the allocated memory 10.37 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hopeful-sweep-15</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/okuatdvp' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/okuatdvp</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_232149-okuatdvp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run okuatdvp errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_142970/1539672656.py\", line 30, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.train()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2240, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return inner_training_loop(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2555, in _inner_training_loop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3745, in training_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3810, in compute_loss\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(**inputs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 1202, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = self.roberta(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 869, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     encoder_outputs = self.encoder(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                       ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 618, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     layer_outputs = layer_module(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 549, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     layer_output = apply_chunking_to_forward(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\", line 253, in apply_chunking_to_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_fn(*input_tensors)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 561, in feed_forward_chunk\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     intermediate_output = self.intermediate(attention_output)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 459, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     hidden_states = self.dense(hidden_states)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.linear(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 83.81 MiB is free. Process 934104 has 10.66 GiB memory in use. Of the allocated memory 10.37 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9ddv8rev with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.646760710654419e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_232159-9ddv8rev</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/9ddv8rev' target=\"_blank\">efficient-sweep-16</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/9ddv8rev' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/9ddv8rev</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_142970/1539672656.py\", line 22, in train\n",
      "    trainer = Trainer(\n",
      "              ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 622, in __init__\n",
      "    self._move_model_to_device(model, args.device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 905, in _move_model_to_device\n",
      "    model = model.to(device)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 3850, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 83.81 MiB is free. Process 934104 has 10.66 GiB memory in use. Of the allocated memory 10.37 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-sweep-16</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/9ddv8rev' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/9ddv8rev</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_232159-9ddv8rev/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 9ddv8rev errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_142970/1539672656.py\", line 22, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 622, in __init__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._move_model_to_device(model, args.device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 905, in _move_model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 3850, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 83.81 MiB is free. Process 934104 has 10.66 GiB memory in use. Of the allocated memory 10.37 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nhuv7ier with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.3384765303368449e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_232204-nhuv7ier</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/nhuv7ier' target=\"_blank\">volcanic-sweep-17</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/nhuv7ier' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/nhuv7ier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_142970/1539672656.py\", line 22, in train\n",
      "    trainer = Trainer(\n",
      "              ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 622, in __init__\n",
      "    self._move_model_to_device(model, args.device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 905, in _move_model_to_device\n",
      "    model = model.to(device)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 3850, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 83.81 MiB is free. Process 934104 has 10.66 GiB memory in use. Of the allocated memory 10.37 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">volcanic-sweep-17</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/nhuv7ier' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/nhuv7ier</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_232204-nhuv7ier/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run nhuv7ier errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_142970/1539672656.py\", line 22, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 622, in __init__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._move_model_to_device(model, args.device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 905, in _move_model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 3850, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 83.81 MiB is free. Process 934104 has 10.66 GiB memory in use. Of the allocated memory 10.37 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u53kyqpc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8.266380743550708e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_232210-u53kyqpc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/u53kyqpc' target=\"_blank\">fancy-sweep-18</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/u53kyqpc' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/u53kyqpc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_142970/1539672656.py\", line 22, in train\n",
      "    trainer = Trainer(\n",
      "              ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 622, in __init__\n",
      "    self._move_model_to_device(model, args.device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 905, in _move_model_to_device\n",
      "    model = model.to(device)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 3850, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 83.81 MiB is free. Process 934104 has 10.66 GiB memory in use. Of the allocated memory 10.37 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fancy-sweep-18</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/u53kyqpc' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/u53kyqpc</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_232210-u53kyqpc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run u53kyqpc errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_142970/1539672656.py\", line 22, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 622, in __init__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._move_model_to_device(model, args.device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 905, in _move_model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 3850, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 83.81 MiB is free. Process 934104 has 10.66 GiB memory in use. Of the allocated memory 10.37 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1eu83yc8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.741494597431253e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_232215-1eu83yc8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/1eu83yc8' target=\"_blank\">earthy-sweep-19</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/1eu83yc8' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/1eu83yc8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_142970/1539672656.py\", line 22, in train\n",
      "    trainer = Trainer(\n",
      "              ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 622, in __init__\n",
      "    self._move_model_to_device(model, args.device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 905, in _move_model_to_device\n",
      "    model = model.to(device)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 3850, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 83.81 MiB is free. Process 934104 has 10.66 GiB memory in use. Of the allocated memory 10.37 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earthy-sweep-19</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/1eu83yc8' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/1eu83yc8</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_232215-1eu83yc8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 1eu83yc8 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_142970/1539672656.py\", line 22, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 622, in __init__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._move_model_to_device(model, args.device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 905, in _move_model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 3850, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 83.81 MiB is free. Process 934104 has 10.66 GiB memory in use. Of the allocated memory 10.37 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rbgrsh82 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4.373593217679872e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250521_232238-rbgrsh82</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/rbgrsh82' target=\"_blank\">valiant-sweep-20</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/sweeps/wecneufa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/rbgrsh82' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/rbgrsh82</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_142970/1539672656.py\", line 22, in train\n",
      "    trainer = Trainer(\n",
      "              ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 622, in __init__\n",
      "    self._move_model_to_device(model, args.device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 905, in _move_model_to_device\n",
      "    model = model.to(device)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 3850, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 83.81 MiB is free. Process 934104 has 10.66 GiB memory in use. Of the allocated memory 10.37 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">valiant-sweep-20</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/rbgrsh82' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1/runs/rbgrsh82</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-sweep-lab1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_232238-rbgrsh82/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run rbgrsh82 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_142970/1539672656.py\", line 22, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 622, in __init__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._move_model_to_device(model, args.device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 905, in _move_model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 3850, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 83.81 MiB is free. Process 934104 has 10.66 GiB memory in use. Of the allocated memory 10.37 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=train, count=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edbcb5d-dd11-4967-913e-9d78e161445b",
   "metadata": {},
   "source": [
    "Test best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7651797-ec90-4ab0-aa18-c686b7086ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Run: sage-sweep-10\n",
      "eval/f1: 0.967391304347826\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "project = \"roberta-sweep-lab1\"\n",
    "\n",
    "# Get all runs of sweep\n",
    "runs = api.runs(path=f\"natalia-timokhova-v-lule-university-of-technology/{project}\")\n",
    "\n",
    "# Sort after best f1 score\n",
    "best_run = sorted(runs, key=lambda r: r.summary.get(\"eval/f1\", 0), reverse=True)[0]\n",
    "\n",
    "print(\"Best Run:\", best_run.name)\n",
    "print(\"eval/f1:\", best_run.summary[\"eval/f1\"])\n",
    "\n",
    "best_model_path = f\"./models/{best_run.name}\"\n",
    "model = RobertaForSequenceClassification.from_pretrained(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5648f5d8-27dc-49aa-80b0-a0fc4792a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8\n",
    "    ),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aef969f-2eac-4025-9e35-8b0dfe563543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnatalia-timokhova-v\u001b[0m (\u001b[33mnatalia-timokhova-v-lule-university-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab_1/wandb/run-20250522_000003-db514rmy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/huggingface/runs/db514rmy' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/huggingface' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/huggingface/runs/db514rmy' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/huggingface/runs/db514rmy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2408459484577179, 'eval_model_preparation_time': 0.0036, 'eval_precision': 0.9493670886075949, 'eval_accuracy': 0.9466666666666667, 'eval_f1': 0.9493670886075949, 'eval_recall': 0.9493670886075949, 'eval_runtime': 3.1396, 'eval_samples_per_second': 95.554, 'eval_steps_per_second': 12.104}\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54ddc265-3703-4bbc-a0e8-13e3f5cda48f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.94667</td></tr><tr><td>eval/f1</td><td>0.94937</td></tr><tr><td>eval/loss</td><td>0.24085</td></tr><tr><td>eval/model_preparation_time</td><td>0.0036</td></tr><tr><td>eval/precision</td><td>0.94937</td></tr><tr><td>eval/recall</td><td>0.94937</td></tr><tr><td>eval/runtime</td><td>3.1396</td></tr><tr><td>eval/samples_per_second</td><td>95.554</td></tr><tr><td>eval/steps_per_second</td><td>12.104</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">./results</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/huggingface/runs/db514rmy' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/huggingface/runs/db514rmy</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/huggingface' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250522_000003-db514rmy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f400c3",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8e46d",
   "metadata": {},
   "source": [
    "> Compare the performance of the two models and explain in which scenarios you would\n",
    "prefer one over the other.\n",
    "\n",
    "The F1-Score for ANN and the Transformers are very similar, so both models perform quite well.\n",
    "If you want to consistently train the network, I'd recommend using the ANN network, as the training is faster there. Especially the 25K dataset took very long to train on the transformer (15 mins with 4070, 7hrs with CPU). But that might be due to the fact that we used a pretrained model with a big architecture.\n",
    "If you plan to only evaluate, I'd use the Transformer, as it utilizes the GPU much better.\n",
    "\n",
    "> How did the two models’ complexity, accuracy, and efficiency differ? Did one model\n",
    "outperform the other in specific scenarios or tasks? If so, why?\n",
    "\n",
    "Both models perform quite good, but the ANN performs under certain aspects worse. In the transformers we use a pre-trained model, so that might explain why it performs better.\n",
    "\n",
    "> What insights did you obtain concerning data amount to train? Embedding utilized?\n",
    "Architectural choices made?\n",
    "\n",
    "For the ANN training with huge datasets, like the 1.2GB one, we cannot use a lot of features for TF-IDF, as\n",
    "we easily hit the RAM limit. e.g. 3 million samples with 100 features account to around 2.4GB of RAM alone\n",
    "for the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
