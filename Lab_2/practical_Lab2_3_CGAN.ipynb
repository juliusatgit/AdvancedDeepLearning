{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oWA6qFFUIt1k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn.functional as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import tensorflow as tf\n",
        "#from torchvision import datasets, transforms\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import tensorflow as tf\n",
        "#from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "\n",
        "# Download MNIST data\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True)\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True)\n",
        "\n",
        "# Extract data and labels\n",
        "X_train = mnist_train.data.numpy()\n",
        "y_train = mnist_train.targets.numpy()\n",
        "X_test = mnist_test.data.numpy()\n",
        "y_test = mnist_test.targets.numpy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxB5OMGbahfW",
        "outputId": "058633ad-5842-415e-a218-9419527fa622"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 39.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.16MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.3MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.87MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "# Load MNIST dataset\n",
        "\n",
        "#mnist_data = tf.keras.datasets.mnist\n",
        "#(X_train, y_train), (X_test, y_test) = mnist_data.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train = X_train.reshape(-1, 28 * 28) / 255.0\n",
        "X_test = X_test.reshape(-1, 28 * 28) / 255.0\n",
        "\n",
        "# Convert class vectors to binary class matrices (one-hot encoding)\n",
        "num_classes = 10 # There are 10 classes in MNIST\n",
        "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test_one_hot = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "mb_size = 64\n",
        "Z_dim = 100\n",
        "X_dim = 28*28\n",
        "y_dim = num_classes\n",
        "h_dim = 128\n",
        "cnt = 0\n",
        "lr = 1e-3"
      ],
      "metadata": {
        "id": "aYdhh7MxJHef"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
        "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)"
      ],
      "metadata": {
        "id": "F5388LURJ8n6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
        "\n",
        "Wzh = xavier_init(size=[Z_dim + y_dim, h_dim])\n",
        "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
        "\n",
        "Whx = xavier_init(size=[h_dim, X_dim])\n",
        "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
        "\n",
        "\n",
        "def G(z, c):\n",
        "    inputs = torch.cat([z, c], 1)\n",
        "    h = nn.relu(inputs @ Wzh + bzh.repeat(inputs.size(0), 1))\n",
        "    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
        "    return X\n",
        "\n",
        "\n",
        "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
        "\n",
        "Wxh = xavier_init(size=[X_dim + y_dim, h_dim])\n",
        "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
        "\n",
        "Why = xavier_init(size=[h_dim, 1])\n",
        "bhy = Variable(torch.zeros(1), requires_grad=True)\n",
        "\n",
        "\n",
        "def D(X, c):\n",
        "    inputs = torch.cat([X, c], 1)\n",
        "    h = nn.relu(inputs @ Wxh + bxh.repeat(inputs.size(0), 1))\n",
        "    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(0), 1))\n",
        "    return y\n",
        "\n",
        "\n",
        "G_params = [Wzh, bzh, Whx, bhx]\n",
        "D_params = [Wxh, bxh, Why, bhy]\n",
        "params = G_params + D_params\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DQR3uhszJ9qb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
        "\n",
        "\n",
        "def reset_grad():\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            data = p.grad.data\n",
        "            p.grad = Variable(data.new().resize_as_(data).zero_())\n",
        "\n",
        "\n",
        "G_solver = optim.Adam(G_params, lr=1e-3)\n",
        "D_solver = optim.Adam(D_params, lr=1e-3)\n",
        "\n",
        "ones_label = Variable(torch.ones(mb_size, 1))\n",
        "zeros_label = Variable(torch.zeros(mb_size, 1))\n",
        "\n",
        "\n",
        "for it in range(100000):\n",
        "    # Sample data\n",
        "    z = Variable(torch.randn(mb_size, Z_dim))\n",
        "    idx = np.random.randint(0, X_train.shape[0], mb_size)\n",
        "    X = Variable(torch.from_numpy(X_train[idx]).float())\n",
        "    c = Variable(torch.from_numpy(y_train_one_hot[idx]).float())\n",
        "\n",
        "    # Dicriminator forward-loss-backward-update\n",
        "    G_sample = G(z, c)\n",
        "    D_real = D(X, c)\n",
        "    D_fake = D(G_sample, c)\n",
        "\n",
        "    D_loss_real = nn.binary_cross_entropy(D_real, ones_label)\n",
        "    D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)\n",
        "    D_loss = D_loss_real + D_loss_fake\n",
        "\n",
        "    D_loss.backward()\n",
        "    D_solver.step()\n",
        "\n",
        "    # Housekeeping - reset gradient\n",
        "    reset_grad()\n",
        "\n",
        "    # Generator forward-loss-backward-update\n",
        "    z = Variable(torch.randn(mb_size, Z_dim))\n",
        "    G_sample = G(z, c)\n",
        "    D_fake = D(G_sample, c)\n",
        "\n",
        "    G_loss = nn.binary_cross_entropy(D_fake, ones_label)\n",
        "\n",
        "    G_loss.backward()\n",
        "    G_solver.step()\n",
        "\n",
        "    # Housekeeping - reset gradient\n",
        "    reset_grad()\n",
        "\n",
        "    # Print and plot every now and then\n",
        "    if it % 1000 == 0:\n",
        "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.numpy(), G_loss.data.numpy()))\n",
        "\n",
        "        c = np.zeros(shape=[mb_size, y_dim], dtype='float32')\n",
        "        c[:, np.random.randint(0, 10)] = 1.\n",
        "        #digit = 5  # replace with any number between 0 and 9\n",
        "        #c[:, digit] = 1\n",
        "        c = Variable(torch.from_numpy(c))\n",
        "        samples = G(z, c).data.numpy()[:16]\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "        gs = gridspec.GridSpec(4, 4)\n",
        "        gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            ax = plt.subplot(gs[i])\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "        if not os.path.exists('out/'):\n",
        "            os.makedirs('out/')\n",
        "\n",
        "        plt.savefig('out/{}.png'.format(str(cnt).zfill(3)), bbox_inches='tight')\n",
        "        cnt += 1\n",
        "        plt.close(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jcrfCyVKNpr",
        "outputId": "a98a88c6-1426-44b6-a544-0931f835703e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter-0; D_loss: 1.6277838945388794; G_loss: 2.1827023029327393\n",
            "Iter-1000; D_loss: 0.0024340443778783083; G_loss: 9.610469818115234\n",
            "Iter-2000; D_loss: 0.009403710253536701; G_loss: 8.802350997924805\n",
            "Iter-3000; D_loss: 0.0055671576410532; G_loss: 8.55102825164795\n",
            "Iter-4000; D_loss: 0.0207245871424675; G_loss: 6.659706115722656\n",
            "Iter-5000; D_loss: 0.05679783225059509; G_loss: 6.349987983703613\n",
            "Iter-6000; D_loss: 0.2502146065235138; G_loss: 5.027286529541016\n",
            "Iter-7000; D_loss: 0.3342430591583252; G_loss: 4.003122329711914\n",
            "Iter-8000; D_loss: 0.38310837745666504; G_loss: 3.663804292678833\n",
            "Iter-9000; D_loss: 0.566585898399353; G_loss: 3.930354595184326\n",
            "Iter-10000; D_loss: 0.45839086174964905; G_loss: 3.3684844970703125\n",
            "Iter-11000; D_loss: 0.927562952041626; G_loss: 3.024186134338379\n",
            "Iter-12000; D_loss: 0.9525339603424072; G_loss: 2.7846009731292725\n",
            "Iter-13000; D_loss: 0.7931914329528809; G_loss: 2.0179967880249023\n",
            "Iter-14000; D_loss: 0.9718301892280579; G_loss: 1.8636219501495361\n",
            "Iter-15000; D_loss: 0.6773267984390259; G_loss: 2.3388500213623047\n",
            "Iter-16000; D_loss: 0.8725754022598267; G_loss: 2.0060815811157227\n",
            "Iter-17000; D_loss: 0.7645155191421509; G_loss: 1.9696518182754517\n",
            "Iter-18000; D_loss: 1.0133130550384521; G_loss: 1.9755672216415405\n",
            "Iter-19000; D_loss: 0.6432342529296875; G_loss: 1.6806361675262451\n",
            "Iter-20000; D_loss: 0.7805761098861694; G_loss: 1.8814828395843506\n",
            "Iter-21000; D_loss: 0.9387160539627075; G_loss: 1.8435266017913818\n",
            "Iter-22000; D_loss: 0.9736930131912231; G_loss: 1.8763059377670288\n",
            "Iter-23000; D_loss: 0.9221545457839966; G_loss: 1.5467005968093872\n",
            "Iter-24000; D_loss: 0.8885014057159424; G_loss: 1.6313526630401611\n",
            "Iter-25000; D_loss: 0.6715391874313354; G_loss: 1.8513083457946777\n",
            "Iter-26000; D_loss: 0.9208815097808838; G_loss: 1.8975776433944702\n",
            "Iter-27000; D_loss: 0.9794270396232605; G_loss: 1.517311930656433\n",
            "Iter-28000; D_loss: 0.7363638877868652; G_loss: 1.6413211822509766\n",
            "Iter-29000; D_loss: 0.8068856000900269; G_loss: 1.81468665599823\n",
            "Iter-30000; D_loss: 0.8727758526802063; G_loss: 2.009249687194824\n",
            "Iter-31000; D_loss: 0.7246184349060059; G_loss: 1.7453303337097168\n",
            "Iter-32000; D_loss: 0.6946274042129517; G_loss: 1.6985143423080444\n",
            "Iter-33000; D_loss: 0.7613573670387268; G_loss: 1.8418145179748535\n",
            "Iter-34000; D_loss: 0.8573763370513916; G_loss: 1.6971639394760132\n",
            "Iter-35000; D_loss: 0.8460753560066223; G_loss: 2.088489532470703\n",
            "Iter-36000; D_loss: 0.7395601272583008; G_loss: 1.653804063796997\n",
            "Iter-37000; D_loss: 0.7574973106384277; G_loss: 1.9540040493011475\n",
            "Iter-38000; D_loss: 0.8202311992645264; G_loss: 1.9546769857406616\n",
            "Iter-39000; D_loss: 0.7105836868286133; G_loss: 1.8685104846954346\n",
            "Iter-40000; D_loss: 0.854201078414917; G_loss: 2.3069405555725098\n",
            "Iter-41000; D_loss: 0.5776896476745605; G_loss: 2.091310739517212\n",
            "Iter-42000; D_loss: 0.6467201709747314; G_loss: 2.306572914123535\n",
            "Iter-43000; D_loss: 0.5825388431549072; G_loss: 2.1676764488220215\n",
            "Iter-44000; D_loss: 0.7475887537002563; G_loss: 1.8645902872085571\n",
            "Iter-45000; D_loss: 0.5673299431800842; G_loss: 2.2099788188934326\n",
            "Iter-46000; D_loss: 0.8503560423851013; G_loss: 1.887770652770996\n",
            "Iter-47000; D_loss: 0.7109351754188538; G_loss: 1.9244849681854248\n",
            "Iter-48000; D_loss: 0.7642885446548462; G_loss: 2.1057989597320557\n",
            "Iter-49000; D_loss: 0.5562368631362915; G_loss: 1.7566587924957275\n",
            "Iter-50000; D_loss: 0.9022692441940308; G_loss: 1.9885940551757812\n",
            "Iter-51000; D_loss: 0.7844251394271851; G_loss: 2.0458693504333496\n",
            "Iter-52000; D_loss: 0.7088499069213867; G_loss: 1.9121500253677368\n",
            "Iter-53000; D_loss: 0.6352028846740723; G_loss: 2.1615636348724365\n",
            "Iter-54000; D_loss: 0.7950124740600586; G_loss: 1.8720014095306396\n",
            "Iter-55000; D_loss: 0.7898017764091492; G_loss: 1.7053167819976807\n",
            "Iter-56000; D_loss: 0.9968940019607544; G_loss: 2.0653815269470215\n",
            "Iter-57000; D_loss: 0.713081955909729; G_loss: 2.0309455394744873\n",
            "Iter-58000; D_loss: 0.7099578380584717; G_loss: 1.9158185720443726\n",
            "Iter-59000; D_loss: 0.6292569637298584; G_loss: 2.1258859634399414\n",
            "Iter-60000; D_loss: 0.7539560794830322; G_loss: 1.7952178716659546\n",
            "Iter-61000; D_loss: 0.8545850515365601; G_loss: 1.5009958744049072\n",
            "Iter-62000; D_loss: 0.7452707290649414; G_loss: 1.9152052402496338\n",
            "Iter-63000; D_loss: 0.9070447683334351; G_loss: 1.5809522867202759\n",
            "Iter-64000; D_loss: 0.7431161999702454; G_loss: 1.9423017501831055\n",
            "Iter-65000; D_loss: 0.7216162085533142; G_loss: 1.7296435832977295\n",
            "Iter-66000; D_loss: 0.673820972442627; G_loss: 1.7490544319152832\n",
            "Iter-67000; D_loss: 1.015641450881958; G_loss: 1.9770574569702148\n",
            "Iter-68000; D_loss: 0.8241180181503296; G_loss: 1.975609540939331\n",
            "Iter-69000; D_loss: 0.8428890705108643; G_loss: 1.7478876113891602\n",
            "Iter-70000; D_loss: 0.6962143182754517; G_loss: 1.983130931854248\n",
            "Iter-71000; D_loss: 0.8162021636962891; G_loss: 1.6445566415786743\n",
            "Iter-72000; D_loss: 0.8979286551475525; G_loss: 1.943800449371338\n",
            "Iter-73000; D_loss: 0.9274245500564575; G_loss: 1.7027807235717773\n",
            "Iter-74000; D_loss: 0.7479398250579834; G_loss: 1.8599449396133423\n",
            "Iter-75000; D_loss: 0.6157425045967102; G_loss: 1.6682202816009521\n",
            "Iter-76000; D_loss: 0.7754694223403931; G_loss: 1.5849169492721558\n",
            "Iter-77000; D_loss: 0.7204200029373169; G_loss: 1.5365500450134277\n",
            "Iter-78000; D_loss: 0.6566987037658691; G_loss: 1.8268080949783325\n",
            "Iter-79000; D_loss: 0.8468881249427795; G_loss: 1.8091506958007812\n",
            "Iter-80000; D_loss: 0.7158241271972656; G_loss: 1.876135230064392\n",
            "Iter-81000; D_loss: 0.7865286469459534; G_loss: 1.7044627666473389\n",
            "Iter-82000; D_loss: 0.7886151075363159; G_loss: 2.0705065727233887\n",
            "Iter-83000; D_loss: 0.5999751091003418; G_loss: 1.6683330535888672\n",
            "Iter-84000; D_loss: 0.8595788478851318; G_loss: 1.7148975133895874\n",
            "Iter-85000; D_loss: 0.8167780637741089; G_loss: 1.7915728092193604\n",
            "Iter-86000; D_loss: 0.7366447448730469; G_loss: 1.8141188621520996\n",
            "Iter-87000; D_loss: 0.7012875080108643; G_loss: 2.005134344100952\n",
            "Iter-88000; D_loss: 0.7888855338096619; G_loss: 1.7063524723052979\n",
            "Iter-89000; D_loss: 0.7717477679252625; G_loss: 1.7432963848114014\n",
            "Iter-90000; D_loss: 0.8640233278274536; G_loss: 2.0796384811401367\n",
            "Iter-91000; D_loss: 0.9134032726287842; G_loss: 1.8415143489837646\n",
            "Iter-92000; D_loss: 0.8064791560173035; G_loss: 1.8788747787475586\n",
            "Iter-93000; D_loss: 0.7684863805770874; G_loss: 2.0667858123779297\n",
            "Iter-94000; D_loss: 0.960270345211029; G_loss: 1.568083643913269\n",
            "Iter-95000; D_loss: 0.9593279361724854; G_loss: 1.9631811380386353\n",
            "Iter-96000; D_loss: 0.796014666557312; G_loss: 1.6388639211654663\n",
            "Iter-97000; D_loss: 0.7136057615280151; G_loss: 2.0659000873565674\n",
            "Iter-98000; D_loss: 0.775783121585846; G_loss: 2.359123945236206\n",
            "Iter-99000; D_loss: 0.9381026029586792; G_loss: 1.6076489686965942\n"
          ]
        }
      ]
    }
  ]
}