{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ce3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0731b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b37867",
   "metadata": {},
   "source": [
    "Vocabulary Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a2d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in spacy.load(\"en_core_web_sm\").tokenizer(text)]\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                frequencies[word] += 1\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "        return [\n",
    "            self.stoi.get(token, self.stoi[\"<UNK>\"])\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe7804",
   "metadata": {},
   "source": [
    "We will be using the Flickr8k dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba1af19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.imgs = self.df[\"image\"].tolist()\n",
    "        raw_captions = self.df[\"caption\"].tolist()\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(raw_captions)\n",
    "\n",
    "        # Preprocess all captions once\n",
    "        self.captions = []\n",
    "        for cap in raw_captions:\n",
    "            tokens = [self.vocab.stoi[\"<SOS>\"]]\n",
    "            tokens += self.vocab.numericalize(cap)\n",
    "            tokens.append(self.vocab.stoi[\"<EOS>\"])\n",
    "            self.captions.append(torch.tensor(tokens))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        caption = self.captions[idx]\n",
    "        return img, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c111c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    captions = []\n",
    "    for img, cap in batch:\n",
    "        images.append(img)\n",
    "        captions.append(cap)\n",
    "    \n",
    "    images = torch.stack(images)\n",
    "    captions = nn.utils.rnn.pad_sequence(captions, batch_first=True, padding_value=0)\n",
    "    return images, captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb3469",
   "metadata": {},
   "source": [
    "Modelle: ResNet50 for Image Feature Extraction and LSTM for language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24fee3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        return self.fc(features)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions[:, :-1])\n",
    "        inputs = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        hiddens, _ = self.lstm(inputs)\n",
    "        outputs = self.fc(hiddens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2114290",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "464fba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, vocab_size, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Start epoch {epoch}\")\n",
    "        for idx, (imgs, captions) in enumerate(dataloader):\n",
    "            imgs, captions = imgs.to(device), captions.to(device)\n",
    "\n",
    "            features = encoder(imgs)\n",
    "            outputs = decoder(features, captions)\n",
    "\n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions[:, 1:].reshape(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865d838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_image(encoder, decoder, image, vocab, max_length=50):\n",
    "    result_caption = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = encoder(image.unsqueeze(0).to(device))\n",
    "        states = None\n",
    "        word = torch.tensor([vocab.stoi[\"<SOS>\"]]).to(device)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            embedding = decoder.embed(word).unsqueeze(1)\n",
    "            hiddens, states = decoder.lstm(embedding, states)\n",
    "            output = decoder.fc(hiddens.squeeze(1))\n",
    "            predicted = output.argmax(1)\n",
    "\n",
    "            word = predicted\n",
    "            predicted_word = vocab.itos[predicted.item()]\n",
    "            if predicted_word == \"<EOS>\":\n",
    "                break\n",
    "            result_caption.append(predicted_word)\n",
    "\n",
    "    return \" \".join(result_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9cafcf",
   "metadata": {},
   "source": [
    "Download Flickr8k dataset:\n",
    "https://www.kaggle.com/datasets/adityajn105/flickr8k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810181c1",
   "metadata": {},
   "source": [
    "Convert txt file to csv file. Goal is to use csv directly in the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdacbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt_path = \"Flickr8k_text/captions.txt\"\n",
    "# csv_path = \"Flickr8k_text/captions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a2d58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(txt_path)\n",
    "# df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e548ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and create dataloaders\n",
    "dataset = FlickrDataset(\n",
    "    root_dir=\"Flickr8k_Dataset\",\n",
    "    captions_file=\"Flickr8k_text/captions.csv\",\n",
    "    transform=transform\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e558fc6-27a1-4686-a6da-cf20d0c6bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(dataset.vocab)\n",
    "num_epochs = 10\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Models\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
    "\n",
    "# Optimizer & Loss\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "params = list(decoder.parameters()) + list(encoder.fc.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "train((encoder, decoder), loader, criterion, optimizer, vocab_size, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890efef5",
   "metadata": {},
   "source": [
    "Caption Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f87c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_caption(image_path, encoder, decoder, vocab, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = transform(image).to(device)\n",
    "\n",
    "    caption = caption_image(encoder, decoder, img_tensor, vocab)\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.title(caption)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba2ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Use picture from Flickr8k_Dataset\n",
    "image_path = \"Flickr8k_Dataset/2513260012_03d33305cf.jpg\"\n",
    "show_caption(image_path, encoder, decoder, dataset.vocab, transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
