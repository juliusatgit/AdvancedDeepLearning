{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d25a74ca",
   "metadata": {},
   "source": [
    "# Image Captioning with Beam Search (PyTorch)\n",
    "Template notebook for training an imageâ€‘captioning model and generating captions with beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "140ab9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os, random\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import models, transforms as T\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "387db026-0cb3-4dcd-9ca0-47d2d7c32d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from \n",
    "# https://www.kaggle.com/datasets/adityajn105/flickr8k/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "9a289243-b5f2-4354-9de0-d342e2c13ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f55200aa030>"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Configuration\n",
    "# ---------------------------\n",
    "IMG_DIR   = \"images\"        # folder with all JPGs\n",
    "CAPT_FILE = \"captions.txt\"  # TSV file with <img>\\t<caption>\n",
    "\n",
    "EMBED_DIM  = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN    = 30\n",
    "EPOCHS     = 10\n",
    "LR         = 1e-3\n",
    "BEAM_SIZE  = 3\n",
    "SEED       = 42\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "random.seed(SEED); torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "e59cf724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Vocabulary helper\n",
    "# ---------------------------\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold:int=5):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
    "        self.stoi = {v:k for k,v in self.itos.items()}\n",
    "    def tokenize(self,text:str)->List[str]:\n",
    "        return text.lower().strip().split()\n",
    "    def build(self,sents:List[str]):\n",
    "        freqs = Counter()\n",
    "        for s in sents: freqs.update(self.tokenize(s))\n",
    "        idx=4\n",
    "        for tok,f in freqs.items():\n",
    "            if f>=self.freq_threshold:\n",
    "                self.stoi[tok]=idx; self.itos[idx]=tok; idx+=1\n",
    "    def numericalize(self,text:str)->List[int]:\n",
    "        return [self.stoi.get(tok,self.stoi['<UNK>']) for tok in self.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "837af51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dataset & collate\n",
    "# ---------------------------\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, img_dir:str, captions_file:str, transform=None, freq_threshold:int=5):\n",
    "        assert os.path.isdir(img_dir), f\"Image folder '{img_dir}' not found!\"\n",
    "        assert os.path.isfile(captions_file), f\"Captions file '{captions_file}' missing!\"\n",
    "        with open(captions_file, encoding='utf8') as f:\n",
    "            self.data=[ln.strip().split('\\t') for ln in f if '\\t' in ln]\n",
    "        self.img_dir=img_dir\n",
    "        self.transform=transform\n",
    "        self.vocab=Vocabulary(freq_threshold)\n",
    "        self.vocab.build([c for _,c in self.data])\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        img_id,cap=self.data[idx]\n",
    "        img=Image.open(os.path.join(self.img_dir,img_id)).convert('RGB')\n",
    "        if self.transform: img=self.transform(img)\n",
    "        cap_idx=[self.vocab.stoi['<SOS>']]+self.vocab.numericalize(cap)+[self.vocab.stoi['<EOS>']]\n",
    "        return img,torch.tensor(cap_idx)\n",
    "\n",
    "class PadCollate:\n",
    "    def __init__(self,pad_idx): self.pad_idx=pad_idx\n",
    "    def __call__(self,batch):\n",
    "        imgs=torch.stack([b[0] for b in batch])\n",
    "        caps=pad_sequence([b[1] for b in batch], batch_first=True, padding_value=self.pad_idx)\n",
    "        return imgs,caps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "1752d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Encoder & Decoder (with optional beam search)\n",
    "# ---------------------------\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed:int):\n",
    "        super().__init__()\n",
    "        res=models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        for p in res.parameters(): p.requires_grad=False\n",
    "        self.backbone=nn.Sequential(*list(res.children())[:-1])\n",
    "        self.flat=nn.Flatten()\n",
    "        self.fc=nn.Linear(res.fc.in_features, embed)\n",
    "        self.relu=nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        with torch.no_grad(): x=self.backbone(x)\n",
    "        x=self.flat(x)\n",
    "        return self.relu(self.fc(x))\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed, hidden, vocab, num_layers=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embed=nn.Embedding(vocab, embed)\n",
    "        self.lstm=nn.LSTM(embed, hidden, num_layers)\n",
    "        self.fc=nn.Linear(hidden, vocab)\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "    def forward(self, feats, caps):\n",
    "        caps=caps[:-1]\n",
    "        emb=self.drop(self.embed(caps))\n",
    "        emb=torch.cat((feats.unsqueeze(0), emb), 0)\n",
    "        h,_=self.lstm(emb)\n",
    "        return self.fc(h)\n",
    "    def beam_search(self, feats, vocab, beam=BEAM_SIZE, max_len=MAX_LEN):\n",
    "        seqs=[(0.0,[],None,feats.unsqueeze(0))]\n",
    "        finished=[]\n",
    "        for _ in range(max_len):\n",
    "            cand=[]\n",
    "            for sc,seq,st,inp in seqs:\n",
    "                if seq and seq[-1]==vocab.stoi['<EOS>']:\n",
    "                    finished.append((sc,seq)); continue\n",
    "                h,ns=self.lstm(inp,st)\n",
    "                logp=torch.log_softmax(self.fc(h.squeeze(0)),dim=-1)\n",
    "                lp,idx=logp.topk(beam)\n",
    "                for k in range(beam):\n",
    "                    cand.append((sc+lp[0,k].item(), seq+[idx[0,k].item()], ns, self.embed(idx[0,k]).unsqueeze(0)))\n",
    "            seqs=sorted(cand,key=lambda t:t[0],reverse=True)[:beam]\n",
    "            if not seqs: break\n",
    "        finished += [(sc,seq) for sc,seq,_,_ in seqs]\n",
    "        return max(finished,key=lambda t:t[0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "c09c6421-acbe-487a-a23d-111b569cec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Training routine\n",
    "# ---------------------------\n",
    "\n",
    "def train(output_path=\"model.pth\"):\n",
    "    transform=T.Compose([\n",
    "        T.Resize(256), T.CenterCrop(224), T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])\n",
    "    ds=FlickrDataset(IMG_DIR, CAPT_FILE, transform)\n",
    "    pad_idx=ds.vocab.stoi['<PAD>']\n",
    "    dl=DataLoader(ds,batch_size=BATCH_SIZE,shuffle=True, collate_fn=PadCollate(pad_idx))\n",
    "\n",
    "    enc=EncoderCNN(EMBED_DIM).to(DEVICE)\n",
    "    dec=DecoderRNN(EMBED_DIM,HIDDEN_DIM,len(ds.vocab.stoi),NUM_LAYERS).to(DEVICE)\n",
    "\n",
    "    criterion=nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "    params=list(dec.parameters())+list(enc.fc.parameters())\n",
    "    optim=torch.optim.Adam(params, lr=LR)\n",
    "\n",
    "    enc.train(); dec.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        total=0\n",
    "        for imgs,caps in dl:\n",
    "            imgs,caps=imgs.to(DEVICE),caps.to(DEVICE)\n",
    "            feats=enc(imgs)\n",
    "            outputs=dec(feats,caps.t())\n",
    "            loss=criterion(outputs.reshape(-1, outputs.size(-1)), caps.reshape(-1))\n",
    "            optim.zero_grad(); loss.backward(); optim.step()\n",
    "            total+=loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | loss: {total/len(dl):.4f}\")\n",
    "    torch.save({'encoder':enc.state_dict(),'decoder':dec.state_dict(),'vocab':ds.vocab.stoi}, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "30ff91ce-b3fc-412c-b644-491bbdc55d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(img_path:str, model_path=\"model.pth\", beam=True, beam_size=BEAM_SIZE):\n",
    "    ckpt=torch.load(model_path, map_location=DEVICE)\n",
    "    stoi=ckpt['vocab']; vocab=Vocabulary(); vocab.stoi=stoi; vocab.itos={i:t for t,i in stoi.items()}\n",
    "    enc=EncoderCNN(EMBED_DIM).to(DEVICE); enc.load_state_dict(ckpt['encoder']); enc.eval()\n",
    "    dec=DecoderRNN(EMBED_DIM,HIDDEN_DIM,len(stoi)).to(DEVICE); dec.load_state_dict(ckpt['decoder']); dec.eval()\n",
    "    trans=T.Compose([T.Resize(256),T.CenterCrop(224),T.ToTensor(),T.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])])\n",
    "    img=trans(Image.open(img_path).convert('RGB')).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        feat=enc(img)\n",
    "        ids=dec.beam_search(feat,vocab,beam_size) if beam else dec.beam_search(feat,vocab,1)\n",
    "    tokens=[vocab.itos[i] for i in ids if i not in {vocab.stoi['<EOS>']}]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955b158-e22d-4283-8b05-130644877cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
